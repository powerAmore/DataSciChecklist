{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction 工作中涉及到了数据领域的方方面面，包括大数据技术、机器学习、深度学习、推荐系统、数据产品、数据分析等非常多的领域。然而由于工作重心不断变换，一段时间不接触，很多领域的基础知识非常容易遗忘，非常可惜。重新查阅相关资料非常耗时，有些基于自身理解的概念，即使查阅资料也并不容易找回。 对于查阅资料。网络上各种资料教程非常多，也不乏高质量的课程。但是问题也很明显，多数教程面向初学者，极其细致，看完往往需要很长时间，学习效率较低。尤其视频教程，甚至很难快速直接找到关注点。对于在该领域已经建立整体概念框架的人来说，需要的往往是提纲挈领，直击要害，把书读薄。 故做此Checklist，帮助自己也帮助需要复习相关领域知识点的小伙伴快速恢复记忆。务必做到言简意赅，突出重点。 "},"GradientDescent.html":{"url":"GradientDescent.html","title":"梯度下降","keywords":"","body":"梯度下降法 机器学习深度学习中用梯度下降法来优化损失函数，试图求解损失函数的最小值以及其对应的参数。要搞清楚梯度下降法，我们从方向导数的概念引入。 方向导数 方向导数就是曲面切线的斜率。曲面的切线不是唯一的，360∘360^{\\circ}360∘ 各个方向都有，所以不同方向的切线斜率也不一定相同，方向导数也不是唯一的。 例如二元函数 f(x,y)=x2+y2f(x,y)=x^2+y^2f(x,y)=x2+y2 的方向导数如图所示： 函数 f(x,y)f(x,y)f(x,y) 在 PPP 点沿着切线 lll 方向的方向导数为： lim⁡ρ→0f(x+Δx,y+Δy)−f(x,y)ρ {\\lim_{\\rho\\to0}\\frac{f(x+\\Delta x, y+\\Delta y)-f(x,y)}{\\rho}} ρ→0lim​ρf(x+Δx,y+Δy)−f(x,y)​ 其中 ρ=(Δx)2+(Δy)2\\rho=\\sqrt{(\\Delta x)^2+(\\Delta y)^2}ρ=(Δx)2+(Δy)2​ 。把该极限记作方向导数 ∂f∂l\\frac{\\partial f}{\\partial l}∂l∂f​ 。其计算公式为： ∂f∂l=∂f∂xcosφ+∂f∂ysinφ \\frac{\\partial f}{\\partial l} = \\frac{\\partial f}{\\partial x}cos\\varphi + \\frac{\\partial f}{\\partial y}sin\\varphi ∂l∂f​=∂x∂f​cosφ+∂y∂f​sinφ 其中 φ\\varphiφ 为x轴正方向到 lll 的角度。 梯度 梯度是个向量，它的模为曲面上该点取值最大的方向导数的值，方向为最大方向导数对应的切线的投影方向。也是该点所处的等高线的法向量，也就是函数值变化最快的方向。 [!NOTE] 梯度的方向并不是最大方向导数对应的切线方向，而是切线的投影方向。但即使认为是切线方向，对理解梯度的概念也不会产生太大的影响。 图中绿色箭头所示向量即为梯度： 对于二元函数 f(x,y)f(x,y)f(x,y) ，其曲面上任意点 P(x,y)P(x,y)P(x,y) 的梯度，记作 gradf(x,y)gradf(x,y)gradf(x,y) 或 ∇f(x,y)\\nabla f(x,y)∇f(x,y) ，定义为： ∇f(x,y)=(∂f∂x,∂f∂y)=fx(x,y)i⃗+fy(x,y)j⃗ \\nabla f(x,y)=(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y})=f_x(x,y)\\vec i+f_y(x,y)\\vec j ∇f(x,y)=(∂x∂f​,∂y∂f​)=fx​(x,y)i+fy​(x,y)j​ 仍然以二元函数 f(x,y)=x2+y2f(x,y)=x^2+y^2f(x,y)=x2+y2 为例。设 e⃗=(cosφ,sinφ)\\vec e=(cos\\varphi, sin\\varphi)e=(cosφ,sinφ) 为曲面某点切线 lll 方向导数所对应方向的单位向量，则该方向导数为： ∂f∂l=∂f∂xcosφ+∂f∂ysinφ=(∂f∂x,∂f∂y)⋅(cosφ,sinφ)=∣∇f(x,y)∣⋅∣e⃗∣⋅cos⟨∇f(x,y),e⃗⟩=∣∇f(x,y)∣⋅1⋅cos⟨∇f(x,y),e⃗⟩ \\begin{align*} \\frac{\\partial f}{\\partial l} =\\frac{\\partial f}{\\partial x}cos\\varphi + \\frac{\\partial f}{\\partial y}sin\\varphi &=(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y})\\cdot(cos\\varphi, sin\\varphi) \\\\ &=|\\nabla f(x,y)|\\cdot |\\vec e| \\cdot cos\\langle\\nabla f(x,y),\\vec e\\rangle \\\\ &=|\\nabla f(x,y)|\\cdot 1 \\cdot cos\\langle\\nabla f(x,y),\\vec e\\rangle \\end{align*} ∂l∂f​=∂x∂f​cosφ+∂y∂f​sinφ​=(∂x∂f​,∂y∂f​)⋅(cosφ,sinφ)=∣∇f(x,y)∣⋅∣e∣⋅cos⟨∇f(x,y),e⟩=∣∇f(x,y)∣⋅1⋅cos⟨∇f(x,y),e⟩​ 其中，⟨∇f(x,y),e⃗⟩\\langle\\nabla f(x,y),\\vec e\\rangle⟨∇f(x,y),e⟩ 为 ∇f(x,y)\\nabla f(x,y)∇f(x,y) 和 e⃗\\vec ee 的夹角。当 cos⟨∇f(x,y),e⃗⟩=1cos\\langle\\nabla f(x,y),\\vec e\\rangle=1cos⟨∇f(x,y),e⟩=1 时，方向导数 ∂f∂l\\frac{\\partial f}{\\partial l}∂l∂f​ 有最大值，为梯度的模 ∣∇f(x,y)∣|\\nabla f(x,y)|∣∇f(x,y)∣ ，且此时 e⃗\\vec ee 的方向和梯度 ∇f(x,y)\\nabla f(x,y)∇f(x,y) 的方向保持一致。 梯度下降法 为求解函数的最小值以及最小值对应的坐标，数学上通常要么有直接的求解公式。要么就对函数进行求导，令导函数等于0进行求解。但面对机器学习这类场景，往往数据量大、维度高导致计算量过大，或者目标函数复杂本身无法获得解析解。机器学习场景下，往往利用梯度下降这类迭代优化算法快速逼近目标函数的最小值以获得最优化的参数值。 梯度下降法的作用不仅仅是求解函数最小值，在机器学习深度学习算法中，更重要的是获取函数最小值（尽可能小）时对应的坐标，即最优化的函数参数值。 梯度下降法的原理是：试图通过迭代的方式，每步都沿着函数数值下降最快的方向（也就是负梯度方向）走一小步。每走一步都重新确认一下负梯度方向，然后再沿着该方向下降，直到找到函数最小值以及对应的位置。 [!NOTE] 之所以每步都沿着负梯度方向走有两个原因： 负梯度方向一定是会让函数值变小（至少不变大）的方向。需要注意：走出了这一步不意味着函数值就一定会变小，也有可能步子迈大了，函数值反而有所变大。 负梯度是函数值下降最快的方向，便于更快速的找到最小值。其实，即使每步不是沿着负梯度方向，只要是沿着一个函数值变小的方向，最终也是能找到函数的最小值的。 代码实现梯度下降 仍然以二元函数 f(x,y)=x2+y2f(x,y)=x^2+y^2f(x,y)=x2+y2 为例，写代码实现梯度下降法求解函数最小值。代码其实很简单，关键需要自己手动把函数梯度先求出来。10次迭代之后可以看到已经很接近函数的最小值了。如果再多迭代几次就肯定能达到最小值。 # 原函数f(x,y) def f(x,y): return x ** 2 + y ** 2 # f(x,y)对x的偏导 def fx(x): return 2 * x # f(x,y)对y的偏导 def fy(y): return 2 * y # 设置梯度下降起始点 x=2; y=2 # 设置梯度下降步长 step = 0.1 # 循环迭代进行梯度下降 for i in range(10): # 设置梯度下降迭代次数为10 before = f(x, y) # 梯度下降开始前，起始坐标下的函数值 x = x - step * fx(x) # x轴方向进行梯度下降，获得新的新坐标 y = y - step * fy(y) # y轴方向进行梯度下降，获得新的y坐标 after = f(x, y) # 梯度下降完成，基于新坐标的函数值 theta = before - after # 完成一次梯度下降迭代，前后函数值的差 print(\"before:{:.2f}, after:{:.2f}, theta:{:.2f}, x:{:.2f}, y:{:.2f}\".format(before, after, theta, x, y)) # 输出结果并保留2为小数 before:8.00, after:5.12, theta:2.88, x:1.60, y:1.60 before:5.12, after:3.28, theta:1.84, x:1.28, y:1.28 before:3.28, after:2.10, theta:1.18, x:1.02, y:1.02 before:2.10, after:1.34, theta:0.75, x:0.82, y:0.82 before:1.34, after:0.86, theta:0.48, x:0.66, y:0.66 before:0.86, after:0.55, theta:0.31, x:0.52, y:0.52 before:0.55, after:0.35, theta:0.20, x:0.42, y:0.42 before:0.35, after:0.23, theta:0.13, x:0.34, y:0.34 before:0.23, after:0.14, theta:0.08, x:0.27, y:0.27 before:0.14, after:0.09, theta:0.05, x:0.21, y:0.21 实验：用方向导数替代梯度下降 假设用方向导数来替代梯度，可以看到其实也可以起到下降的效果，只是速度要慢一些，10次迭代后离最小值还有点距离。在真实的大数据应用场景中，速度是需要考虑的非常重要的点。梯度不仅下降快，求梯度也比求方向导数更加容易。 import math # 设置梯度下降起始点 x=2; y=2 # 设置梯度下降步长 step = 0.1 for i in range(10): # 设置迭代次数为10 before = f(x, y) # 开始前，起始坐标下的函数值 x = x - step * fx(x) * math.cos(math.pi/3) # 沿着切线投影与x成60度角的方向导数进行下降 y = y - step * fy(y) * math.sin(math.pi/3) # 沿着切线投影与x成60度角的方向导数进行下降 after = f(x, y) # 完成后，基于新坐标的函数值 theta = before - after # 完成一次迭代，前后函数值的差 print(\"before:{:.2f}, after:{:.2f}, theta:{:.2f}, x:{:.2f}, y:{:.2f}\".format(before, after, theta, x, y)) # 输出结果并保留2为小数 before:8.00, after:5.97, theta:2.03, x:1.80, y:1.65 before:5.97, after:4.49, theta:1.48, x:1.62, y:1.37 before:4.49, after:3.40, theta:1.09, x:1.46, y:1.13 before:3.40, after:2.60, theta:0.81, x:1.31, y:0.93 before:2.60, after:1.99, theta:0.60, x:1.18, y:0.77 before:1.99, after:1.54, theta:0.45, x:1.06, y:0.64 before:1.54, after:1.19, theta:0.34, x:0.96, y:0.53 before:1.19, after:0.93, theta:0.26, x:0.86, y:0.44 before:0.93, after:0.73, theta:0.20, x:0.77, y:0.36 before:0.73, after:0.58, theta:0.16, x:0.70, y:0.30 画图展示梯度下降 采用plotly进行作图。之所以不用matplot，是因为matplot不太好实现surface和scatter在同一张图里展示。如图所见，红色线表示梯度下降迭代过程，逐渐逼近函数曲面最小值，每个小红点代表一次迭代。而蓝色线处于xy平面上，每一个小蓝点才是真正每一步迭代的梯度。 # 设置梯度下降起始点 x=2; y=2 # 设置梯度下降步长 step = 0.1 pos_x = [] pos_y = [] pos_z = [] # 循环迭代进行梯度下降 for i in range(10): # 设置梯度下降迭代次数为10 before = f(x, y) # 梯度下降开始前，起始坐标下的函数值 pos_x.append(x) pos_y.append(y) pos_z.append(before) x = x - step * fx(x) # x轴方向进行梯度下降，获得新的新坐标 y = y - step * fy(y) # y轴方向进行梯度下降，获得新的y坐标 after = f(x, y) # 梯度下降完成，基于新坐标的函数值 pos_x.append(x) pos_y.append(y) pos_z.append(after) from plotly.offline import init_notebook_mode, iplot import plotly.graph_objects as go import numpy as np # 画二元函数曲面 xx = np.arange(-3,3,0.1) yy = np.arange(-3,3,0.1) X, Y = np.meshgrid(xx, yy) Z = X ** 2 +Y ** 2 trace_surface= go.Surface(x=X, y=Y, z=Z, colorscale='redor', showscale=False, opacity=0.7) # 画红色trace线 trace_scatter3d = go.Scatter3d(x=pos_x, y=pos_y, z=pos_z, mode='lines+markers', marker=dict(color='red', size=3), name = 'trace') # 画蓝色gradient线 trace_gradient = go.Scatter3d(x=pos_x, y=pos_y, z=np.zeros(len(pos_x)), mode='lines+markers', marker=dict(color='blue', size=3), name = 'gradient') data=[trace_surface, trace_scatter3d, trace_gradient] # 图片布局调整 layout = go.Layout(scene = dict(aspectratio = dict(x=1.5, y=1.5, z=1)), margin=dict(l=5, r=5, t=5, b=5), width=700) fig = dict(data = data, layout = layout) iplot(fig) "},"LinearRegression.html":{"url":"LinearRegression.html","title":"线性回归","keywords":"","body":"线性回归的目标函数 已知数据集 X\\boldsymbol{X}X ，其中有 mmm 个 nnn 维样本 xi\\boldsymbol{x}^ixi ，iii 代表第 iii 个样本。每个样本 xi\\boldsymbol{x}^ixi 对应着一个目标值 yiy^iyi ，或称标签。把目标值 yiy^iyi 的集合（向量）记作 Y\\boldsymbol{Y}Y 。 我们试图去拟合一个线性函数f(x)f(\\boldsymbol{x})f(x) ： f(x)=ωTx+b=ω1x1+ω2x2+⋯+ωnxn+b f(\\boldsymbol{x}) = \\boldsymbol{\\omega}^\\mathsf{T} \\boldsymbol{x} + b=\\omega_1x_1+\\omega_2x_2+\\cdots+\\omega_nx_n+b f(x)=ωTx+b=ω1​x1​+ω2​x2​+⋯+ωn​xn​+b 使得 yi=f(xi)y^i =f(\\boldsymbol{x}^i)yi=f(xi) ，这就是线性回归。但显然线性函数本质上只是一条直线（2维）或者一个超平面（n维），并不太能让任意 xi\\boldsymbol{x}^ixi 满足yi=f(xi)y^i =f(\\boldsymbol{x}^i)yi=f(xi) 。只能退而求其次，找到一个尽可能接近真实情况的f(x)f(\\boldsymbol{x})f(x) 。一旦有了这个f(x)f(\\boldsymbol{x})f(x) ，如果我们输入一个数据集 X\\boldsymbol{X}X 中不存在的样本 x\\boldsymbol{x}x ，就可以求解一个预测值 yyy 达到机器学习目的。 线性回归的损失函数 如何度量所谓的尽可能接近真实情况呢？我们采用均方误差来度量。均方误差越小，则说明该线性函数f(x)f(\\boldsymbol{x})f(x) 越接近真实的情况。我们把均方误差函数作为线性回归的损失函数： J(ω,b)=12∑i=1m(fω,b(xi)−yi)2=12∑i=1m(ωTxi+b−yi)2 J(\\boldsymbol{\\omega},b)=\\frac{1}{2}\\sum_{i=1}^m(f_{\\boldsymbol{\\omega},b}(\\boldsymbol{x}^i)-y^i)^2=\\frac{1}{2}\\sum_{i=1}^m(\\boldsymbol{\\omega}^\\mathsf{T} \\boldsymbol{x}^i + b-y^i)^2 J(ω,b)=21​i=1∑m​(fω,b​(xi)−yi)2=21​i=1∑m​(ωTxi+b−yi)2 同样都作为线性函数f(x)f(\\boldsymbol{x})f(x) 需要求解的未知参数，可以把 ω\\boldsymbol{\\omega}ω 和 b b b 合并为一个向量 θ\\boldsymbol{\\theta}θ ，相应的需要把每一个样本 x=(x1,x2,⋯ ,xn)T\\boldsymbol{x}=(x_1, x_2,\\cdots,x_n)^\\mathsf{T}x=(x1​,x2​,⋯,xn​)T 最后都增加一个 111 改写为 x=(x1,x2,⋯ ,xn,1)T\\boldsymbol{x}=(x_1, x_2,\\cdots,x_n,1)^\\mathsf{T}x=(x1​,x2​,⋯,xn​,1)T 。经过这样的变换，需要求解的线性函数f(x)f(\\boldsymbol{x})f(x) 就可以简写为：f(x)=θTxf(\\boldsymbol{x}) = \\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}f(x)=θTx ，而损失函数可以简写为： J(θ)=12∑i=1m(fθ(xi)−yi)2=12∑i=1m(θTxi−yi)2 J(\\boldsymbol{\\theta})=\\frac{1}{2}\\sum_{i=1}^m(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}^i)-y^i)^2=\\frac{1}{2}\\sum_{i=1}^m(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i -y^i)^2 J(θ)=21​i=1∑m​(fθ​(xi)−yi)2=21​i=1∑m​(θTxi−yi)2 损失函数求最小值 解析解：正规方程 将样本通通带入损失函数，找到损失函数最小时对应的参数向量 θ\\boldsymbol{\\theta}θ ，则可得到我们希望拟合的最优线性函数f(x)f(\\boldsymbol{x})f(x) 。对于线性回归而言，对基于均方误差的损失函数求最小值，是可以求得解析解的，即存在一个简单的公式表达最小值。具体的做法就是损失函数对 θ\\boldsymbol{\\theta}θ 求导，令导数等于0。最后求得损失函数的解析解为： θ=(XTX)−1XTY \\boldsymbol{\\theta}=(\\boldsymbol{X}^\\mathsf{T}\\boldsymbol{X})^{-1}\\boldsymbol{X}^\\mathsf{T}\\boldsymbol{Y} θ=(XTX)−1XTY 也称之为正规方程。这个解析解存在的一个问题是需要满足 XTX\\boldsymbol{X}^\\mathsf{T}\\boldsymbol{X}XTX 可逆，所以在具体算法实现中，通常并不直接利用这个解析解公式，而是利用SVD奇异值分解的方式进行求解，sklearn中的linear_model.LinearRegression()就是这样的实现，这里暂时不详说。我们更想要深入讨论的是通过梯度下降法来优化（求解）损失函数的方法。另外基于SVD分解的线性回归计算复杂度为 O(mn2)O(mn^2)O(mn2) ，复杂度过高，不适合高维大样本的计算。 梯度下降法 目标是优化损失函数 J(θ)J(\\boldsymbol{\\theta})J(θ) ，找到合适的参数向量 θ=(θ1,θ2,⋯ ,θn)\\boldsymbol{\\theta}=(\\theta_1,\\theta_2,\\cdots,\\theta_n)θ=(θ1​,θ2​,⋯,θn​) 使得损失函数 J(θ)J(\\boldsymbol{\\theta})J(θ) 的值尽可能小。根据梯度下降法的原理，先求梯度，对每一个分量 θj\\theta_jθj​ 求偏导： ∂J(θ)∂θj=∂∂θj12∑i=1m(θTxi−yi)2=∑i=1m[(θTxi−yi)⋅∂∂θj(θTxi−yi)]=∑i=1m[(θTxi−yi)⋅∂∂θj(θ1x1i+θ2x2i+⋯+θnxni−yi)]=∑i=1m[(θTxi−yi)⋅xji] \\begin{align*} \\frac{\\partial J(\\boldsymbol{\\theta})}{\\partial \\theta_j} &=\\frac{\\partial}{\\partial \\theta_j}\\frac{1}{2}\\sum_{i=1}^m(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i -y^i)^2\\\\ &=\\sum_{i=1}^m[(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i -y^i)\\cdot\\frac{\\partial}{\\partial \\theta_j}(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i -y^i)]\\\\ &=\\sum_{i=1}^m[(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i -y^i)\\cdot\\frac{\\partial}{\\partial \\theta_j}(\\theta_1x^i_1+\\theta_2x^i_2+\\cdots+\\theta_nx^i_n-y^i)]\\\\ &=\\sum_{i=1}^m[(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i -y^i)\\cdot x^i_j] \\end{align*} ∂θj​∂J(θ)​​=∂θj​∂​21​i=1∑m​(θTxi−yi)2=i=1∑m​[(θTxi−yi)⋅∂θj​∂​(θTxi−yi)]=i=1∑m​[(θTxi−yi)⋅∂θj​∂​(θ1​x1i​+θ2​x2i​+⋯+θn​xni​−yi)]=i=1∑m​[(θTxi−yi)⋅xji​]​ 有了梯度之后，就可以沿着负梯度方向不断迭代更新参数 θ\\boldsymbol{\\theta}θ ： θj:=θj−α∑i=1m[(θTxi−yi)⋅xji] \\theta_j := \\theta_j - \\alpha\\sum_{i=1}^m[(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i -y^i)\\cdot x^i_j] θj​:=θj​−αi=1∑m​[(θTxi−yi)⋅xji​] 其中 α\\alphaα 为学习率。等式右边的 θj\\theta_jθj​ 为更新前的值，左边的 θj\\theta_jθj​ 为更新后的值。随着不断的迭代，损失函数 J(θ)J(\\boldsymbol{\\theta})J(θ) 的值会逐渐变小直至收敛。一旦收敛则可停止迭代，此时的参数向量 θ\\boldsymbol{\\theta}θ 即为线性回归最终的求解结果。已知 θ\\boldsymbol{\\theta}θ 就可以写出我们最终希望拟合的那个线性函数 fθ(x)f_\\theta (\\boldsymbol{x})fθ​(x) 。 代码实现线性回归的梯度下降法 以二维平面中的点数据作为样本，尝试编写用梯度下降法求优化线性回归问题的代码。代码本身并未局限于二维样本，可以将更高维样本数据作为输入进行代码测试。 import numpy as np points = np.genfromtxt('data.csv', delimiter=',') m = len(points) # 样本量 n = len(points[0]) # 样本维数 # 从points中取出样本列，并在每个样本后面增加一个1 X = np.c_[points[:,0], np.ones((m, 1))] # 从points中取出目标值向量 Y= points[:,1] # 目标线性函数 y = f(x) def f(w, x): return np.dot(w, x) # 点乘 # 均方误差损失函数 J(w) def J(w): j = 0 for i in range(m): y_hat = f(w, X[i]) y = Y[i] j += 0.5 * (y_hat - y) ** 2 return j # 损失函数 J(w) 的梯度向量 def grad_J(w): grad = np.zeros(n) for i in range(m): y_hat = f(w, X[i]) y = Y[i] grad += (y_hat - y) * X[i] return grad # 设置学习率 alpha = 0.000001 def grad_desc(w, num_iter): J_list = [] grad_J_list = [] w_list = [] for i in range(num_iter): valueJ = J(w) vectorGradJ = grad_J(w) J_list.append(valueJ) grad_J_list.append(vectorGradJ) w_list.append(w) print(\"J(w):{}, grad_J(w):{}, w:{}\".format(valueJ, vectorGradJ, w)) w = w - alpha * vectorGradJ return J_list, grad_J_list, w_list # 初始化参数向量W，全都设置为0 W = np.zeros(n) # 设置迭代次数 steps = 10 J_list, grad_J_list, w_list = grad_desc(W, steps) J(w):278255.3917241606, grad_J(w):[-368535.14867955 -7273.50505537], w:[0. 0.] J(w):159313.34591670343, grad_J(w):[-276698.86358788 -5468.4907399 ], w:[0.36853515 0.00727351] J(w):92264.27096878138, grad_J(w):[-207747.47773819 -4113.27214627], w:[0.64523401 0.012742 ] J(w):54467.89340270772, grad_J(w):[-155978.24942245 -3095.76362377], w:[0.85298149 0.01685527] J(w):33161.61632500935, grad_J(w):[-117109.516512 -2331.81040782], w:[1.00895974 0.01995103] J(w):21151.008995888868, grad_J(w):[-87926.57430465 -1758.228457 ], w:[1.12606926 0.02228284] J(w):14380.483504690563, grad_J(w):[-66015.79790157 -1327.57870932], w:[1.21399583 0.02404107] J(w):10563.855732624896, grad_J(w):[-49565.0192525 -1004.2435539], w:[1.28001163 0.02536865] J(w):8412.375949653142, grad_J(w):[-37213.64871322 -761.48101583], w:[1.32957665 0.02637289] J(w):7199.560294294272, grad_J(w):[-27940.14516359 -579.21301566], w:[1.3667903 0.02713437] 将损失函数迭代优化的过程和最终的目标函数画图做出 import matplotlib.pyplot as plt plt.figure(figsize=(20, 8)) ax1 = plt.subplot(1, 2, 1) plt.plot(J_list) ax2 = plt.subplot(1, 2, 2) plt.scatter(points[:,0], points[:,1]) # 针对每一个x，计算出预测的y值 pred_y = [f(w_list[-1], x) for x in X] plt.plot(points[:,0], pred_y, c='r') plt.show() 梯度下降法学习率的设定 在上面线性回归梯度下降的代码中可以发现步长或者说学习率alpha = 0.000001，是一个非常小的值。刚开始写代码的时候也没想到alpha会如此之小，以为0.01或者0.001就可以，但是发现都不行，计算结果都会导致损失函数完全无法收敛，反而变的巨大无比。经过不断尝试才最终选到一个这么小的学习率。 在梯度下降法中，学习率的设定是重要且敏感的。不合适的学习率可能导致如下问题： 学习率设置太小，需要花费过多的时间来收敛 学习率设置较大，无法收敛到最小值 进入局部极值点就收敛，没有真正找到的最优解 停在鞍点处，不能够在另一维度继续下降 过拟合与正则化 过拟合：在训练数据集上拟合效果不错， 但是在测试数据集上拟合效果却不好。通常原因在于原始的样本维度或者说特征过多，存在一些嘈杂特征， 模型过于复杂是因为模型尝试去兼顾每一个训练数据点。 欠拟合：训练数据上拟合的不好，并且在测试数据集上也不能很好地拟合。通常原因是学习到的特征过少，需要增加训练数据的特征数量。 正则化是一种降低模型复杂度来解决过拟合问题的方法。如下图所示，如果我们能有效降低高次项对模型的影响，则能解决模型的过拟合问题。 线性回归中正则化实际的操作方式是在损失函数中增加惩罚项，在原有的损失函数中增加对参数的限制。 L1正则（L1-norm），Lasso： J(θ)=12∑i=1m(fθ(xi)−yi)2+λ∑i=1n∣θj∣ J(\\boldsymbol{\\theta})=\\frac{1}{2}\\sum_{i=1}^m(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}^i)-y^i)^2+\\lambda\\sum_{i=1}^n|\\theta_j| J(θ)=21​i=1∑m​(fθ​(xi)−yi)2+λi=1∑n​∣θj​∣ L2正则（L2-norm），岭回归（Ridge）： J(θ)=12∑i=1m(fθ(xi)−yi)2+λ∑i=1nθj2 J(\\boldsymbol{\\theta})=\\frac{1}{2}\\sum_{i=1}^m(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}^i)-y^i)^2+\\lambda\\sum_{i=1}^n\\theta_j^2 J(θ)=21​i=1∑m​(fθ​(xi)−yi)2+λi=1∑n​θj2​ 其中 λ\\lambdaλ 为正则化力度，用来控制正则项的惩罚力度，要求 λ>0\\lambda>0λ>0 。 L1 的最优解是稀疏的，L1 会趋向于产生少量的特征，这些特征的值相对较大，而其他的特征都是 0 或者几乎为 0 值。L2 则会选择更多的特征。与 L1 范数相比，L2 的特征值几乎很少有非常明显的大值，都是一些相对较小的值，甚至接近于 0，但不会像 L1 范数等于 0。L2 范数最优化解出这些小的参数，并非没有好处，因为越小的参数意味着模型越简单，越简单的模型就越不容易产生过拟合现象。而且从参数的分布来看，几乎很少出现突兀的大的峰谷值，它们更多地会呈现缓和平稳的状态，如果没有离群值，那么这种小而平稳的参数分布会使得模型拟合的更好，所以一般情况下，L2 范数往往比 L1 范数表现的更好。 "},"ABTest.html":{"url":"ABTest.html","title":"A/B实验","keywords":"","body":"A/B实验 "},"TimeSeries.html":{"url":"TimeSeries.html","title":"时间序列","keywords":"","body":"import pandas as pd df = pd.read_csv(\"AirPassengers.csv\", parse_dates=[\"Month\"]).rename(columns={\"Month\":\"ds\", \"#Passengers\":\"y\"}) import matplotlib.pyplot as plt plt.close(\"all\") X_train = df[df.ds=\"19580101\"] plt.plot(X_train['ds'], X_train['y']) plt.plot(X_test['ds'], X_test['y']) [] from prophet import Prophet pro = Prophet() pro.fit(X_train) 18:55:40 - cmdstanpy - INFO - Chain [1] start processing 18:55:40 - cmdstanpy - INFO - Chain [1] done processing pred = pro.predict(X_test) df.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ds y 139 1960-08-01 606 140 1960-09-01 508 141 1960-10-01 461 142 1960-11-01 390 143 1960-12-01 432 future = pro.make_future_dataframe(periods=10, freq = 'M') future.tail(20) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ds 98 1957-03-01 99 1957-04-01 100 1957-05-01 101 1957-06-01 102 1957-07-01 103 1957-08-01 104 1957-09-01 105 1957-10-01 106 1957-11-01 107 1957-12-01 108 1957-12-31 109 1958-01-31 110 1958-02-28 111 1958-03-31 112 1958-04-30 113 1958-05-31 114 1958-06-30 115 1958-07-31 116 1958-08-31 117 1958-09-30 df ds y 0 1949-01-01 112 1 1949-02-01 118 2 1949-03-01 132 3 1949-04-01 129 4 1949-05-01 121 ... ... ... 139 1960-08-01 606 140 1960-09-01 508 141 1960-10-01 461 142 1960-11-01 390 143 1960-12-01 432 144 rows × 2 columns "},"DimensionalModeling.html":{"url":"DimensionalModeling.html","title":"维度建模","keywords":"","body":"实际上不太喜欢研究数仓建模相关的理论。感觉像是在看英文语法书，一点也不Geek。但是真看起来，还是有一些要点值得被记录。 数据仓库领域数据建模通常采用维度建模。维度模型将复杂的业务通过事实和维度两个概念进行呈现。 事实通常对应业务过程，就是具体的用户行为或者发生的事件。电商交易中的下单，取消订单，付款，退单等，都是业务过程。 维度通常对应业务过程发生时所处的环境，就是行为或者事件相关的属性。比如下单这个业务过程包含了以下维度：Date（日期），Customer（顾客），Product（产品），Location（地区）等。 维度建模以数据分析作为出发点，为数据分析服务，因此它关注的重点是如何更快的完成需求分析，以及如何较好的提升大规模复杂查询的性能。 事实表 事实表是包含具体业务过程的表。包含与该业务过程有关的维度引用（维度表外键）以及该业务过程的度量（通常是数字类型字段）。 维度引用可以用来后续关联维度表获知更具体的维度信息。 事实表中也有可能不包含所谓的业务过程度量，比如关注一个作者，点赞一个帖子这种业务过程。而下单，通常会包含商品数量以及订单金额这种业务过程的度量。 事实表有三种类型：分别是事务型事实表、周期快照事实表和累积快照事实表，每种事实表都具有不同的特点和适用场景。 事务型事实表 数仓中最常见的就是事务型事实表。用于分析与各业务过程相关的各项统计指标，由于其保存了最细粒度的记录，可以提供最大限度的灵活性，可以支持无法预期的各种细节层次的统计需求。 设计事务事实表时一般可遵循以下四个步骤：选择业务过程→声明粒度→确认维度→确认事实 选择一个个不可拆分的行为事件作为业务过程。例如电商交易中的下单，取消订单，付款，退单等。 声明粒度，一个业务过程也可能用不同的粒度来表示。比如下单可能包含多个商品，一个订单既可以用一行数据来表达，也可以被拆分成多条数据，每一条数据只记录一个商品。通常应尽可能选择最细粒度，以此来应各种细节程度的需求。 确认维度，确定事实表中应该包含哪些维度信息。维度的丰富程度就决定了维度模型能够支持的指标丰富程度。 确定事实。此处的事实一词，指的是每个业务过程的度量值。可以分为： 可加事实：比如订单金额，观看时长 半可加事实：比如库存，余额。可以按照商品或者用户维度进行累加，但是不能按照时间维度进行累加。 不可加事实：比如比率，转化率。不可加事实通常需要转化为可加事实，例如比率可转化为分子和分母。 事务型事实表的不足 理论上事务型事实表可以支撑与各业务过程相关的各种统计粒度的需求。但对于某些特定类型的需求，其逻辑可能会比较复杂，或者效率会比较低下。 存量型指标 例如商品库存，账户余额等。以账户余额为例，事务型事实表记录的是用户账户每一笔支出和收入。假定现有一个需求，要求统计截至当日的各用户账户的余额。则需要扫描全量用户有史以来的全表数据聚合才能得到统计结果。可以看到事务型事实表对于这个需求而言并不是一个好的方案。 多事务关联统计 例如，现需要统计最近30天，用户下单到支付的时间间隔的平均值。统计思路应该是找到下单事务事实表和支付事务事实表，过滤出最近30天的记录，然后按照订单id对两张事实表进行关联，之后用支付时间减去下单时间，然后再求平均值。 逻辑上虽然并不复杂，但是其效率较低，应为下单事务事实表和支付事务事实表均为大表，大表join大表的操作应尽量避免。 为了解决以上两个问题，引入周期快照事实表和累积快照事实表。 周期快照事实表 周期快照事实表以具有规律性的、可预见的时间间隔来记录事实，主要用于分析一些存量型（例如商品库存，账户余额）或者状态型（空气温度，行驶速度）指标。 对于商品库存、账户余额这些存量型指标，业务系统中通常就会计算并保存最新结果，所以定期同步一份全量数据到数据仓库，构建周期型快照事实表，就能轻松应对此类统计需求，而无需再对事务型事实表中大量的历史记录进行聚合了。 对于空气温度、行驶速度这些状态型指标，由于它们的值往往是连续的，我们无法捕获其变动的原子事务操作，所以无法使用事务型事实表统计此类需求。而只能定期对其进行采样，构建周期型快照事实表。 周期快照事实表设计的时候除了要考虑事务型事实表遵循的4个步骤，另外还需要考虑采样周期。通常采样周期选择”天“，以一天为周期进行快照。当然也有按周，按月，按季度的周期快照。 周期快照事实表和事务型事实表的一个关键区别在于密度。事务事实表本质是稀疏的，当天只有有业务过程发生才会记录相关的数据。而对于周期快照事实表，实际上是稠密的，即使当天没有业务过程发生，仍然会全量记录。比如余额数据，即使用户当天并未有收入或者支出相关的行为发生，周期快照事实表仍然会记录一条数据，即使是与前一天数据完全相同。 注意事项： 事务与快照成对设计 通常在数仓维度建模的时候，为了更好的满足业务过程度量值的分析，往往要求事务型事实表和周期快照事实表成对设计。周期快照事实表主要是为了满足存量或状态型数据分析任务。 附加事实 快照事实表在确定状态度量时， 一般都是保存采样周期结束时的状态度量。但是也有分析需求需要关注上一个采样周期结束时的状态度量，而又不愿意多次使用快照事实表，因此一般在设计周期快照事实表时会附加一些上一个采样周期的状态度量。 累积快照事实表 累计快照事实表是基于一个业务流程中的多个关键业务过程联合处理而构建的事实表，如交易流程中的下单、支付、发货、确认收货业务过程。 累积快照事实表通常具有多个日期字段，每个日期对应业务流程中的一个关键业务过程。 订单ID 用户ID 下单日期 支付日期 发货日期 确认收货日期 订单金额 支付金额 1001 0001 2022-01-01 2022-01-02 2022-01-03 2022-01-04 2000 2000 累积快照事实表主要用于分析业务过程之间的时间间隔等需求。例如前文提到的用户下单到支付的平均时间间隔，使用累积快照事实表进行统计，就能避免两个事务事实表的关联操作，从而变得十分简单高效。 事务型事实表和周期快照事实表数据只存在插入操作，但是累计快照事实表不仅存在插入操作还存在更新操作。 在实际应用中，业务流程中所包含的业务过程可能并不固定，甚至相当复杂。仍以下单为例，可能并非是下单→支付→发货→收货这么简单直接。中间也可能包含退款、申诉、取消订单等等一些不确定的业务过程。所以在设计和使用累计快照事实表的时候需要格外小心。 物理实现 第一种方式是全量表的形式。此全量表一般为日期分区表，每天的分区存储昨天的全量数据和当天的增量数据合并的结果，保证每条记录的状态最新。此种方式适用于全量数据较少的情况。如果数据量很大，此全量表数据量不断膨胀，存储了大量永远不再更新的历史数据，对ETL 和分析统计性能影响较大。 第二种方式是全量表的变化形式。此种方式主要针对事实表数据量很大的情况。较短生命周期的业务实体一般从产生到消亡都有一定的时间间隔，可以测算此时间间隔，或者根据商业用户的需求确定一个相对较大的时间间隔。比如针对交易订单，我们以200 天作为订单从产生到消亡的最大间隔。设计最近200 天的交易订单累积快照事实表，每天的分区存储最近200 天的交易订单；而200 天之前的订单则按照gmt_create 创建分区存储在归档表中。此方式存在的一个问题是200 天的全量表根据商业需求需要保留多天的分区数据，而由于数据量较大，存储消耗较大。 第三种方式是以业务实体的结束时间分区。每天的分区存放当天结束的数据，设计一个时间非常大的分区，比如3000-12-31 ，存放截至当前未结束的数据。由于每天将当天结束的数据归档至当天分区中，时间非常大的分区数据量不会很大， ETL 性能较好；并且无存储的浪费， 对于业务实体的某具体实例，在该表的全量数据中唯一。比如对于交易订单，在交易累积快照事实表中唯一。 针对第三种方式，可能存在极特殊情况，即业务系统无法标识业务实体的结束时间： 使用相关业务系统的业务实体的结束标志作为此业务系统的结束标志。比如针对物流订单，可以使用交易订单。理论上， 交易订单完结了，则物流订单已经完结。 和前端业务系统确定口径或使用前端归档策略。累积快照事实表针对业务实体一般是具有较短生命周期的，和前端业务系统确定口径，确定从业务实体的产生到消亡的最大间隔。 维度表 规范化与反规范化 规范化是指使用一系列范式设计数据库的过程，其目的是减少数据冗余，增强数据的一致性。通常情况下，规范化之后，一张表的字段会拆分到多张表。 反规范化是指将多张表的数据冗余到一张表，其目的是减少join操作，提高查询性能。在设计维度表时，如果对其进行规范化，得到的维度模型称为雪花模型，如果对其进行反规范化，得到的模型称为星型模型。 数据仓库系统的主要目的是用于数据分析和统计，所以是否方便用户进行统计分析决定了模型的优劣。采用雪花模型，用户在统计分析的过程中需要大量的关联操作，使用复杂度高，同时查询性能很差，而采用星型模型，则方便、易用且性能好。所以出于易用性和性能的考虑，维度表在设计时一般存在大量反规范化。 维度变化 维度属性通常不是静态的，而是会随时间变化的，数据仓库的一个重要特点就是反映历史的变化，所以如何保存维度的历史状态是维度设计的重要工作之一。保存维度数据的历史状态，通常有以下两种做法，分别是全量快照表和拉链表。 全量快照表 离线数据仓库的计算周期通常为每天一次，所以可以每天保存一份全量的维度数据。这种方式的优点和缺点都很明显。优点是简单而有效，开发和维护成本低，且方便理解和使用。缺点是浪费存储空间，尤其是当数据的变化比例比较低时。 拉链表 拉链表，记录每条信息的生命周期，一旦一条记录的生命周期结束，就重新开始一条新的记录，并把当前日期放入生效开始日期。如果当前信息至今有效，在生效结束日期中填入一个极大值（如9999-12-31 ）。 用户ID 姓名 手机号码 开始日期 结束日期 0001 李四 136**1111 2022-01-01 2022-01-02 0001 李四 137**2222 2022-01-03 2022-01-09 0001 李四 138**1234 2022-01-10 9999-12-31 拉链表适合于：数据会发生变化，但是变化频率并不高的维度（即：缓慢变化维） 。比如：用户信息会发生变化，但是每天变化的比例不高。如果数据量有一定规模，按照每日全量的方式保存效率很低。 比如：1亿用户*365天，每天做一份全量用户信息表效率就会很低。 拉链表在具体使用过程中可以通过：生效开始日期=某个日期 ，得到某个时间点的数据全量切片： SELECT * FROM user_info WHERE start_date = '2022-01-01' "},"UserBehaviorsAnalysisPlatform.html":{"url":"UserBehaviorsAnalysisPlatform.html","title":"用户行为分析平台","keywords":"","body":"用户行为分析平台的出现完全是为了满足产品研发团队对数据分析简单、灵活、高频、快速的实际需求，直接面向数据驱动、增长营销、智能运营的集成数据分析平台。 相比传统数仓复杂的数据分层和数据模型，用户行为分析平台建模的思想虽然也源自维度建模，但将整个数据模型进行了极大的简化。降低了数据使用者对数据的理解门槛。在数据质量和数据实时性上也因为模型本身的简洁得到了有效的保证。 相比传统BI，数据源相对分散，数据使用过于灵活，用户行为分析平台统一了数据来源（统一的用户行为数据上报），根据实际产品运营研发团队对数据分析的要求提炼出了数据的特定使用模式（事件分析，留存分析，漏斗分析，分布分析，路径分析等等）。源自产品运营研发团队实际工作的分析方法，自然使得产品运营研发人员在使用数据时更加的得心应手，能够将目光聚焦于具体数据分析和使用本身。 用户行为分析平台从数据上报到数据可用可以做到分钟级。数据虽然并非实时，但是对于满足产品运营研发团队的数据分析已经足够了，完全满足数据驱动的要求。对于分析的场景，其实完全没有必要追求实时，够用才是最好的，没必要去承担实时系统的高成本。另一方面基于离线数仓的天级数据对于一个数据驱动的团队来说又完全不够。量变引起质变，只有足够快的出图速度才能达到连贯的数据思维。 用户行为分析平台完全基于用户行为数据的上报，分析的也是用户行为数据。缺失业务数据库中的数据。于是针对一些业务数据库中的数据无法有效分析。不过不断进化的用户行为分析平台允许用户自行上报一些维表数据补充部分业务数据的缺失。但仍然的，一些业务数据库相关的累计数据或者状态数据，分析起来仍然不方便。但是剩下这些搞不定的分析真的不多了。只需要稍微再针对这些数据做一些BI报表即可。 用户行为分析平台尽可能用最少的资源最低的成本最简单的逻辑最快的速度满足产品运营研发团队最多的数据分析需求。让团队中的每个人都能轻松玩转数据。这就是它巨大的价值。 对于中小公司，如果开始考虑搭建自己的数据体系，务必优先考虑搭建用户行为分析平台。对于中小公司的业务体量而言，真正可以称之为大数据的可能只有用户行为数据，业务数据库中的数据总体来说不会太大，这就更凸显了用户行为分析平台的价值。市面上也有着一批优秀的第三方公司提供基于SaaS或者私有化部署的用户行为分析平台。国内的比如：GrowingIO，神策以及字节跳动火山引擎的DataFinder；国外的比如：Mixpanel。虽然Google Analytics和Firebase Analytics是来自于巨头的产品，但其实做的不太好，所以也不太推荐。 用户行为分析平台的数据模型 用户行为分析平台的数据模型并没有统一的命名，有些称之为事件模型，有些称之为事件-用户模型。但核心要素就是用户（User）和事件（Event）。对应着基于用户标识的行为事件埋点上报，以及最终分析性数据库中存储的事件宽表和用户宽表。这样一个数据模型也决定了最终的数据分析的核心就是围绕着用户和事件展开的。 显然的，就像一句完整的话是由主-谓-宾构成的，用户产生的行为事件必然包含一个目标，将其称之为物品（Item）。早期用户行为分析平台并未太关注物品（Item）。导致物品相关的维度分析存在缺失。但随着用户行为分析平台不断的发展完善，物品（Item）也被纳入其中，成为数据模型的重要补充。在用户行为分析平台中与之对应的是相关的物品（Item）维度表。 目前，一个完善的用户行为分析平台的数据模型就是由：用户（User）-- 事件（Event）-- 物品（Item），这三个核心要素组成。随着用户行为事件的上报，系统尽可能快速的将上报事件数据解析为对应的事件表、用户表以及物品维度表的字段，供下游数据分析使用。同时，用户表中的一些属性字段以及一些物品维度表也可以通过专门的同步机制，从业务数据库中同步相应的数据，比如用户等级表，商品信息表，帖子状态表等等。用户行为分析平台通常由一张事件表、一张用户表和N张物品维度表组成。 用户行为分析平台数据上报一般采用json格式，一条典型的数据上报格式如下：event_name为事件名称，local_time_ms为事件发生的时间；user为触发事件的用户以及相应的用户信息，用于关联用户表；params为事件包含的属性信息；item为与事件相关的物品信息，用于关联物品（Item）维度表。 { \"user\": { \"user_id\": \"0001\", \"device_id\": \"asdf123\", \"app_version\": \"9.13.5\", \"network_type\": \"wifi\", \"platform\": \"Android\" }, \"params\": { \"author_id\": \"809654\", \"article_type\": \"video\", \"view_list\": \"main\" }, \"items\":{ \"article\": [{ \"id\": \"789\" }] }, \"event_name\": \"like_article\", \"app_id\": \"4567\", \"app_name\": \"buybuy\", \"local_time_ms\": 1671593592638 } 以上示例为一条给视频内容点赞的用户行为数据。通过这条数据上报我们可以多维度的分析”内容点赞“这个事件，不同网络环境下给视频内容的点赞情况，比如不同内容类型的点赞情况等等。但一旦我们要分析内容的点赞总量或者我们想分析帖子状态信息（审核是否通过），仅仅通过行为数据的上报是很困难的，这时就要关联物品维度表中相关的内容属性。 用户行为分析平台的系统架构 下图展示的是神策的用户行为分析平台系统架构，以此为例，可以看到用户行为分析平台的整体技术架构大致可以分为以下几个部分： 数据采集子系统。这块儿以前端后端的数据采集SDK为主，用来提供用户行为数据的采集功能和上报功能。因为用户行为分析平台天然包含了数据采集模块儿，有时候会有同学直接把用户行为分析平台叫做数据采集工具。 数据接入子模块。用户行为数据通过Http请求进行上报，由Nginx作为数据接收端将用户行为数据写入日志。用Flume或者Logstash这类工具从日志读取数据并写入消息队列Kafka，供下游读取并处理。 数据导入与存储模块。Spark或者Flink从Kafka中读取数据进行ETL之后写入存储。神策的OLAP模块使用的是Parquet+Kudu+Impala的模式，存储和查询分离。GrowingIO早期是使用HBase自研了一套OLAP引擎，后面重构之后采用的是ClickHouse。字节火山引擎的DataFinder从一开始就是使用自研的ByteHouse作为OLAP引擎，ByteHouse是基于ClickHouse的魔改版本。 最后就是各种数据应用模块。包括数据可视化分析，智能运营等等。 整个系统架构摒弃了传统数仓的多层分层架构，数据接入之后以最短路径进入OLAP存储成大宽表。提升数据实时性的同时也降低了中间环节出错的可能性，尽量提升数据质量。 用户行为分析平台的数据能力 用户细查 上报的是用户行为数据，自然地，如果按照时间线展开每个用户的行为事件，就能够清晰的还原用户使用网站或者应用的行为细节以及所处的环境。能够帮助理解特定用的户行为以及定位线上问题。 用户细查需要包含的最基本功能就是能够按照用户ID或者设备ID搜索到该用户，并且展示该用户的基本用户属性信息。同时能够查询特定时间范围内的该用户的行为事件。如果做的更细致一些，可以提供更丰富的过滤条件，比如：可以选择需要包含的事件或者不想要包含的事件。 用户分群 用户分群是精细化运营的重要支撑手段之一，产品运营同学可以基于细分后的用户群开展用户画像、精细化分析、精准触达等工作。用户行为分析平台利用上报的用户行为数据以及用户属性相关的数据，可以非常容易的根据具体的行为事件和用户属性创建规则将用户进行分群，并固化下来。固化下来的用户分群不仅可以选择具体的用户进行用户细查，也可以作为目标用户进行事件分析，留存分析，漏斗分析等一系列的数据分析工作。 用户分群也可以通过直接上传用户ID列表文件形成分群，让分群方式更加灵活。 事件分析 事件分析应该是用户行为分析平台使用最广泛的功能。事件分析是针对特定事件特定目标用户的某一指标的分析。事件分析的大致步骤： 选择想要分析的事件。例如：”内容点赞“事件。 选择想要分析的指标。例如：”内容点赞“事件量，”内容点赞“事件人数，”内容点赞“事件人均次数等等。对于一些包含数值信息的事件，比如“下单”事件，还可以选择按照“下单金额”求和，求平均，求最大最小值等等。 对事件增加过滤条件：对事件属性或者属性的组合进行相应的过滤，选择想要分析的特定属性条件。比如：通过过滤选择”内容类型”为“视频”，“内容出现位置”为“推荐列表”的”内容点赞“事件。 选择目标用户：比如：目标用户为“新用户”或者使用其它用户属性来筛选。甚至可以做到更为复杂的用户圈选，比如：“过去30天访问过7天以上并且关注了3个以上up主的用户” 针对以上这样一个条件组合生成最终指标的可视化图表。实际中还可以做多个事件指标的计算，不同属性维度的对比展示，不同用户群组的对比展示等等。 留存分析 留存基本是产品运营最关注的指标之一。留存分析可以通过方便的选择起始事件和回访事件生成针对目标用户成留存曲线和留存率变化图。 留存曲线是指针对一群特定的目标用户由其次日留存，2日留存，3日留存，4日留存……构成的随时间逐渐衰减的曲线。从留存率曲线中可以观察一群用户留存衰减的情况。 留存率变化图是指由每天用户的次日（也可以是7日，14日等）留存构成的次日留存率曲线。从留存率变化图中我们可以观察随着时间的推移次日留存变化的曲线。是产品运营做增长最重要的图表之一。 我们通常意义下的用户留存，指的是全量用户针对全量事件的留存，起始事件是任意事件，回访事件也是任意事件。新用户留存，就是把目标用户设定为新用户，起始事件和回访事件设置为任意事件的留存。 之所以需要能够任意设置留存的起始事件和回访事件，是因为我们同样会关注某一些特定事件的留存。比如：买过了再买的复购率，比如某些功能模块的留存率，来了又来。 留存分析同样可以选择需要的目标用户群进行留存分析。方便产品运营进行精细化的用户运营。 留存分析有可能还包含一个关联属性的设置。关联属性用于让起始事件和回访事件的某个属性值保持一致，常用于活动名称、页面标题或商品名称等。 漏斗分析 漏斗分析主要用来分析用户在流程中每一步的转化情况。典型的分析场景比如：我们会分析新用户打开APP到注册登录进入主页面这一重要留存每个步骤的转化情况。亦或：电商场景下，用户浏览商品到加入购物车到提交订单完成支付这一系列动作每一步的转化情况。 漏斗分析首先要设计漏斗：根据需要分析的流程选择每一步的转化事件，以及转化周期。转化周期是指事件与事件之间转化的时间间隔。然后同样的，可以选择特定目标用户群，针对特定群体的用户进行转化分析。 漏斗分析最终会给出流程中每一步之间的转化率漏斗，从中可以分析出核心的转化问题存在于哪个环节之中。同时也可以获取到具体流失用户的用户ID，将相应的流失用户生成用户分群，做进一步的流式用户分析。 同样，漏斗分析也可以设置关联属性。 漏斗分析不仅可以分析事件之间的转化率。也可以生成某一转化率随时间变化的转化趋势图，类似留存率变化图。另外，事件转化的时间间隔也可以生成基于转化周期的分布分析图。 分布分析 分布分析指的是事件在整体或某一维度下，按照计算结果划分出一些区间，查看对应人数在各区间内的分布情况。分布分析有很多种类，比如按事件发生频次查看人数分布、按属性值计算结果查看人数分布、按一段时间内累计发生的时长或天数查看人数分布等。 分布分析在使用时通常需要自定义分布区间。自动划分的分布区间往往并不能符合我们实际的分析需要。 分布分析在用户行为分析平台的使用过程中需要注意的是，它一定是针对人的分布。这一点是对数据理解不够深入的产品运营同学在使用时经常忽视的一个问题，总是试图在用户行为分析平台做非人的分布分析。之所以一定是对人的分布，还是因为用户行为分析平台所选择的事件-用户模型所致。 路径分析 路径分析多少有点类似漏斗分析。漏斗分析往往用于分析明确的转化路径，但在一些情况下路径分叉比较多，想要从整体上把控用户的流向，这时候路径分析就会起到作用。而且路径分析展示的是连续的用户行为，可能会出现A事件后接着是B事件，然后又回到A事件的情况。 路径分析第一步是设置起始事件或者终止事件。起始事件用于分析用从这个事件开始用户去到了哪里。终止事件用于分析用户从哪里来到这个事件。 第二步是设置N个想要分析的用户可能流向的事件或者排除不想分析的事件。在探索分析或者对用户流向不清楚的时候，刚开始通常也可以不设置具体的事件，默认包含全部埋点事件。这样就可以有个用户流向的全貌展示，之后再选择关注的事件进行相应分析或者配合漏斗分析。 LTV分析 LTV是指用户生命周期价值(life time value)。通过分析用户从开始进入应用到最终流失全生命周期的消费数据，衡量单个用户的平均价值，进而帮助产品运营了解产品营收情况以及确立拉新成本上限。 LTV的计算规则如下：用户进入应用第n天的人均LTV称为LTVn，LTVn = 新用户同期群在随后n天内花费的总金额 ÷\\div÷ 该同期群用户数。 同期群（cohort）的含义是在规定时间内对具有共同行为特征的用户群。新用户同期群通常指在同一时间段（同一天，或者同一星期等）开始使用应用的新用户的总体。 通过计算第0天，第1天，第2天，第3天……的LTVn绘制成相应的LTV趋势图。LTV趋势图是一个逐步上升然后趋缓最终水平的一条曲线。因为用户进入应用后开始消费，随着消费增加LTV的值越来越大，但是用户也会随着时间逐步流失，直到某天的该新用户同期群的用户全部流失掉，LTV的值将保持恒定不再增加。 同样类似留存分析中的留存率变化图，LTV分析不仅可以展示LTV趋势图，可以展示具体某一个LTVn的LTV对比图，用于分析随时间推移的不同同期群的LTVn的变化情况。 LTV的概念还可以进一步抽象化。不一定特指用户生命周期的消费。LTV计算的分子，可以抽象为同期群的任意数值度量。比如：可以考察用户生命周期人均获得的金币量，用户生命周期人均的阅读量等等。 用户行为分析平台的高级能力 基于以上的分析能力，产品运营研发团队能够解决绝大多数数据分析需求。但这并未发挥出用户行为分析平台的全部潜力。 SQL分析 上面提到的基本的分析能力都是确定套路的分析方法。虽然足够高频，但也存在不够灵活的缺陷。用户行为分析平台的事件-用户模型本身是由事件宽表、用户宽表以及若干的维度表组成的。开放基于这些表的SQL查询，自然能够获得更灵活的分析能力。 可能普通产品运营同学不具备足够的SQL能力，SQL分析更多的是提供给数据分析师以及研发同学使用。SQL的结果同样也可以固化为相应的分析报表提供给产品运营同学使用。 在实际工作中，数据产品经理或者数据分析师也会经常性的利用SQL分析来分析问题保障数据质量。毕竟，只有通过SQL分析系才能完整明确的看到数据表中真实存储的原始数据。不断提升数据质量才是业务部门愿意使用数据平台的基础。 智能运营 产品运营在日常工作中通常都会希望通过各种方式触达用户。比如给用户发送短信，推送消息，发放礼券等等。触达的逻辑通常是在特定的时机给满足某些条件的用户进行相应的”推送“。这里有两个关键点，一个是特定时机，一个是满足条件的用户。以往，这样一个逻辑通常需要产品运营提出需求，开发同学写代码来实现。但是基于用户行为分析平台，这样一套逻辑似乎并不需要每次都有开发介入。 所谓满足条件的用户实际上对应着用户行为分析平台的用户分群功能。所谓特定时机可以是特定事件的触发，也可以是特定用户路径，或者是一些组合逻辑。不管怎样，最基本的用户行为事件已经上传进入了用户行为分析平台。用户行为分析平台需要实现一套流程画布系统来编辑触发的逻辑。比如一个简单的逻辑：针对一周内的新用户，如果进入了商品购买页面，就给他们发放一张8折券。一周内的新用户可以利用用户分群功能圈选出来，每天自动更新。进入商品购买页面，直接利用用户行为上报的事件进行触发。最后需要实现发放优惠券的逻辑。 礼券系统应该是一个独立的与其它系统解耦的后端服务系统。对外应该有提供通过RPC或者http的接口调用服务。那么用户行为分析平台应当提供一个推送通道管理的界面，用于注册推送调用的接口，实际上就是我们通常所说的Webhook。针对礼券接口的例子，就应该是在用户行为分析平台的推送通道管理界面注册一个调用礼券发放礼券的接口。推送通道管理的界面可以注册不同的推送通道，比如消息推送，短息推送，私聊消息，弹窗，礼券发放等等。 一旦完成这样一套智能运营系统，运营人员即可自行编辑相关的逻辑，完成自动化的用户触达能力，实现个性化的用户运营。同时极大的降低了开发人员的参与，提升了运营的效率。 数据开放 正如上文提到过的，有些同学会把用户行为分析平台称之为数据采集系统。显然的，能够全量准实时获取到用户行为数据的用户行为分析系统一定是可以对外提供数据的。 用户行为分析平台对外开放数据可以存在于4个不同的阶段： 最原始的数据。用户行为数据上报到用户行为分析系统之后，在ETL之前会位于Kafka消息队列之中。这些都是最原始的未经加工过的数据。如果对外开放Kafka中存储原始数据的Topic的订阅能力，外部系统就可以以最快速度消费到最原始的用户行为数据。 经过ETL之后的数据。用户行为分析系统会对用户上报的行为数据进行ETL处理。经过ETL处理之后未落表之前，这些数据可以推入一个旁路的Kafka消息队列通道。将这个通道对外开发。则外部系统可以获取到相对实时，同时数据质量不错的用户行为数据。 落表之后的数据。用户行为数据最终会落到事件表中。可以直接对外开放事件表的访问权限。当然，落表数据往往数据延时会更大，提供的数据往往也是数据片段，例如若干特定事件。订阅这些特定数据的方式通常也是通过Webhook。 统计数据。产品运营已经在用户行为分析平台上制作了相应的事件分析表，漏斗分析表，分布分析表等等数据表。这些数据表都可以开放给外部系统进行订阅。这些数据的数据量相对较小，可以直接通过Http请求获取。 "},"SQLKeypoints.html":{"url":"SQLKeypoints.html","title":"SQL要点","keywords":"","body":"只包含DML（Data Manipulation Language，数据操纵语言）的内容，不包含DDL（Data Definition Language，数据定义语言）和DCL（Data Control Language，数据控制语言） 基础语法 仅举例说明，不做过多解释 其中Student表为学生信息表，Score表为学生成绩表，建表语句为： CREATE TABLE Student ( s_id VARCHAR(20) COMMENT 'student ID', s_name VARCHAR(20) NOT NULL DEFAULT '' COMMENT 'student name', s_birth VARCHAR(20) NOT NULL DEFAULT '' COMMENT 'student birthday', s_sex VARCHAR(10) NOT NULL DEFAULT '' COMMENT 'student sex', PRIMARY KEY (s_id) ); CREATE TABLE Score ( s_id VARCHAR(20) COMMENT 'student ID', c_id VARCHAR(20) COMMENT 'course ID', s_score INT(3) COMMENT 'student score', PRIMARY KEY (s_id, c_id) ); 以下为包含基础语法的查询语句： select * from Student where s_name like \"李%\"； select s_id, s_score from Score where c_id in (\"0001\", \"0002\"); select count(distinct s_id) from Score where s_score >= 60; select c_id from Score where s_score60; select s_id, s_name, s_birth from Student where year(s_birth) = 2010 limit 10; select * from Student where month(s_birth) = month(now()); # 求年纪 select s_id, s_name, s_birth, floor(timestampdiff(month, s_birth, now())/12) from Student; 进阶语法 条件表达式 case表达式 /* 语法 CASE WHEN THEN WHEN THEN . ELSE END */ # 将s_sex字段的数值转成'male'、'female'和'null'的表示形式 select s_id, s_name, (case s_sex when 1 then \"male\" when 2 then \"female\" else \"uncertain\" end) as gender from Student; # 行转列 select sum(case s_sex when 1 then 1 else 0 end) as \"male\"， sum(case s_sex when 2 then 1 else 0 end) as \"female\" from Student; if表达式 /* 语法 IF(expr1, expr2, expr3) expr1的值为 TRUE，则返回值为expr2 expr1的值为FALSE，则返回值为expr3 */ select if(s_sex = 1, \"male\", \"female\") as gender from Student where s_sex != \"\"; /* 语法 IFNULL(expr1, expr2) 判断expr1是否为NULL： 如果expr1不为空，返回expr1； 如果expr1为空， 返回expr2 */ select ifnull(s_sex, \"uncertain\") as gender from Student; 子查询 子查询指一个查询语句嵌套在另一个查询语句内部的查询，子查询的结果可以作为一个表，也可以作为一个过滤条件 # 作为一个表 # 前10名从低到高排序 select * from (select * from Score where c_id = \"0001\" order by s_score desc limit 10) order by s_score; # 作为一个过滤条件 # 高于平均分 select * from Score where s_score >= (select avg(s_score) from Score); 表关联 UNION UNION 操作符用于合并两个或多个 SELECT 语句的结果集。 UNION 内部的每个 SELECT 语句必须拥有相同数量的列。列也必须拥有相似的数据类型。同时，每个 SELECT 语句中的列的顺序必须相同。 UNION 是去重的，UNION ALL 是不去重的，只能使用一条ORDER BY子句，它必须出现在最后一条SELECT语句之后。 # UNION去重 select u_id from Score where c_id = \"0001\" union select u_id from Score where c_id = \"0002\" order by s_score; # UNION ALL不去重 select u_id from Score where c_id = \"0001\" union all select u_id from Score where c_id = \"0002\" order by s_score; JOIN 常见的7种JOIN方式见下图： # 列出学生ID、学生姓名的同时也把对应学生的分数列出来 select st.s_id, st.s_name, sc.c_id, sc.s_score from Student as st left join Score as sc on st.s_id = sc.s_id; s_id s_name c_id s_score 01 李四 03 99 01 李四 02 90 01 李四 01 80 02 王二 03 80 02 王二 02 60 查询语句运行结果中需要注意，Student表中原本的一条数据被重复了N次 窗口函数 窗口函数的语法为： OVER ([PARTITION BY ] ORDER BY ) MySQL中只有8.0版本以上才支持窗口函数 # 按照c_id分组，按照s_score排序 select s_id, c_id, s_score, rank() over (partition by c_id order by s_score desc) as ranking from Score; s_id c_id s_score ranking 01 01 80 1 03 01 80 1 05 01 76 3 02 01 70 4 04 01 50 5 06 01 31 6 01 02 90 1 02 02 60 2 04 02 30 3 01 03 99 1 排序函数 三种不同排序函数：rank()，dense_rank()，row_number() # 按成绩排序，展示三种不同排序函数的差异 select s_id, c_id, s_score, rank() over (order by s_score desc) as ranking, dense_rank() over (order by s_score desc) as denserank, row_number() over (order by s_score desc) as rownumber from Score; s_id c_id s_score ranking denserank rownumber 01 03 99 1 1 1 07 03 98 2 2 2 01 02 90 3 3 3 01 01 80 4 4 4 02 03 80 4 4 5 03 01 80 4 4 6 05 01 76 7 5 7 02 01 70 8 6 8 rank()：有相同分数时排名相同，且会占用后一名的位置，一些名次不存在，存在名次跳跃 dense_rank()：有相同分数时排名相同，但不会占用后一名的位置，名次是连续的 row_number()：有相同分数时排名也会不相同，名次是连续的 聚合函数 在窗口函数中使用聚合函数（sum，avg，count，max，min）的含义为：根据order by字段的排序，求”到目前为止“的聚合值，比如sum就是累加的含义。语言描述起来有点困难，直接看查询语句的结果理解起来就不难了。 按照s_score字段降序排列。值得注意的是：相同分数count值相同。因为降序，最大分数值出现在第一位，所以每一行的max都等。 #只有order by，没有partition的查询 select s_id, c_id, s_score , sum(s_score) over (order by s_score desc) as current_sum, avg(s_score) over (order by s_score desc) as current_avg, count(s_score) over (order by s_score desc) as count_, max(s_score) over (order by s_score desc) as max_score, min(s_score) over (order by s_score desc) as min_score from Score s_id c_id s_score current_sum current_avg count_ max_score min_score 01 03 99 99 99.0000 1 99 99 07 03 98 197 98.5000 2 99 98 01 02 90 287 95.6667 3 99 90 01 01 80 527 87.8333 6 99 80 02 03 80 527 87.8333 6 99 80 03 01 80 527 87.8333 6 99 80 05 01 76 603 86.1429 7 99 76 02 01 70 673 84.1250 8 99 70 02 02 60 733 81.4444 9 99 60 04 01 50 783 78.3000 10 99 50 06 03 34 817 74.2727 11 99 34 06 01 31 848 70.6667 12 99 31 04 02 30 878 67.5385 13 99 30 04 03 20 898 64.1429 14 99 20 先按照c_id字段分组，分组之后按照s_score升序排列。注意所有的聚合操作都是在分组内进行，组与组之间没有联系。 #既有order by，又有partition的查询 select s_id, c_id, s_score , sum(s_score) over (partition by c_id order by s_score) as current_sum, avg(s_score) over (partition by c_id order by s_score) as current_avg, count(s_score) over (partition by c_id order by s_score) as count_, max(s_score) over (partition by c_id order by s_score) as max_score, min(s_score) over (partition by c_id order by s_score) as min_score from Score s_id c_id s_score current_sum current_avg count_ max_score min_score 04 01 50 81 40.5000 2 50 31 06 01 31 31 31.0000 1 31 31 02 01 70 151 50.3333 3 70 31 05 01 76 227 56.7500 4 76 31 01 01 80 387 64.5000 6 80 31 03 01 80 387 64.5000 6 80 31 04 02 30 30 30.0000 1 30 30 02 02 60 90 45.0000 2 60 30 01 02 90 180 60.0000 3 90 30 04 03 20 20 20.0000 1 20 20 06 03 34 54 27.0000 2 34 20 02 03 80 134 44.6667 3 80 20 07 03 98 232 58.0000 4 98 20 01 03 99 331 66.2000 5 99 20 滑动窗口 # 对前n行（包括当前行），求的值 OVER (ORDER BY ROWS n PRECEDING) # 对前n行到后n行，求的值 OVER (ORDER BY ROWS BETWEEN n PRECEDING AND n FOLLOWING) # UNBOUNDED PRECEDING表示分组内的第一行，UNBOUNDED FOLLOWING表示分组内的最后一行 OVER (PARTITIOIN BY ORDER BY ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) "},"SQLCases.html":{"url":"SQLCases.html","title":"SQL案例","keywords":"","body":"在互联网行业，会有一些常用的数据指标和常见的数据分析方法包括：事件分析，留存分析，漏斗分析，分布分析，路径分析，LTV分析等。在做数据分析，数据报表时，通常用SQL来计算这些数据指标或者实现这些数据分析方法。 下面模拟一个简单的用户行为事件表。基于这个事件表，用SQL来实现常见的数据指标和数据分析方法。 # 用户行为事件表 CREATE TABLE Events ( user_id VARCHAR(20) NOT NULL DEFAULT '', event_name VARCHAR(20) NOT NULL DEFAULT '', event_time VARCHAR(20) NOT NULL DEFAULT '' ); 事件量，用户量 一段时间内的事件量。在真实的大数据分析场景，因为数据量比较大，通常一定是需要限定时间范围的。而且时间字段通常用来作为分区表的分区字段，比如：按天分区。 一段时间内的用户量就是总事件量按用户做去重处理。 # 一段时间内的总事件量 SELECT count(event_name) FROM Events WHERE event_time >= '2022-01-01 00:00:00' AND event_time = '2022-01-01 00:00:00' AND event_time DAU，UV，PV 数据分析中经常需要按天展示DAU，在Web侧通常称为UV。或者按天展示PV，APP侧不太讲PV这个概念。 # 一段时间内特定事件的按天DAU SELECT date(event_time) AS day, COUNT(DISTINCT user_id) FROM Events WHERE event_name = \"play\" AND event_time >= \"2022-01-01 00:00:00\" AND event_time = \"2022-01-01 00:00:00\" AND event_time 以上SQL直接通过GROUP BY按天展示数据的一个问题是：如果某一天没有数据，那么在SQL查询的结果中就不会出现该天的数据。但因为最终的数据是要拿来做展示的，所以如果某天没有数据，也需要增加该日期的数据行，用0来补充。具体到项目中，如果用来展示数据的 BI 能够兼容缺失日期的数据当然最好。如果 BI 无法兼容，用于请求数据库或者数仓的后端服务也可以兼容该逻辑。如果后端服务也不愿意兼容，最后只能还是利用SQL实现，不过SQL实现这个需求并不简单且不直观。 SQL实现该需求需要分两步进行： 生成按行的连续日期。根据所用数据库或者数仓的不同，应该有不同的实现方式。 按行的连续日期和原数据做LEFT JOIN。利用LEFT JOIN时右表某行不存在则用NULL替代的特性，将NULL替换为0即可。 # 给定时间范围，生成按行的连续日期 SELECT @i := @i + 1 AS 'NO', DATE( DATE_ADD('2022-01-01',INTERVAL @i DAY)) as date # 开始时间 FROM Events, # 随便给表，需要保证表的行数比需要的天数多 (SELECT @i := - 1) t # 设置初始值为-1的变量i WHERE @i 以时间范围内的按行连续日期表为基础，结合之前查询DAU的数据，进行联表查询，获得每个日期的DAU数据。观察一下数据查询结果中，2022-01-08~2022-01-10三天的DAU数据均为0。 SELECT t1.day, ifnull(t2.DAU, 0) as DAU FROM (SELECT @i := @i + 1 AS NO, DATE( DATE_ADD( '2022-01-01', INTERVAL @i DAY )) AS day # 开始时间 FROM Events, ( SELECT @i := - 1 ) t WHERE @i = \"2022-01-01 00:00:00\" AND event_time day DAU 2022-01-01 3 2022-01-02 3 2022-01-03 3 2022-01-04 1 2022-01-05 2 2022-01-06 2 2022-01-07 2 2022-01-08 0 2022-01-09 0 2022-01-10 0 留存分析，留存率 A日期的活跃用户在B日期的留存率计算可以分为一下步骤： 计算A日期活跃的用户ID 计算A日期活跃的用户哪些在B日期也活跃过 将以上两步的计算结果做join，算出留存率 对于复杂的SQL，可读性通常很差。可以利用WITH AS来相对提升可读性。t1用来计算起始天活跃的用户 ，t2用来计算起始天用户在留存天留存的用户。 WITH t1 AS ( # 计算起始天的活跃用户 SELECT DISTINCT user_id FROM Events WHERE date(event_time) = '2022-01-01' ), t2 AS ( # 计算留存天的活跃用户 SELECT DISTINCT user_id FROM Events WHERE user_id IN (SELECT * FROM t1) AND date(event_time) = '2022-01-02' ) SELECT COUNT(t2.user_id) / COUNT(t1.user_id) AS retention # COUNT不会包含NULL FROM t1 LEFT JOIN t2 ON t1.user_id = t2.user_id; 以上SQL只是求得了某一日期的次日留存率。但在数据分析中，通常我们希望看到的是如下的留存曲线。留存曲线描绘了特定日期之后一段时间每天的留存数据。我们需要用SQL来实现绘制该留存曲线所需要的数据。 和求单日留存率最大的不同点在于需要同时对用户ID和日期做GROUP BY。LEFT JOIN的主要目的是为了求时间差。以下SQL展示了如何求得时间差diff这一中间结果。 WITH t1 AS ( SELECT user_id, date(event_time) AS day FROM Events WHERE date(event_time) = '2022-01-01' GROUP BY user_id, date(event_time) ), t2 AS ( SELECT user_id, date(event_time) AS day FROM Events WHERE event_time >= \"2022-01-01 00:00:00\" AND event_time user_id start_day retent_day diff 01 2022-01-01 2022-01-07 6 01 2022-01-01 2022-01-05 4 01 2022-01-01 2022-01-03 2 01 2022-01-01 2022-01-02 1 01 2022-01-01 2022-01-01 0 03 2022-01-01 2022-01-07 6 03 2022-01-01 2022-01-06 5 03 2022-01-01 2022-01-05 4 03 2022-01-01 2022-01-01 0 04 2022-01-01 2022-01-03 2 04 2022-01-01 2022-01-02 1 04 2022-01-01 2022-01-01 0 以上结果中retent_day表示该用户有留存的日期，diff就是留存日期到起始日期的差值，1就是次日留存，2就2日留存，3就是3日留存，以此类推。要求留人数存就只需要按照diff进行GROUP BY求COUNT。如果要求留存率就还要增加一步。而且这里同样存在某天如果是留存人数为0，则结果中不存在该天的问题，这里就不再赘述。 SELECT datediff(t2.day, t1.day) AS diff, COUNT(*) FROM t1 LEFT JOIN t2 ON t1.user_id = t2.user_id GROUP BY diff ORDER BY diff; diff count(*) 0 3 1 2 2 2 4 2 5 1 6 2 漏斗分析，转化率 漏斗分析中的转化率其实和留存率有相似之处。留存率可以看成是相对于日期的转化率，而漏斗分析中的转化率可以看成是相对于具体事件的转化率。但是通常在漏斗分析时，我们会对转化的时间做出限制，比如转化必须发生在1小时之内。 选取Events表中的login、play、like三个事件做转化分析。用Events表和自身进行JOIN，筛选其中间隔小于1小时的两个不相同的事件。每个转化，同一个用户只记录一次。 SELECT event1.event_name AS event1, event2.event_name AS event2, COUNT(DISTINCT event1.user_id) AS num_conversions FROM Events event1 INNER JOIN Events event2 ON event1.event_name <> event2.event_name AND event1.user_id = event2.user_id AND TIMESTAMPDIFF(SECOND, event1.event_time, event2.event_time) 0 WHERE event1.event_name IN ('login', 'play', 'like') AND event2.event_name IN ('play', 'like') GROUP BY event1.event_name, event2.event_name event1 event2 num_conversions login like 3 login play 4 play like 3 以上可以看出login到like发生了3人次的转化，login到play发生了4次转化，play到like发生了3次转化。如果最终要求出转化率，还要先把起始事件的人数求出。对以上SQL做一点改动。 SELECT event1.event_name AS event1, event2.event_name AS event2, COUNT(DISTINCT event1.user_id) AS num_conversions, CAST(COUNT(DISTINCT event1.user_id) AS FLOAT) / (SELECT COUNT(DISTINCT user_id) FROM Events AS e WHERE e.event_name = event1.event_name) AS conversion_rate FROM Events AS event1 INNER JOIN Events AS event2 ON event1.event_name <> event2.event_name AND event1.user_id = event2.user_id AND TIMESTAMPDIFF(SECOND, event1.event_time, event2.event_time) 0 where event1.event_name in ('login', 'play', 'like') and event2.event_name in ('play', 'like') GROUP BY event1.event_name, event2.event_name event1 event2 num_conversions conversion_rate login like 3 0.75 login play 4 1 play like 3 0.75 用户分群 在实际数据分析中，我们通常需要按照一定的用户属性或者行为特征对用户进行分群分析。一道经典的SQL面试题：求连续登录3天的用户ID，就是一个典型的用户分群案例。用户连续登录N天可以反应用户某种程度的活跃。用户连续N次做某件事情也会经常被应用于运营活动中。 以下就尝试用SQL求解连续登录3天的用户ID： 将事件时间event_time转换为事件天event_day。 利用窗口函数，按用户ID分组，按照事件天event_day排序。给每一天打上一个排序后的行号rn。 将事件天event_day减去对应的行号rn得到一个flag_day。如果是连续登录的，则flag_day应该是同一天。 按照用户ID和flag_day分组，如果同一天出现大于等3，则表示该用户从start_day开始连续登录了3天及以上。 WITH t1 AS ( # 将事件时间event_time转换为事件天event_day SELECT user_id, date(event_time) AS event_day FROM Events GROUP BY user_id, date(event_time) ), t2 AS ( # 利用窗口函数，按用户ID分组，按照事件天event_day排序。给每一天打上一个排序后的行号rn SELECT user_id, event_day, row_number() OVER (PARTITION BY user_id ORDER BY event_day) AS rn FROM t1 ), t3 AS ( # 将事件天event_day减去对应的行号rn得到一个flag_day。如果是连续登录的，则flag_day应该是同一天 SELECT *, date_sub(event_day, INTERVAL rn DAY) AS flag_day FROM t2 ) # 按照用户ID和flag_day分组，如果同一天出现大于等3，则表示该用户从start_day开始连续登录了3天及以上 SELECT date_add(flag_day, INTERVAL 1 DAY) AS start_day, user_id FROM t3 GROUP BY user_id, flag_day HAVING COUNT(user_id) >= 3; start_day user_id 2022-01-01 01 2022-01-02 02 2022-01-04 03 2022-01-01 04 附 以上内容所需的数据库表结构和数据库表数据如下： CREATE TABLE Events( user_id VARCHAR(20) NOT NULL DEFAULT '', event_name VARCHAR(20) NOT NULL DEFAULT '', event_time VARCHAR(20) NOT NULL DEFAULT '' ); insert into Events values('01', 'login', '2022-01-01 11:50:31'); insert into Events values('01', 'click', '2022-01-01 11:51:10'); insert into Events values('01', 'view', '2022-01-01 11:55:59'); insert into Events values('01', 'play', '2022-01-01 12:01:03'); insert into Events values('01', 'like', '2022-01-01 12:06:42'); insert into Events values('01', 'login', '2022-01-02 15:50:31'); insert into Events values('01', 'play', '2022-01-02 16:01:03'); insert into Events values('01', 'like', '2022-01-02 16:06:42'); insert into Events values('01', 'login', '2022-01-03 21:50:31'); insert into Events values('01', 'play', '2022-01-03 22:01:03'); insert into Events values('01', 'login', '2022-01-05 08:50:31'); insert into Events values('01', 'view', '2022-01-05 08:55:59'); insert into Events values('01', 'play', '2022-01-05 09:01:03'); insert into Events values('01', 'like', '2022-01-05 09:06:42'); insert into Events values('01', 'logout', '2022-01-05 09:51:10'); insert into Events values('01', 'login', '2022-01-07 22:50:31'); insert into Events values('02', 'login', '2022-01-02 09:10:31'); insert into Events values('02', 'play', '2022-01-02 09:15:03'); insert into Events values('02', 'like', '2022-01-02 09:20:55'); insert into Events values('02', 'login', '2022-01-03 13:15:31'); insert into Events values('02', 'play', '2022-01-03 13:20:03'); insert into Events values('02', 'logout', '2022-01-03 13:25:55'); insert into Events values('02', 'login', '2022-01-04 01:50:31'); insert into Events values('02', 'click', '2022-01-04 02:51:10'); insert into Events values('02', 'view', '2022-01-04 02:55:59'); insert into Events values('02', 'play', '2022-01-04 03:01:03'); insert into Events values('02', 'like', '2022-01-04 03:06:42'); insert into Events values('02', 'login', '2022-01-06 18:50:31'); insert into Events values('02', 'view', '2022-01-06 18:55:01'); insert into Events values('02', 'like', '2022-01-06 18:59:42'); insert into Events values('03', 'login', '2022-01-01 16:23:07'); insert into Events values('03', 'login', '2022-01-05 23:50:31'); insert into Events values('03', 'play', '2022-01-06 00:01:03'); insert into Events values('03', 'like', '2022-01-06 00:06:42'); insert into Events values('03', 'login', '2022-01-07 13:08:31'); insert into Events values('03', 'click', '2022-01-07 13:10:10'); insert into Events values('03', 'view', '2022-01-07 13:47:59'); insert into Events values('04', 'login', '2022-01-01 19:43:31'); insert into Events values('04', 'play', '2022-01-01 19:47:03'); insert into Events values('04', 'logout', '2022-01-01 19:53:55'); insert into Events values('04', 'login', '2022-01-02 18:43:31'); insert into Events values('04', 'play', '2022-01-02 18:47:03'); insert into Events values('04', 'logout', '2022-01-02 18:53:55'); insert into Events values('04', 'login', '2022-01-03 17:43:31'); insert into Events values('04', 'play', '2022-01-03 17:47:03'); insert into Events values('04', 'logout', '2022-01-03 17:53:55'); "},"ProbabilityAndStatistics.html":{"url":"ProbabilityAndStatistics.html","title":"概率论与数理统计","keywords":"","body":"基本概念定义 概率论基本概念 随机事件：随机实验中可能发生也可能不发生的事情，简称事件 必然事件：随机实验中必然发生写事件，用符号 Ω\\OmegaΩ 表示 不可能事件：随机试验中必然不发生的事件，用符号 ∅\\varnothing∅ 表示 随机实验 EEE 中必然发生一个且仅发生一个的最简单事件为实验 EEE 的基本事件，由若干基本事件组合而成的事件成为复合事件。一个事件是否为基本事件是相对于实验目的而言的。 我们用集合表示事件，对于随机实验 EEE 的每一个基本事件，用一个只包含一个元素 ω\\omegaω 的单元素 {ω}\\{\\omega\\}{ω} 表示；复合事件，则用对应的若干个元素所组成的集合表示； 由全体基本事件所对应的全部元素所组成的集合，称为随机实验 EEE 的样本空间，样本空间仍然用 Ω\\OmegaΩ 表示，和必然事件一样。样本空间的每一个元素 ω\\omegaω 为样本点。 随机变量：设 Ω\\OmegaΩ 是随机试验 EEE 的样本空间，若对于每一个样本点 ω∈Ω\\omega \\in \\Omegaω∈Ω ，都有唯一的实数 X(ω)X(\\omega)X(ω) 与之对应，且对于任意实数 xxx ，都有确定的概率 P{X(ω)≤x}P\\{X(\\omega) \\leq x\\}P{X(ω)≤x} 与之对应，则称 X(ω)X(\\omega)X(ω) 为随机变量，简记为 XXX 。随机变量是一个函数。 概率分布函数：设 Ω\\OmegaΩ 是随机试验 EEE 的样本空间，xxx 是任意实数，称函数 F(x)=P{X≤x}=P{ω:X(ω)≤x} F(x) = P\\{X \\leq x\\}= P\\{\\omega:X(\\omega) \\leq x\\} F(x)=P{X≤x}=P{ω:X(ω)≤x} 为随机变量 XXX 的分布函数 ，F(x)F(x)F(x) 也可以记作 Fx(x)F_x(x)Fx​(x) 。 概率密度：设 F(x)F(x)F(x) 是随机变量 XXX 的分布函数，若存在非负函数 f(x)f(x)f(x) ，对任意实数 xxx ，有 F(x)=∫−∞xf(u)du F(x) = \\int_{-\\infty}^x f(u)du F(x)=∫−∞x​f(u)du 则称 XXX 是连续型随机变量，称 f(x)f(x)f(x) 为 XXX 的概率密度。 数理统计基本概念 总体：研究对象的全体 个体：组成总体的每个基本元素 赋有一定概率分布的总体称为统计总体，其概率分布称为总体分布。当总体分布为正态分布时，称为正态分布总体或简称正态总体。 总体的概率分布是总体的核心。因此，进一步将总体看成具有相应的概率分布的随机变量，比如 XXX ，称作总体 XXX ，则随机变量 XXX 的概率分布就是总体分布。 样本是按一定的规定从总体中抽出的一部分个体。这里的”按一定的规定“，是指为保证总体中的每一个个体有同等的被抽出的机会而采取的一些措施。取得样本的过程，称为抽样。 样本是一组随机变量，记为 X1,X2,⋯ ,XnX_1,X_2,\\cdots,X_nX1​,X2​,⋯,Xn​ ，其中 nnn 称为样本容量或样本大小或样本量。实施抽样后得到的具体数据 x1,x2,⋯ ,xnx_1,x_2,\\cdots,x_nx1​,x2​,⋯,xn​ 称为样本观测值。 简单随机样本： 样本 X1,X2,⋯ ,XnX_1,X_2,\\cdots,X_nX1​,X2​,⋯,Xn​ 满足以下要求的称之为简单随机样本，如果没有特别说明，通常都是简单随机样本： 代表性。每个 XiX_iXi​ 应该与总体 XXX 有相同的分布； 独立性。 X1,X2,⋯ ,XnX_1,X_2,\\cdots,X_nX1​,X2​,⋯,Xn​ 应该是相互独立的随机变量。 统计量：设 X1,X2,⋯ ,XnX_1,X_2,\\cdots,X_nX1​,X2​,⋯,Xn​ 为来自总体 XXX 的一个样本，若样本函数 g(X1,X2,⋯ ,Xn)g(X_1,X_2,\\cdots,X_n)g(X1​,X2​,⋯,Xn​) 中不含任何未知参数，则称 g(X1,X2,⋯ ,Xn)g(X_1,X_2,\\cdots,X_n)g(X1​,X2​,⋯,Xn​) 为一个统计量。常用的统计量有：样本均值，样本方差，样本标准差。 将样本观测值 x1,x2,⋯ ,xnx_1,x_2,\\cdots,x_nx1​,x2​,⋯,xn​ 带入统计量公式中得到的值称之为统计值。 统计量也是随机变量，统计量的分布称为抽样分布，比如样本均值的抽样分布。 抽样分布定理 设 X1,X2,⋯ ,XnX_1,X_2,\\cdots,X_nX1​,X2​,⋯,Xn​ 是正态总体 N(μ,σ2)N(\\mu,\\sigma^2)N(μ,σ2) 的样本，Xˉ\\bar{X}Xˉ 、S2S^2S2 分别是样本均值和样本方差，则有： (1)Xˉ与S2相互独立(2)Xˉ∼N(μ,σ2n)(3)n−1σ2S2∼χ2(n−1)(4)Xˉ−μS/n∼t(n−1) \\begin{align*} &(1)\\quad \\bar{X} 与 S^2 相互独立\\\\ &(2)\\quad \\bar{X}\\sim N(\\mu,\\frac{\\sigma^2}{n}) \\\\ &(3)\\quad \\frac{n-1}{\\sigma^2}S^2\\sim \\chi^2(n-1) \\\\ &(4)\\quad \\frac{\\bar{X}-\\mu}{S/\\sqrt{n}}\\sim t(n-1)\\\\ \\end{align*} ​(1)Xˉ与S2相互独立(2)Xˉ∼N(μ,nσ2​)(3)σ2n−1​S2∼χ2(n−1)(4)S/n​Xˉ−μ​∼t(n−1)​ 参数估计 参数估计在A/B实验领域起着关键性的作用 参数的点估计 跟据矩估计法和极大似然估计法均可得出，若 X1,X2,⋯ ,XnX_1,X_2,\\cdots,X_nX1​,X2​,⋯,Xn​ 是正态总体 N(μ,σ2)N(\\mu,\\sigma^2)N(μ,σ2) 的样本，则均值 μ\\muμ 和方差 σ2\\sigma^2σ2 的估计为： μ^=Xˉ,σ^2=1n∑i=1n(Xi−Xˉ)2 \\hat{\\mu}=\\bar{X},\\quad \\hat{\\sigma}^2=\\frac{1}{n}\\sum_{i=1}^n(X_i-\\bar{X})^2 μ^​=Xˉ,σ^2=n1​i=1∑n​(Xi​−Xˉ)2 但基于参数估计无偏性的准则，由矩估计法和极大似然估计法求得的 σ^2\\hat{\\sigma}^2σ^2 并不是无偏的。需要将分母 nnn 修正为 n−1n-1n−1，也就是 S2=1n−1∑i=1n(Xi−Xˉ)2 S^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X})^2 S2=n−11​i=1∑n​(Xi​−Xˉ)2 为 σ2\\sigma^2σ2 的无偏估计。 区间估计 利用枢轴变量法构造置信区间 一个正态总体参数的置信区间 设 X1,X2,⋯ ,XnX_1,X_2,\\cdots,X_nX1​,X2​,⋯,Xn​ 是正态总体 N(μ,σ2)N(\\mu,\\sigma^2)N(μ,σ2) 的样本，求未知参数 μ\\muμ 的置信度为 1−α1-\\alpha1−α 的置信区间： σ2\\sigma^2σ2 已知 因为样本均值 Xˉ\\bar{X}Xˉ 是 μ\\muμ 的无偏估计，且根据抽样分布定理 Xˉ∼N(μ,σ2n)\\bar{X}\\sim N(\\mu,\\frac{\\sigma^2}{n})Xˉ∼N(μ,nσ2​) ，所以： U=Xˉ−μσ/n∼N(0,1) U=\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1) U=σ/n​Xˉ−μ​∼N(0,1) 于是由标准正态分布的上侧分位数的定义可知，对于给定的置信度 1−α1-\\alpha1−α ，有 P{∣U∣≤uα2}=1−αP\\{|U|\\leq u_\\frac{\\alpha}{2}\\}=1-\\alphaP{∣U∣≤u2α​​}=1−α，即： P{−uα2≤Xˉ−μσ/n≤uα2}=P{Xˉ−σnuα2≤μ≤Xˉ+σnuα2}=1−α \\begin{align*} & P\\{-u_\\frac{\\alpha}{2} \\leq \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\leq u_\\frac{\\alpha}{2}\\} \\\\ =& P\\{\\bar{X}-\\frac{\\sigma}{\\sqrt{n}}u_\\frac{\\alpha}{2} \\leq \\mu \\leq \\bar{X}+\\frac{\\sigma}{\\sqrt{n}}u_\\frac{\\alpha}{2}\\} \\\\ =& 1-\\alpha \\end{align*} ==​P{−u2α​​≤σ/n​Xˉ−μ​≤u2α​​}P{Xˉ−n​σ​u2α​​≤μ≤Xˉ+n​σ​u2α​​}1−α​ 从而得到 μ\\muμ 的置信度为 1−α1-\\alpha1−α 的置信区间为 [Xˉ−σnuα2,Xˉ+σnuα2][\\bar{X}-\\frac{\\sigma}{\\sqrt{n}}u_\\frac{\\alpha}{2},\\bar{X}+\\frac{\\sigma}{\\sqrt{n}}u_\\frac{\\alpha}{2}][Xˉ−n​σ​u2α​​,Xˉ+n​σ​u2α​​] σ2\\sigma^2σ2 未知 此时 U=Xˉ−μσ/nU=\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}U=σ/n​Xˉ−μ​ 不再构成枢轴变量，因为 σ2\\sigma^2σ2 未知，故用 S2S^2S2 代替 σ2\\sigma^2σ2 。根据抽样分布定理，枢轴变量 T=Xˉ−μS/n∼t(n−1)T=\\frac{\\bar{X}-\\mu}{S/\\sqrt{n}}\\sim t(n-1)T=S/n​Xˉ−μ​∼t(n−1) 。因为 ttt 分布也是关于 YYY 轴对称，于是有： P{−tα2(n−1)≤Xˉ−μS/n≤tα2(n−1)}=1−α P\\{-t_\\frac{\\alpha}{2}(n-1) \\leq \\frac{\\bar{X}-\\mu}{S/\\sqrt{n}} \\leq t_\\frac{\\alpha}{2}(n-1)\\}=1-\\alpha P{−t2α​​(n−1)≤S/n​Xˉ−μ​≤t2α​​(n−1)}=1−α 经过恒等变形，得到参数 μ\\muμ 的置信度为 1−α1-\\alpha1−α 的置信区间是 [Xˉ−Sntα2(n−1),Xˉ+Sntα2(n−1)][\\bar{X}-\\frac{S}{\\sqrt{n}}t_\\frac{\\alpha}{2}(n-1),\\bar{X}+\\frac{S}{\\sqrt{n}}t_\\frac{\\alpha}{2}(n-1)][Xˉ−n​S​t2α​​(n−1),Xˉ+n​S​t2α​​(n−1)] 两个正态总体的区间估计 大样本方法构造置信区间 单侧置信区间 "},"GitCheatSheet.html":{"url":"GitCheatSheet.html","title":"Git Cheat Sheet","keywords":"","body":" 创建仓库提交文件 创建一个版本库 $ git init Initialized empty Git repository in /Users/xxx/xxx/.git/ 把文件添加进暂存区 $ git add readme.txt $ git add file1.txt file2.txt # 添加当前目录下的所有文件进暂存区 $ git add . 把文件提交到本地仓库 $ git commit -m \"wrote a readme file\" 修改上一次commit。比如修改之前提交时的日志，或者追加提交文件等。 $ git commit --amend 查看当前状态，也可以查看到位于哪个分支 $ git status # On branch master # Your branch is ahead of 'origin/master' by 202 commits. # # Untracked files: # (use \"git add ...\" to include in what will be committed) # # ../profile.svg # ../templates/customer_service.html nothing added to commit but untracked files present (use \"git add\" to track) 查看修改内容 $ git diff readme.txt diff --git a/readme.txt b/readme.txt index 46d49bf..9247db6 100644 --- a/readme.txt +++ b/readme.txt @@ -1,2 +1,2 @@ -Git is a version control system. +Git is a distributed version control system. Git is free software. 本地版本控制 版本查看 $ git log commit 1094adb7b9b3807259d8cb349e7df1d4d6477073 (HEAD -> master) Author: Michael Liao Date: Fri May 18 21:06:15 2018 +0800 append GPL commit e475afc93c209a690c39c13a46716e8fa000c366 Author: Michael Liao Date: Fri May 18 21:03:36 2018 +0800 add distributed 回退到上一个版本，回退到上上个版本 $ git reset --hard HEAD^ HEAD is now at e475afc add distributed $ git reset --hard HEAD^ 通过版本号更改到指定版本，可以是之后的版本，也可以是之前的版本 $ git reset --hard 1094a HEAD is now at 83b0afe append GPL 回退到之前的版本之后，无法通过git log查看到后面的版本，可以通过git relog来查看每一次输入的命令，查看到后面版本的版本号 $ git reflog e475afc HEAD@{1}: reset: moving to HEAD^ 1094adb (HEAD -> master) HEAD@{2}: commit: append GPL e475afc HEAD@{3}: commit: add distributed eaadf4e HEAD@{4}: commit (initial): wrote a readme file 远程仓库 配置本地全局用户名和Email $ git config --global user.name \"Your Name\" $ git config --global user.email \"email@example.com\" # 查看配置信息 git config --list 给本地特定仓库设置特定的用户名和Email，需要去到该仓库的根目录下利用git config进行设置。去掉--global参数。可以在该仓库的.git/config查看具体的配置信息。 $ git config user.name \"Your Name\" $ git config user.email \"email@example.com\" 验证本地git和github的连通性 $ ssh -T git@github.com Hi powerAmore! You've successfully authenticated, but GitHub does not provide shell access. 关联远程仓库，并把远程仓库取名为origin $ git remote add origin git@github.com:xxx/learngit.git 第一次将本地仓库master分支推送到远程origin仓库。-u参数会将本地master分支和远程master分支做关联。 $ git push -u origin master Counting objects: 20, done. Delta compression using up to 4 threads. Compressing objects: 100% (15/15), done. Writing objects: 100% (20/20), 1.64 KiB | 560.00 KiB/s, done. Total 20 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), done. To github.com:michaelliao/learngit.git * [new branch] master -> master Branch 'master' set up to track remote branch 'master' from 'origin'. 后续将本地仓库master分支内容推送到远程仓库 $ git push origin master 查看远程仓库信息 $ git remote -v origin git@github.com:xxx/learn-git.git (fetch) origin git@github.com:xxx/learn-git.git (push) 解除和远程仓库origin的关联。如果要删除远程仓库，需要去到远程仓库所在地址进行删除。 $ git remote rm origin 从远程仓库克隆一个本地库 $ git clone git@github.com:xxx/gitskills.git Cloning into 'gitskills'... remote: Counting objects: 3, done. remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 3 Receiving objects: 100% (3/3), done. 从远程仓库拉取代码并合并本地版本 $ git pull $ git pull origin git pull其实就是 git fetch 和git merge FETCH_HEAD 的简写。格式如下： # git pull : # 拉取远程仓库origin的master分支，与本地的brantest分支合并 $ git pull origin master:brantest # 如果远程分支是与当前分支合并，则冒号后面的部分可以省略 $ git pull origin master 分支操作 创建dev分支，然后切换到dev分支 $ git checkout -b dev Switched to a new branch 'dev' git checkout命令加上-b参数表示创建并切换，相当于以下两条命令。其中git branch dev为创建本地分支，git checkout dev为切换分支 $ git branch dev $ git checkout dev Switched to branch 'dev' switch也可以用来切换分支，相比checkout更容易理解。因为撤销修改是git checkout -- ，同一个命令，有两种作用，确实有点令人迷惑。 创建并切换到新的dev分支 $ git switch -c dev 直接切换到已有的master分支 $ git switch master 用git branch命令查看当前分支 $ git branch * dev master 把dev分支的工作成果合并到当前所处的master分支。Fast-forward信息表示这次合并是“快进模式”，也就是直接把master指向dev的当前提交。也不是每次合并都能Fast-forward。 $ git merge dev Updating d46f35e..b17d20e Fast-forward readme.txt | 1 + 1 file changed, 1 insertion(+) 删除dev分支 $ git branch -d dev Deleted branch dev (was b17d20e). "}}