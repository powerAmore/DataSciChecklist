{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction 工作中涉及到了数据领域的方方面面，包括大数据技术、机器学习、深度学习、推荐系统、数据产品、数据分析等非常多的领域。然而由于工作重心不断变换，一段时间不接触，很多领域的基础知识非常容易遗忘，非常可惜。重新查阅相关资料非常耗时，有些基于自身理解的概念想法，即使查阅资料也并不容易找回。 对于查阅资料。网络上各种资料教程非常多，也不乏高质量的课程。但是问题也很明显，多数教程面向初学者，极其细致，看完往往需要很长时间，学习效率较低。尤其视频教程，甚至很难快速直接找到关注点。对于在该领域已经建立整体概念框架的人来说，需要的往往是提纲挈领，直击要害，把书读薄。 故做此Checklist，帮助自己总结归纳知识，即使后面忘记了也不再需要手忙脚乱的到处查阅资料。也帮助需要复习相关领域知识点的小伙伴快速恢复记忆。务必做到言简意赅，突出重点。 如果可能，希望能写成一本数据工作者实用手册。 "},"GradientDescent.html":{"url":"GradientDescent.html","title":"梯度下降","keywords":"","body":"梯度下降法 机器学习深度学习中用梯度下降法来优化损失函数，试图求解损失函数的最小值以及其对应的参数。要搞清楚梯度下降法，我们从方向导数的概念引入。 方向导数 方向导数就是曲面切线的斜率。曲面的切线不是唯一的，360∘360^{\\circ}360∘ 各个方向都有，所以不同方向的切线斜率也不一定相同，方向导数也不是唯一的。 例如二元函数 f(x,y)=x2+y2f(x,y)=x^2+y^2f(x,y)=x2+y2 的方向导数如图所示： 函数 f(x,y)f(x,y)f(x,y) 在 PPP 点沿着切线 lll 方向的方向导数为： lim⁡ρ→0f(x+Δx,y+Δy)−f(x,y)ρ {\\lim_{\\rho\\to0}\\frac{f(x+\\Delta x, y+\\Delta y)-f(x,y)}{\\rho}} ρ→0lim​ρf(x+Δx,y+Δy)−f(x,y)​ 其中 ρ=(Δx)2+(Δy)2\\rho=\\sqrt{(\\Delta x)^2+(\\Delta y)^2}ρ=(Δx)2+(Δy)2​ 。把该极限记作方向导数 ∂f∂l\\frac{\\partial f}{\\partial l}∂l∂f​ 。其计算公式为： ∂f∂l=∂f∂xcosφ+∂f∂ysinφ \\frac{\\partial f}{\\partial l} = \\frac{\\partial f}{\\partial x}cos\\varphi + \\frac{\\partial f}{\\partial y}sin\\varphi ∂l∂f​=∂x∂f​cosφ+∂y∂f​sinφ 其中 φ\\varphiφ 为x轴正方向到 lll 的角度。 梯度 梯度是个向量，它的模为曲面上该点取值最大的方向导数的值，方向为最大方向导数对应的切线的投影方向。也是该点所处的等高线的法向量，也就是函数值变化最快的方向。 [!NOTE] 梯度的方向并不是最大方向导数对应的切线方向，而是切线的投影方向。但即使认为是切线方向，对理解梯度的概念也不会产生太大的影响。 图中绿色箭头所示向量即为梯度： 对于二元函数 f(x,y)f(x,y)f(x,y) ，其曲面上任意点 P(x,y)P(x,y)P(x,y) 的梯度，记作 gradf(x,y)gradf(x,y)gradf(x,y) 或 ∇f(x,y)\\nabla f(x,y)∇f(x,y) ，定义为： ∇f(x,y)=(∂f∂x,∂f∂y)=fx(x,y)i⃗+fy(x,y)j⃗ \\nabla f(x,y)=(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y})=f_x(x,y)\\vec i+f_y(x,y)\\vec j ∇f(x,y)=(∂x∂f​,∂y∂f​)=fx​(x,y)i+fy​(x,y)j​ 仍然以二元函数 f(x,y)=x2+y2f(x,y)=x^2+y^2f(x,y)=x2+y2 为例。设 e⃗=(cosφ,sinφ)\\vec e=(cos\\varphi, sin\\varphi)e=(cosφ,sinφ) 为曲面某点切线 lll 方向导数所对应方向的单位向量，则该方向导数为： ∂f∂l=∂f∂xcosφ+∂f∂ysinφ=(∂f∂x,∂f∂y)⋅(cosφ,sinφ)=∣∇f(x,y)∣⋅∣e⃗∣⋅cos⟨∇f(x,y),e⃗⟩=∣∇f(x,y)∣⋅1⋅cos⟨∇f(x,y),e⃗⟩ \\begin{align*} \\frac{\\partial f}{\\partial l} =\\frac{\\partial f}{\\partial x}cos\\varphi + \\frac{\\partial f}{\\partial y}sin\\varphi &=(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y})\\cdot(cos\\varphi, sin\\varphi) \\\\ &=|\\nabla f(x,y)|\\cdot |\\vec e| \\cdot cos\\langle\\nabla f(x,y),\\vec e\\rangle \\\\ &=|\\nabla f(x,y)|\\cdot 1 \\cdot cos\\langle\\nabla f(x,y),\\vec e\\rangle \\end{align*} ∂l∂f​=∂x∂f​cosφ+∂y∂f​sinφ​=(∂x∂f​,∂y∂f​)⋅(cosφ,sinφ)=∣∇f(x,y)∣⋅∣e∣⋅cos⟨∇f(x,y),e⟩=∣∇f(x,y)∣⋅1⋅cos⟨∇f(x,y),e⟩​ 其中，⟨∇f(x,y),e⃗⟩\\langle\\nabla f(x,y),\\vec e\\rangle⟨∇f(x,y),e⟩ 为 ∇f(x,y)\\nabla f(x,y)∇f(x,y) 和 e⃗\\vec ee 的夹角。当 cos⟨∇f(x,y),e⃗⟩=1cos\\langle\\nabla f(x,y),\\vec e\\rangle=1cos⟨∇f(x,y),e⟩=1 时，方向导数 ∂f∂l\\frac{\\partial f}{\\partial l}∂l∂f​ 有最大值，为梯度的模 ∣∇f(x,y)∣|\\nabla f(x,y)|∣∇f(x,y)∣ ，且此时 e⃗\\vec ee 的方向和梯度 ∇f(x,y)\\nabla f(x,y)∇f(x,y) 的方向保持一致。 梯度下降法 为求解函数的最小值以及最小值对应的坐标，数学上通常要么有直接的求解公式。要么就对函数进行求导，令导函数等于0进行求解。但面对机器学习这类场景，往往数据量大、维度高导致计算量过大，或者目标函数复杂本身无法获得解析解。机器学习场景下，往往利用梯度下降这类迭代优化算法快速逼近目标函数的最小值以获得最优化的参数值。 梯度下降法的作用不仅仅是求解函数最小值，在机器学习深度学习算法中，更重要的是获取函数最小值（尽可能小）时对应的坐标，即最优化的函数参数值。 梯度下降法的原理是：试图通过迭代的方式，每步都沿着函数数值下降最快的方向（也就是负梯度方向）走一小步。每走一步都重新确认一下负梯度方向，然后再沿着该方向下降，直到找到函数最小值以及对应的位置。 [!NOTE] 之所以每步都沿着负梯度方向走有两个原因： 负梯度方向一定是会让函数值变小（至少不变大）的方向。需要注意：走出了这一步不意味着函数值就一定会变小，也有可能步子迈大了，函数值反而有所变大。 负梯度是函数值下降最快的方向，便于更快速的找到最小值。其实，即使每步不是沿着负梯度方向，只要是沿着一个函数值变小的方向，最终也是能找到函数的最小值的。 代码实现梯度下降 仍然以二元函数 f(x,y)=x2+y2f(x,y)=x^2+y^2f(x,y)=x2+y2 为例，写代码实现梯度下降法求解函数最小值。代码其实很简单，关键需要自己手动把函数梯度先求出来。10次迭代之后可以看到已经很接近函数的最小值了。如果再多迭代几次就肯定能达到最小值。 # 原函数f(x,y) def f(x,y): return x ** 2 + y ** 2 # f(x,y)对x的偏导 def fx(x): return 2 * x # f(x,y)对y的偏导 def fy(y): return 2 * y # 设置梯度下降起始点 x=2; y=2 # 设置梯度下降步长 step = 0.1 # 循环迭代进行梯度下降 for i in range(10): # 设置梯度下降迭代次数为10 before = f(x, y) # 梯度下降开始前，起始坐标下的函数值 x = x - step * fx(x) # x轴方向进行梯度下降，获得新的x坐标 y = y - step * fy(y) # y轴方向进行梯度下降，获得新的y坐标 after = f(x, y) # 基于新坐标的函数值 theta = before - after # 完成一次梯度下降迭代，前后函数值的差 print(\"before:{:.2f}, after:{:.2f}, theta:{:.2f}, x:{:.2f}, y:{:.2f}\".format(before, after, theta, x, y)) # 输出结果并保留2为小数 before:8.00, after:5.12, theta:2.88, x:1.60, y:1.60 before:5.12, after:3.28, theta:1.84, x:1.28, y:1.28 before:3.28, after:2.10, theta:1.18, x:1.02, y:1.02 before:2.10, after:1.34, theta:0.75, x:0.82, y:0.82 before:1.34, after:0.86, theta:0.48, x:0.66, y:0.66 before:0.86, after:0.55, theta:0.31, x:0.52, y:0.52 before:0.55, after:0.35, theta:0.20, x:0.42, y:0.42 before:0.35, after:0.23, theta:0.13, x:0.34, y:0.34 before:0.23, after:0.14, theta:0.08, x:0.27, y:0.27 before:0.14, after:0.09, theta:0.05, x:0.21, y:0.21 实验：用方向导数替代梯度下降 假设用方向导数来替代梯度，可以看到其实也可以起到下降的效果，只是速度要慢一些，10次迭代后离最小值还有点距离。在真实的大数据应用场景中，速度是需要考虑的非常重要的点。梯度不仅下降快，求梯度也比求方向导数更加容易。 import math # 设置梯度下降起始点 x=2; y=2 # 设置梯度下降步长 step = 0.1 for i in range(10): # 设置迭代次数为10 before = f(x, y) # 开始前，起始坐标下的函数值 x = x - step * fx(x) * math.cos(math.pi/3) # 沿着切线投影与x成60度角的方向导数进行下降 y = y - step * fy(y) * math.sin(math.pi/3) # 沿着切线投影与y成60度角的方向导数进行下降 after = f(x, y) # 完成后，基于新坐标的函数值 theta = before - after # 完成一次迭代，前后函数值的差 print(\"before:{:.2f}, after:{:.2f}, theta:{:.2f}, x:{:.2f}, y:{:.2f}\".format(before, after, theta, x, y)) # 输出结果并保留2为小数 before:8.00, after:5.97, theta:2.03, x:1.80, y:1.65 before:5.97, after:4.49, theta:1.48, x:1.62, y:1.37 before:4.49, after:3.40, theta:1.09, x:1.46, y:1.13 before:3.40, after:2.60, theta:0.81, x:1.31, y:0.93 before:2.60, after:1.99, theta:0.60, x:1.18, y:0.77 before:1.99, after:1.54, theta:0.45, x:1.06, y:0.64 before:1.54, after:1.19, theta:0.34, x:0.96, y:0.53 before:1.19, after:0.93, theta:0.26, x:0.86, y:0.44 before:0.93, after:0.73, theta:0.20, x:0.77, y:0.36 before:0.73, after:0.58, theta:0.16, x:0.70, y:0.30 画图展示梯度下降 采用plotly进行作图。之所以不用matplot，是因为matplot不太好实现surface和scatter在同一张图里展示。如图所见，红色线表示梯度下降迭代过程，逐渐逼近函数曲面最小值，每个小红点代表一次迭代。而蓝色线处于xy平面上，每一个小蓝点才是真正每一步迭代的梯度。 # 设置梯度下降起始点 x=2; y=2 # 设置梯度下降步长 step = 0.1 pos_x = [] pos_y = [] pos_z = [] # 循环迭代进行梯度下降 for i in range(10): # 设置梯度下降迭代次数为10 before = f(x, y) # 梯度下降开始前，起始坐标下的函数值 pos_x.append(x) pos_y.append(y) pos_z.append(before) x = x - step * fx(x) # x轴方向进行梯度下降，获得新的x坐标 y = y - step * fy(y) # y轴方向进行梯度下降，获得新的y坐标 after = f(x, y) # 基于新坐标的函数值 pos_x.append(x) pos_y.append(y) pos_z.append(after) from plotly.offline import init_notebook_mode, iplot import plotly.graph_objects as go import numpy as np # 画二元函数曲面 xx = np.arange(-3,3,0.1) yy = np.arange(-3,3,0.1) X, Y = np.meshgrid(xx, yy) Z = X ** 2 +Y ** 2 trace_surface= go.Surface(x=X, y=Y, z=Z, colorscale='redor', showscale=False, opacity=0.7) # 画红色trace线 trace_scatter3d = go.Scatter3d(x=pos_x, y=pos_y, z=pos_z, mode='lines+markers', marker=dict(color='red', size=3), name = 'trace') # 画蓝色gradient线 trace_gradient = go.Scatter3d(x=pos_x, y=pos_y, z=np.zeros(len(pos_x)), mode='lines+markers', marker=dict(color='blue', size=3), name = 'gradient') data=[trace_surface, trace_scatter3d, trace_gradient] # 图片布局调整 layout = go.Layout(scene = dict(aspectratio = dict(x=1.5, y=1.5, z=1)), margin=dict(l=5, r=5, t=5, b=5), width=700) fig = dict(data = data, layout = layout) iplot(fig) PyTorch自动求导autograd 在前面的代码中最关键的点在于需要实现手动将函数的梯度公式计算出来，代码只是将具体的数值带入梯度公式计算。而PyTorch作为市面上最流行的机器学习深度学习框架，它的核心魔法就在于自动求导：autograd。 将上面手写的梯度下降代码中梯度计算的部分用PyTorch的autograd进行替换，如下所示。可以看到，代码只给出了原函数，并未给出梯度的计算公式，整个梯度的计算过程完全由PyTorch自动完成。而且PyTorch的代码跟原本的代码只有很少的差别。最终结果，跟上面手写的代码完全一致。 import torch # 原函数f(x,y) def f(x,y): return x ** 2 + y ** 2 # 设置梯度下降起始点 x=torch.tensor(2.0,requires_grad=True) # tensor如果需要能够求导需要是浮点数 y=torch.tensor(2.0,requires_grad=True) # 设置梯度下降步长 step = 0.1 # 循环迭代进行梯度下降 for i in range(10): # 设置梯度下降迭代次数为10 before = f(x, y) # 梯度下降开始前，起始坐标下的函数值 before.backward() # 反向传播，实际就是利用链式法则求偏导数，返回值为None，想要获得导数需要调用x.grad、y.grad # torch.autograd.backward(z) # 跟上面的函数完全一样 # torch.autograd.grad(z,[x,y]) # 求导数，函数会直接返回具体的导数值 x = x.detach() - step * x.grad # x轴方向进行梯度下降，获得新的x坐标 x.requires_grad_(True) y = y.detach() - step * y.grad # y轴方向进行梯度下降，获得新的y坐标 y.requires_grad_(True) after = f(x, y) # 基于新坐标的函数值 theta = before - after # 完成一次梯度下降迭代，前后函数值的差 print(\"before:{:.2f}, after:{:.2f}, theta:{:.2f}, x:{:.2f}, y:{:.2f}\".format(before, after, theta, x, y)) # 输出结果并保留2为小数 before:8.00, after:5.12, theta:2.88, x:1.60, y:1.60 before:5.12, after:3.28, theta:1.84, x:1.28, y:1.28 before:3.28, after:2.10, theta:1.18, x:1.02, y:1.02 before:2.10, after:1.34, theta:0.75, x:0.82, y:0.82 before:1.34, after:0.86, theta:0.48, x:0.66, y:0.66 before:0.86, after:0.55, theta:0.31, x:0.52, y:0.52 before:0.55, after:0.35, theta:0.20, x:0.42, y:0.42 before:0.35, after:0.23, theta:0.13, x:0.34, y:0.34 before:0.23, after:0.14, theta:0.08, x:0.27, y:0.27 before:0.14, after:0.09, theta:0.05, x:0.21, y:0.21 "},"LinearRegression.html":{"url":"LinearRegression.html","title":"线性回归","keywords":"","body":"线性回归的目标函数 已知数据集 X\\boldsymbol{X}X ，其中有 mmm 个 nnn 维样本 xi\\boldsymbol{x}^ixi ，iii 代表第 iii 个样本。每个样本 xi\\boldsymbol{x}^ixi 对应着一个目标值 yiy^iyi ，或称标签。把目标值 yiy^iyi 的集合（向量）记作 Y\\boldsymbol{Y}Y 。 我们试图去拟合一个线性函数f(x)f(\\boldsymbol{x})f(x) ： f(x)=ωTx+b=ω1x1+ω2x2+⋯+ωnxn+b f(\\boldsymbol{x}) = \\boldsymbol{\\omega}^\\mathsf{T} \\boldsymbol{x} + b=\\omega_1x_1+\\omega_2x_2+\\cdots+\\omega_nx_n+b f(x)=ωTx+b=ω1​x1​+ω2​x2​+⋯+ωn​xn​+b 使得 yi=f(xi)y^i =f(\\boldsymbol{x}^i)yi=f(xi) ，这就是线性回归。但显然线性函数本质上只是一条直线（2维）或者一个超平面（n维），并不太能让任意 xi\\boldsymbol{x}^ixi 满足yi=f(xi)y^i =f(\\boldsymbol{x}^i)yi=f(xi) 。只能退而求其次，找到一个尽可能接近真实情况的f(x)f(\\boldsymbol{x})f(x) 。一旦有了这个f(x)f(\\boldsymbol{x})f(x) ，如果我们输入一个数据集 X\\boldsymbol{X}X 中不存在的样本 x\\boldsymbol{x}x ，就可以求解一个预测值 yyy 达到机器学习目的。 线性回归的损失函数 如何度量所谓的尽可能接近真实情况呢？我们采用均方误差来度量。均方误差有非常好的几何意义，它对应了常用的欧式距离。均方误差越小，则说明该线性函数f(x)f(\\boldsymbol{x})f(x) 越接近真实的情况。我们把均方误差函数作为线性回归的损失函数： J(ω,b)=12∑i=1m(fω,b(xi)−yi)2=12∑i=1m(ωTxi+b−yi)2 J(\\boldsymbol{\\omega},b)=\\frac{1}{2}\\sum_{i=1}^m(f_{\\boldsymbol{\\omega},b}(\\boldsymbol{x}^i)-y^i)^2=\\frac{1}{2}\\sum_{i=1}^m(\\boldsymbol{\\omega}^\\mathsf{T} \\boldsymbol{x}^i + b-y^i)^2 J(ω,b)=21​i=1∑m​(fω,b​(xi)−yi)2=21​i=1∑m​(ωTxi+b−yi)2 同样都作为线性函数f(x)f(\\boldsymbol{x})f(x) 需要求解的未知参数，可以把 ω\\boldsymbol{\\omega}ω 和 b b b 合并为一个向量 θ\\boldsymbol{\\theta}θ ，相应的需要把每一个样本 x=(x1,x2,⋯ ,xn)T\\boldsymbol{x}=(x_1, x_2,\\cdots,x_n)^\\mathsf{T}x=(x1​,x2​,⋯,xn​)T 最后都增加一个 111 改写为 x=(x1,x2,⋯ ,xn,1)T\\boldsymbol{x}=(x_1, x_2,\\cdots,x_n,1)^\\mathsf{T}x=(x1​,x2​,⋯,xn​,1)T 。经过这样的变换，需要求解的线性函数f(x)f(\\boldsymbol{x})f(x) 就可以简写为：f(x)=θTxf(\\boldsymbol{x}) = \\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}f(x)=θTx ，而损失函数可以简写为： J(θ)=12∑i=1m(fθ(xi)−yi)2=12∑i=1m(θTxi−yi)2 J(\\boldsymbol{\\theta})=\\frac{1}{2}\\sum_{i=1}^m(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}^i)-y^i)^2=\\frac{1}{2}\\sum_{i=1}^m(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i -y^i)^2 J(θ)=21​i=1∑m​(fθ​(xi)−yi)2=21​i=1∑m​(θTxi−yi)2 注意到损失函数最前面有个 12\\frac{1}{2}21​ ，如何解释这个 12\\frac{1}{2}21​ 的存在？ 一方面，后续我们会利用梯度下降法优化损失函数，其中对 θ\\boldsymbol{\\theta}θ 求偏导时会把 12\\frac{1}{2}21​ 消掉，让结果更工整。 另一方面，更重要的，既然是要求参数 θ\\boldsymbol{\\theta}θ ，那就把它转化为一个参数估计的问题。用极大似然估计（Maximum Likelihood Estimate，MLE）的方法，要找到一个参数 θ\\boldsymbol{\\theta}θ ，使得所有样本 X\\boldsymbol{X}X 的观测值 Y\\boldsymbol{Y}Y 联合概率最大。最终计算得到的损失函数刚好就会包含这个 12\\frac{1}{2}21​ 。 损失函数求最小值 解析解：正规方程 将样本通通带入损失函数，找到损失函数最小时对应的参数向量 θ\\boldsymbol{\\theta}θ ，则可得到我们希望拟合的最优线性函数f(x)f(\\boldsymbol{x})f(x) 。对于线性回归而言，对基于均方误差的损失函数求最小值，是可以求得解析解的，即存在一个简单的公式表达最小值。具体的做法就是损失函数对 θ\\boldsymbol{\\theta}θ 求导，令导数等于0。最后求得损失函数的解析解为： θ=(XTX)−1XTY \\boldsymbol{\\theta}=(\\boldsymbol{X}^\\mathsf{T}\\boldsymbol{X})^{-1}\\boldsymbol{X}^\\mathsf{T}\\boldsymbol{Y} θ=(XTX)−1XTY 也称之为正规方程。这个解析解存在的一个问题是需要满足 XTX\\boldsymbol{X}^\\mathsf{T}\\boldsymbol{X}XTX 可逆，所以在具体算法实现中，通常并不直接利用这个解析解公式，而是利用SVD奇异值分解的方式进行求解，sklearn中的linear_model.LinearRegression()就是这样的实现，这里暂时不详说。我们更想要深入讨论的是通过梯度下降法来优化（求解）损失函数的方法。另外基于SVD分解的线性回归计算复杂度为 O(mn2)O(mn^2)O(mn2) ，复杂度过高，不适合高维大样本的计算。 梯度下降法 目标是优化损失函数 J(θ)J(\\boldsymbol{\\theta})J(θ) ，找到合适的参数向量 θ=(θ1,θ2,⋯ ,θn)\\boldsymbol{\\theta}=(\\theta_1,\\theta_2,\\cdots,\\theta_n)θ=(θ1​,θ2​,⋯,θn​) 使得损失函数 J(θ)J(\\boldsymbol{\\theta})J(θ) 的值尽可能小。根据梯度下降法的原理，先求梯度，对每一个分量 θj\\theta_jθj​ 求偏导： ∂J(θ)∂θj=∂∂θj12∑i=1m(θTxi−yi)2=∑i=1m[(θTxi−yi)⋅∂∂θj(θTxi−yi)]=∑i=1m[(θTxi−yi)⋅∂∂θj(θ1x1i+θ2x2i+⋯+θnxni−yi)]=∑i=1m[(θTxi−yi)⋅xji] \\begin{align*} \\frac{\\partial J(\\boldsymbol{\\theta})}{\\partial \\theta_j} &=\\frac{\\partial}{\\partial \\theta_j}\\frac{1}{2}\\sum_{i=1}^m(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i -y^i)^2\\\\ &=\\sum_{i=1}^m[(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i -y^i)\\cdot\\frac{\\partial}{\\partial \\theta_j}(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i -y^i)]\\\\ &=\\sum_{i=1}^m[(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i -y^i)\\cdot\\frac{\\partial}{\\partial \\theta_j}(\\theta_1x^i_1+\\theta_2x^i_2+\\cdots+\\theta_nx^i_n-y^i)]\\\\ &=\\sum_{i=1}^m[(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i -y^i)\\cdot x^i_j] \\end{align*} ∂θj​∂J(θ)​​=∂θj​∂​21​i=1∑m​(θTxi−yi)2=i=1∑m​[(θTxi−yi)⋅∂θj​∂​(θTxi−yi)]=i=1∑m​[(θTxi−yi)⋅∂θj​∂​(θ1​x1i​+θ2​x2i​+⋯+θn​xni​−yi)]=i=1∑m​[(θTxi−yi)⋅xji​]​ 有了梯度之后，就可以沿着负梯度方向不断迭代更新参数 θ\\boldsymbol{\\theta}θ ： θj:=θj−α∑i=1m[(θTxi−yi)⋅xji] \\theta_j := \\theta_j - \\alpha\\sum_{i=1}^m[(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i -y^i)\\cdot x^i_j] θj​:=θj​−αi=1∑m​[(θTxi−yi)⋅xji​] 其中 α\\alphaα 为学习率。等式右边的 θj\\theta_jθj​ 为更新前的值，左边的 θj\\theta_jθj​ 为更新后的值。随着不断的迭代，损失函数 J(θ)J(\\boldsymbol{\\theta})J(θ) 的值会逐渐变小直至收敛。一旦收敛则可停止迭代，此时的参数向量 θ\\boldsymbol{\\theta}θ 即为线性回归最终的求解结果。已知 θ\\boldsymbol{\\theta}θ 就可以写出我们最终希望拟合的那个线性函数 fθ(x)f_\\theta (\\boldsymbol{x})fθ​(x) 。 线性回归的梯度下降法代码实现 以二维平面中的点数据作为样本，尝试编写用梯度下降法求优化线性回归问题的代码。代码本身并未局限于二维样本，可以将更高维样本数据作为输入进行代码测试。 import numpy as np points = np.genfromtxt('data.csv', delimiter=',') m = len(points) # 样本量 n = len(points[0]) # 样本维数 # 从points中取出样本列，并在每个样本后面增加一个1 X = np.c_[points[:,0], np.ones((m, 1))] # 从points中取出目标值向量 Y= points[:,1] # 目标线性函数 y = f(x) def f(w, x): return np.dot(w, x) # 点乘 # 均方误差损失函数 J(w) def J(w): j = 0 for i in range(m): y_hat = f(w, X[i]) y = Y[i] j += 0.5 * (y_hat - y) ** 2 return j # 损失函数 J(w) 的梯度向量 def grad_J(w): grad = np.zeros(n) for i in range(m): y_hat = f(w, X[i]) y = Y[i] grad += (y_hat - y) * X[i] return grad # 设置学习率 alpha = 0.000001 def grad_desc(w, num_iter): J_list = [] grad_J_list = [] w_list = [] for i in range(num_iter): valueJ = J(w) vectorGradJ = grad_J(w) J_list.append(valueJ) grad_J_list.append(vectorGradJ) w_list.append(w) print(\"J(w):{}, grad_J(w):{}, w:{}\".format(valueJ, vectorGradJ, w)) w = w - alpha * vectorGradJ return J_list, grad_J_list, w_list # 初始化参数向量W，全都设置为0 W = np.zeros(n) # 设置迭代次数 steps = 10 J_list, grad_J_list, w_list = grad_desc(W, steps) J(w):278255.3917241606, grad_J(w):[-368535.14867955 -7273.50505537], w:[0. 0.] J(w):159313.34591670343, grad_J(w):[-276698.86358788 -5468.4907399 ], w:[0.36853515 0.00727351] J(w):92264.27096878138, grad_J(w):[-207747.47773819 -4113.27214627], w:[0.64523401 0.012742 ] J(w):54467.89340270772, grad_J(w):[-155978.24942245 -3095.76362377], w:[0.85298149 0.01685527] J(w):33161.61632500935, grad_J(w):[-117109.516512 -2331.81040782], w:[1.00895974 0.01995103] J(w):21151.008995888868, grad_J(w):[-87926.57430465 -1758.228457 ], w:[1.12606926 0.02228284] J(w):14380.483504690563, grad_J(w):[-66015.79790157 -1327.57870932], w:[1.21399583 0.02404107] J(w):10563.855732624896, grad_J(w):[-49565.0192525 -1004.2435539], w:[1.28001163 0.02536865] J(w):8412.375949653142, grad_J(w):[-37213.64871322 -761.48101583], w:[1.32957665 0.02637289] J(w):7199.560294294272, grad_J(w):[-27940.14516359 -579.21301566], w:[1.3667903 0.02713437] 将损失函数迭代优化的过程和最终的目标函数画图做出 import matplotlib.pyplot as plt plt.figure(figsize=(20, 8)) ax1 = plt.subplot(1, 2, 1) plt.plot(J_list) ax2 = plt.subplot(1, 2, 2) plt.scatter(points[:,0], points[:,1]) # 针对每一个x，计算出预测的y值 pred_y = [f(w_list[-1], x) for x in X] plt.plot(points[:,0], pred_y, c='r') plt.show() 梯度下降法学习率的设定 在上面线性回归梯度下降的代码中可以发现步长或者说学习率alpha = 0.000001，是一个非常小的值。刚开始写代码的时候也没想到alpha会如此之小，以为0.01或者0.001就可以，但是发现都不行，计算结果都会导致损失函数完全无法收敛，反而变的巨大无比。经过不断尝试才最终选到一个这么小的学习率。 在梯度下降法中，学习率的设定是重要且敏感的。不合适的学习率可能导致如下问题： 学习率设置太小，需要花费过多的时间来收敛 学习率设置较大，无法收敛到最小值 进入局部极值点就收敛，没有真正找到的最优解 停在鞍点处，不能够在另一维度继续下降 过拟合与正则化 过拟合：在训练数据集上拟合效果不错， 但是在测试数据集上拟合效果却不好。通常原因在于原始的样本维度或者说特征过多，存在一些嘈杂特征， 模型过于复杂是因为模型尝试去兼顾每一个训练数据点。 欠拟合：训练数据上拟合的不好，并且在测试数据集上也不能很好地拟合。通常原因是学习到的特征过少，需要增加训练数据的特征数量。 正则化是一种降低模型复杂度来解决过拟合问题的方法。如下图所示，如果我们能有效降低高次项对模型的影响，则能解决模型的过拟合问题。 线性回归中正则化实际的操作方式是在损失函数中增加惩罚项，在原有的损失函数中增加对参数的限制。 L1正则（L1-norm），Lasso： J(θ)=12∑i=1m(fθ(xi)−yi)2+λ∑j=1n∣θj∣ J(\\boldsymbol{\\theta})=\\frac{1}{2}\\sum_{i=1}^m(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}^i)-y^i)^2+\\lambda\\sum_{j=1}^n|\\theta_j| J(θ)=21​i=1∑m​(fθ​(xi)−yi)2+λj=1∑n​∣θj​∣ L2正则（L2-norm），岭回归（Ridge）： J(θ)=12∑i=1m(fθ(xi)−yi)2+λ∑j=1nθj2 J(\\boldsymbol{\\theta})=\\frac{1}{2}\\sum_{i=1}^m(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}^i)-y^i)^2+\\lambda\\sum_{j=1}^n\\theta_j^2 J(θ)=21​i=1∑m​(fθ​(xi)−yi)2+λj=1∑n​θj2​ 其中 λ\\lambdaλ 为正则化力度，用来控制正则项的惩罚力度，要求 λ>0\\lambda>0λ>0 。 L1 的最优解是稀疏的，L1 会趋向于产生少量的特征，这些特征的值相对较大，而其他的特征都是 0 或者几乎为 0 值。L2 则会选择更多的特征。与 L1 范数相比，L2 的特征值几乎很少有非常明显的大值，都是一些相对较小的值，甚至接近于 0，但不会像 L1 范数等于 0。L2 范数最优化解出这些小的参数，并非没有好处，因为越小的参数意味着模型越简单，越简单的模型就越不容易产生过拟合现象。而且从参数的分布来看，几乎很少出现突兀的大的峰谷值，它们更多地会呈现缓和平稳的状态，如果没有离群值，那么这种小而平稳的参数分布会使得模型拟合的更好，所以一般情况下，L2 范数往往比 L1 范数表现的更好。 通过sklearn实现线性回归 sklearn中的线性模型都在linear_model这个模块中，详见官方文档。以下利用波士顿房价预测数据集做线性回归的实验。sklearn中内置了该数据集，也可以在其UCI官网下载。实验用到了常见的三个线性回归API：LinearRegression ，SGDRegressor和Ridge。 LinearRegression：前文提到过，就是采用正规方程的方式求解线性回归。因其计算复杂度过高，所以不适用于大数据集。 SGDRegressor：这个API采用的就是梯度下降法。而且梯度下降迭代的算法采用的是随机梯度下降算法SGD（Stochastic Gradient Descent）。前面我们自己写的梯度下降法，在每次迭代的时候会用上全部的样本，如果训练集数据量很大，单次迭代就会计算量巨大。而SGD每次迭代的时候随机得选择一个样本做更新，就可以减少单次迭代的计算量，但也带来了收敛速度过慢的问题。 Ridge：顾名思义，就是梯度下降法增加了L2正则化。但实际上SGDRegressor也可以通过penalty参数指定L2正则。Ridge的主要优势在于solver参数，通过solver参数指定更多更有优势的梯度下降迭代算法，比如随机平均梯度下降算法SAG（Stochastic Average Gradient）。 import numpy as np from sklearn import datasets, linear_model from sklearn.metrics import mean_squared_error, r2_score from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler def load_and_preprocess_data(): boston_data = datasets.load_boston() # 加载波士顿房价预测数据集，sklearn内置了该数据集 # 将数据集分解为测试集和训练集 x_train, x_test, y_train, y_test = train_test_split(boston_data.data, boston_data.target, test_size=0.25, # 设置测试集的比例占全体数据集的比例 random_state=16) # 设置随机数种子，相同的随机数种子每次都会产生完全一样的划分结果 transfer = StandardScaler() x_train = transfer.fit_transform(x_train) # 对训练集数据进行标准化处理 x_test = transfer.transform(x_test) # 注意：这里是基于训练集标准化后的均值和方差，对测试集进行标准化 return x_train, x_test, y_train, y_test x_train, x_test, y_train, y_test = load_and_preprocess_data() # 线性回归的正规方程解法 def LinearRegression(): regr = linear_model.LinearRegression() regr.fit(x_train, y_train) # 用训练数据集训练回归模型 print(\"regr.coef_: {}\".format(regr.coef_)) # 输出回归权重系数 print(\"regr.intercept_: {}\".format(regr.intercept_)) # 输出回归偏置 y_predict = regr.predict(x_test) # 利用测试集进行预测 print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_predict)) # 模型评估：均方误差 print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_predict)) # 模型评估：R2 Score #mse = sum((y_predict - y_test) ** 2) / len(y_test) # 模型评估：手动计算均方误差 #print('Mean squared error: {:.2f}'.format(mse)) #print('Coefficient of determination: {:.2f}'.format(1 - mse/np.var(y_test))) # 模型评估：手动计算R2 Score # 线性回归的梯度下降解法 def SGDRegressor(): regr = linear_model.SGDRegressor(loss=\"squared_error\", # 损失函数选择均方误差 penalty=\"l2\", # 选择L2正则化 learning_rate = \"invscaling\", # 学习率算法选择\"invscaling\" eta0=0.01, # 初始学习率 max_iter=10000) # 最大迭代次数 regr.fit(x_train, y_train) print(\"regr.coef_: {}\".format(regr.coef_)) # 输出回归权重系数 print(\"regr.intercept_: {}\".format(regr.intercept_)) # 输出回归偏置 y_predict = regr.predict(x_test) print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_predict)) # 模型评估：均方误差 print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_predict)) # 模型评估：R2 Score # 岭回归，sklearn里面线性回归的另一种API。SGDRegressor中采用的SGD梯度迭代算法收敛速度比较慢 # Ridge中可以选择更具优势的SAG等梯度迭代算法 def Ridge(): regr = linear_model.Ridge(solver=\"sag\") # 通过solver参数指定不同的梯度下降迭代算法，默认为'auto' regr.fit(x_train, y_train) # 用训练数据集训练回归模型 print(\"regr.coef_: {}\".format(regr.coef_)) # 输出回归权重系数 print(\"regr.intercept_: {}\".format(regr.intercept_)) # 输出回归偏置 y_predict = regr.predict(x_test) # 利用测试集进行预测 print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_predict)) # 模型评估：均方误差 print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_predict)) # 模型评估：R2 Score print(\"**LinearRegression**\") LinearRegression() print(\"\\n**SGDRegressor**\") SGDRegressor() print(\"\\n**Ridge**\") Ridge() **LinearRegression** regr.coef_: [-1.06891253 0.95176047 0.17259817 0.65135791 -1.77990874 2.71415562 0.54171198 -2.76672234 2.76591949 -1.90356573 -2.1589082 0.90951967 -4.43823575] regr.intercept_: 23.007651715039604 Mean squared error: 20.95 Coefficient of determination: 0.67 **SGDRegressor** regr.coef_: [-1.00796106 0.87759167 -0.08135132 0.68205057 -1.64984336 2.76875306 0.4784067 -2.73216775 2.1073112 -1.22990291 -2.09970995 0.91730372 -4.41707619] regr.intercept_: [23.01866033] Mean squared error: 21.00 Coefficient of determination: 0.67 **Ridge** regr.coef_: [-1.05634142 0.93529322 0.14022086 0.65830396 -1.74497297 2.72588877 0.52841003 -2.73983393 2.67258332 -1.81787871 -2.14530232 0.90853341 -4.41870355] regr.intercept_: 23.007651715039604 Mean squared error: 20.88 Coefficient of determination: 0.68 通过PyTorch实现线性回归 这里通过两种方式实现线性回归： 只利用PyTorch中的自动求导autogard，其它尽量和原来保持不变 完全利用PyTorch框架以及提供的相关函数接口，采用PyTorch习惯的方式实现 首先，我们只利用PyTorch中的自动求导autogard，尽量少的使用PyTorch的API来重写之前手写的线性回归代码。主要是替代其中自己计算的均方误差的损失函数的梯度。可以看到代码相比之前有所简化，但是最终结果与原来完全一致。 import numpy as np import torch points = np.genfromtxt('data.csv', delimiter=',') m = len(points) # 样本量 n = len(points[0]) # 样本维数 # 从points中取出样本列，并在每个样本后面增加一个1 X = np.c_[points[:,0], np.ones((m, 1))] X = torch.from_numpy(X) # 从points中取出目标值向量 Y = points[:,1].reshape(-1, 1) # 转成列向量 Y = torch.from_numpy(Y) # 初始化权重系数 W = torch.tensor([[0, 0]], dtype=float, requires_grad=True) # 目标线性函数 y = f(x) def f(w, x): return torch.mm(x, w.T) # 叉乘 # 均方误差损失函数 J(w) def J(w): y_hat = f(w, X) return (0.5 * (y_hat - Y) ** 2).sum() alpha = torch.tensor(0.000001) # 设置学习率 num_iter = 10 # 迭代次数 for i in range(num_iter): valueJ = J(W) valueJ.backward() # 反向传播，求权重系数的梯度 print(\"J(w):{}, grad_J(w):{}, w:{}\".format(valueJ, W.grad.detach().numpy(), W.detach().numpy())) W = W.detach() - alpha * W.grad # 梯度下降，迭代权重系数 W.requires_grad_(True) J(w):278255.39172416076, grad_J(w):[[-368535.14867955 -7273.50505537]], w:[[0. 0.]] J(w):159313.34617426153, grad_J(w):[[-276698.86381974 -5468.49074446]], w:[[0.36853515 0.00727351]] J(w):92264.2712591587, grad_J(w):[[-207747.47808636 -4113.27215311]], w:[[0.64523401 0.012742 ]] J(w):54467.89364824159, grad_J(w):[[-155978.24981457 -3095.76363147]], w:[[0.85298149 0.01685527]] J(w):33161.61650955656, grad_J(w):[[-117109.51690453 -2331.81041554]], w:[[1.00895974 0.01995103]] J(w):21151.00912592823, grad_J(w):[[-87926.57467305 -1758.22846424]], w:[[1.12606925 0.02228284]] J(w):14380.4835926563, grad_J(w):[[-66015.79823348 -1327.57871585]], w:[[1.21399583 0.02404107]] J(w):10563.855790476817, grad_J(w):[[-49565.01954324 -1004.24355961]], w:[[1.28001163 0.02536865]] J(w):8412.37598692381, grad_J(w):[[-37213.64896269 -761.48102074]], w:[[1.32957665 0.02637289]] J(w):7199.560317930455, grad_J(w):[[-27940.14537431 -579.2130198 ]], w:[[1.3667903 0.02713437]] 其次，我们完全利用PyTorch提供的相关接口函数来实现线性回归。实验数据集还是采用在sklearn实验中所用到的波士顿房价预测数据集。如果运行环境存在可用GPU，则TyTorch可以将梯度下降的迭代运算放到GPU上进行，对于大数据集的复杂模型训练，将极大的提升运行效率。 import numpy as np import torch import torch.nn as nn from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler def load_and_preprocess_data(): boston_data = datasets.load_boston() # 加载波士顿房价预测数据集，sklearn内置了该数据集 # 将数据集分解为测试集和训练集 x_train, x_test, y_train, y_test = train_test_split(boston_data.data, boston_data.target, test_size=0.25, # 设置测试集的比例占全体数据集的比例 random_state=16) # 设置随机数种子，相同的随机数种子每次都会产生完全一样的划分结果 transfer = StandardScaler() x_train = transfer.fit_transform(x_train) # 对训练集数据进行标准化处理 x_test = transfer.transform(x_test) # 注意：这里是基于训练集标准化后的均值和方差，对测试集进行标准化 return x_train, x_test, y_train, y_test x_train, x_test, y_train, y_test = load_and_preprocess_data() device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') class LinearRegression(nn.Module): def __init__(self, input_dim, output_dim): super().__init__() self.linear = nn.Linear(input_dim, output_dim) def forward(self, x): out = self.linear(x) return out input_dim = 13 output_dim = 1 LRModel=LinearRegression(input_dim, output_dim) LRModel.to(device) # 如果有能用GPU计算，就把model放到GPU上去，后续就会用GPU计算 epoches = 1000 learning_rate = 0.01 optimizer = torch.optim.SGD(LRModel.parameters(), lr=learning_rate) criterion = nn.MSELoss() # ndarray转化成tensor；数据类型变为float32和模型权重参数的数据类型保持一致；如果有能用的GPU，就把数据放到GPU上去 inputs = torch.from_numpy(x_train).to(torch.float32).to(device) # ndarray转化成tensor；形状变成n行1列的矩阵；数据类型变为float32和模型权重参数的数据类型保持一致；如果有能用的GPU，就把数据放到GPU上去 labels = torch.from_numpy(y_train).reshape(-1, 1).to(torch.float32).to(device) for epoch in range(epoches): optimizer.zero_grad() # 每一次迭代梯度要清零 outputs = LRModel(inputs) # 前向传播 loss = criterion(outputs, labels) # 计算损失 loss.backward() # 反向传播 optimizer.step() # 梯度下降迭代 if epoch % 100 == 0: print(\"epoch {}, loss {}\".format(epoch, loss.item())) print(\"inear.weight: {}\".format(LRModel.linear.weight)) # 输出权重参数 print(\"inear.bias: {}\".format(LRModel.linear.bias)) # 输出偏置 epoch 0, loss 604.8123168945312 epoch 100, loss 33.90818786621094 epoch 200, loss 23.943044662475586 epoch 300, loss 23.43199920654297 epoch 400, loss 23.245315551757812 epoch 500, loss 23.13983154296875 epoch 600, loss 23.075153350830078 epoch 700, loss 23.033143997192383 epoch 800, loss 23.004531860351562 epoch 900, loss 22.98432159423828 inear.weight: Parameter containing: tensor([[-1.0230, 0.8905, -0.0299, 0.6863, -1.6675, 2.7600, 0.4912, -2.7511, 2.2308, -1.3380, -2.1101, 0.9156, -4.4212]], requires_grad=True) inear.bias: Parameter containing: tensor([23.0076], requires_grad=True) x_test_tensor = torch.from_numpy(x_test).to(torch.float32).to(device) # 把ndarray转成tensor；如果有能用的GPU，就把数据放到GPU上去 y_test_tensor = torch.from_numpy(y_test).reshape(-1, 1).to(torch.float32).to(device) # 把ndarray转成tensor；如果有能用的GPU，就把数据放到GPU上去 y_predict = LRModel(x_test_tensor) mse = criterion(y_predict, y_test_tensor) # 计算均方误差 print(\"Mean squared error: {:.2f}\".format(mse)) # 模型评估：均方误差 Mean squared error: 20.99 附1：极大似然估计法下的损失函数 "},"LogisticRegression.html":{"url":"LogisticRegression.html","title":"逻辑回归","keywords":"","body":"Logistic 回归虽然名字里带 “回归”，但它实际上是一种二分类方法，即 LR 分类器（Logistic Regression Classifier），其数学模型是一个 sigmoid 函数，因图像像 S，又经常称之为 S 形曲线，sigmoid 本身就是 S 形的意思。 sigmoid 函数： g(z)=11+e−z g(z) =\\frac{1}{1+e^{-z}} g(z)=1+e−z1​ sigmoid 函数图像： 由于sigmoid函数可以将任意值映射到(0, 1)的区间内，类似求得了一个概率值。如果假设需要判断一个测试样本是否属于某一类别，设置0.5为阈值，sigmoid函数的输出如果大于0.5则认为属于该类别，小于0.5则认为不属于该类别。即可实现二分类问题。 逻辑回归的目标函数 逻辑回归和线性回归一脉相承。假设已知数据集 X\\boldsymbol{X}X ，其中有 mmm 个 nnn 维样本 xi\\boldsymbol{x}^ixi ，iii 代表第 iii 个样本。每个样本 xi\\boldsymbol{x}^ixi 对应着一个标签 yiy^iyi ，与线性回归不同的是，这里 yiy^iyi 的值是0或1，而不是任意实数。把标签值 yiy^iyi 的集合（向量）记作 Y\\boldsymbol{Y}Y 。 逻辑回归的目标函数是一个sigmoid的函数，函数的输入就是线性回归的目标函数。线性回归的目标函数为： f(x)=θTx f(\\boldsymbol{x}) = \\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x} f(x)=θTx 则，逻辑回归的目标函数是： σ(x)=hθ(x)=g(f(x))=g(θTx)=11+e−θTx \\sigma(\\boldsymbol{x})=h_{\\boldsymbol{\\theta}}(\\boldsymbol{x})=g(f(\\boldsymbol{x}))=g(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x})=\\frac{1}{1+e^{-\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}}} σ(x)=hθ​(x)=g(f(x))=g(θTx)=1+e−θTx1​ 整个流程绘制成流程图显示如下，之所以要画这样子的流程图，是因为逻辑回归的这个流程图已经很像后续深度学习中的感知机的模样，只是激活函数使用的是sigmoid函数，这样流程图在后续的深度学习中会经常遇见： 逻辑回归的损失函数 逻辑回归的损失函数是一个分段函数： Cost(hθ(x),y)={−log(hθ(x))if y=1−log(1−hθ(x))if y=0 \\begin{align*} Cost(h_{\\boldsymbol{\\theta}}(\\boldsymbol{x}), y) = \\left\\{ \\begin{array}{lr}\t -log(h_{\\boldsymbol{\\theta}}(\\boldsymbol{x})) &if \\ y=1\\\\ -log(1-h_{\\boldsymbol{\\theta}}(\\boldsymbol{x})) &if \\ y=0 \\end{array} \\right. \\end{align*} Cost(hθ​(x),y)={−log(hθ​(x))−log(1−hθ​(x))​if y=1if y=0​​ 下图为 −log(hθ(x))-log(h_{\\boldsymbol{\\theta}}(\\boldsymbol{x}))−log(hθ​(x)) 和 −log(1−hθ(x))-log(1-h_{\\boldsymbol{\\theta}}(\\boldsymbol{x}))−log(1−hθ​(x)) 的函数图像。因为sigmod函数的输出为(0, 1)的区间，所以以下函数图像只显示(0, 1)区间的取值。可以看出当样本真实的标签为1时，预测值越接近于1则损失趋近于0，预测值越接近于0则损失趋近于无穷大；反之，如果样本真实的标签为0时，预测值越接近于1则损失趋近于无穷大，预测值越接近于0则损失趋近于0。 于是接下来要做的事情就是优化损失函数，找到损失函数最小时对应的参数组 θ\\boldsymbol{\\theta}θ 。但是毕竟分段函数不利于计算，将分段函数做一下改写，这个函数就叫做负对数似然损失（Negative Log-Likelihood）函数，顾名思义这个损失函数同样可以通过极大似然估计推导而来： J(θ)=−1m∑i=1m[yi⋅log(hθ(xi))+(1−yi)⋅log(1−hθ(xi))] J(\\boldsymbol{\\theta})=-\\frac{1}{m}\\sum_{i=1}^{m}[y^i\\cdot log(h_{\\boldsymbol{\\theta}}(\\boldsymbol{x}^i))+(1-y^i)\\cdot log(1-h_{\\boldsymbol{\\theta}}(\\boldsymbol{x}^i))] J(θ)=−m1​i=1∑m​[yi⋅log(hθ​(xi))+(1−yi)⋅log(1−hθ​(xi))] 类似线性回归，为了防止过拟合，可以加入正则化项，比如L2正则： J(θ)=−1m∑i=1m[yi⋅log(hθ(xi))+(1−yi)⋅log(1−hθ(xi))]+λ2m∑j=1nθj2 J(\\boldsymbol{\\theta})=-\\frac{1}{m}\\sum_{i=1}^{m}[y^i\\cdot log(h_{\\boldsymbol{\\theta}}(\\boldsymbol{x}^i))+(1-y^i)\\cdot log(1-h_{\\boldsymbol{\\theta}}(\\boldsymbol{x}^i))]+\\frac{\\lambda}{2m}\\sum_{j=1}^n\\theta_j^2 J(θ)=−m1​i=1∑m​[yi⋅log(hθ​(xi))+(1−yi)⋅log(1−hθ​(xi))]+2mλ​j=1∑n​θj2​ 梯度下降法优化逻辑回归的损失函数 先求损失函数 J(θ)J(\\boldsymbol{\\theta})J(θ) 的梯度，对每一个分量 θj\\theta_jθj​ 求偏导： ∂J(θ)∂θj=−1m∑i=1m[yi1hθ(xi)∂∂θjhθ(xi)−(1−yi)11−hθ(xi)∂∂θjhθ(xi)]=−1m∑i=1m[(yi1g(θTxi)−(1−yi)11−g(θTxi))⋅∂∂θjg(θTxi)] \\begin{align*} \\frac{\\partial J(\\boldsymbol{\\theta})}{\\partial \\theta_j} &=-\\frac{1}{m}\\sum_{i=1}^{m}[y^i\\frac{1}{h_{\\boldsymbol{\\theta}}(\\boldsymbol{x}^i)} \\frac{\\partial}{\\partial \\theta_j}h_{\\boldsymbol{\\theta}}(\\boldsymbol{x}^i)-(1-y^i) \\frac{1}{1-h_{\\boldsymbol{\\theta}}(\\boldsymbol{x}^i)} \\frac{\\partial}{\\partial \\theta_j}h_{\\boldsymbol{\\theta}}(\\boldsymbol{x}^i)] \\\\ &=-\\frac{1}{m}\\sum_{i=1}^{m}[(y^i\\frac{1}{g(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i)}-(1-y^i) \\frac{1}{1-g(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i)}) \\cdot \\frac{\\partial}{\\partial \\theta_j}g(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i)] \\\\ \\end{align*} ∂θj​∂J(θ)​​=−m1​i=1∑m​[yihθ​(xi)1​∂θj​∂​hθ​(xi)−(1−yi)1−hθ​(xi)1​∂θj​∂​hθ​(xi)]=−m1​i=1∑m​[(yig(θTxi)1​−(1−yi)1−g(θTxi)1​)⋅∂θj​∂​g(θTxi)]​ 其中sigmod函数 g(z)g(z)g(z) 对于 zzz 的导数为： g′(z)=(11+e−z)′=e−z(1+e−z)2=11+e−z⋅e−z1+e−z=11+e−z⋅(1−11+e−z)=g(z)⋅(1−g(z)) \\begin{align*} g'(z) &= (\\frac{1}{1+e^{-z}})' = \\frac{e^{-z}}{(1+e^{-z})^2} \\\\ \t\t\t&= \\frac{1}{1+e^{-z}} \\cdot \\frac{e^{-z}}{1+e^{-z}} \\\\ \t\t\t&= \\frac{1}{1+e^{-z}} \\cdot (1-\\frac{1}{1+e^{-z}}) \\\\ \t\t\t&= g(z) \\cdot (1-g(z)) \\end{align*} g′(z)​=(1+e−z1​)′=(1+e−z)2e−z​=1+e−z1​⋅1+e−ze−z​=1+e−z1​⋅(1−1+e−z1​)=g(z)⋅(1−g(z))​ 将sigmod函数 g(z)g(z)g(z) 对于 zzz 的导数带入前面 J(θ)J(\\boldsymbol{\\theta})J(θ) 的梯度求偏导的计算过程中： ∂J(θ)∂θj=−1m∑i=1m[(yi1g(θTxi)−(1−yi)11−g(θTxi))⋅g(θTxi)(1−g(θTxi))⋅∂∂θjθTxi]=−1m∑i=1m[(yi(1−g(θTxi))−(1−yi)g(θTxi))⋅xji]=−1m∑i=1m[(yi−g(θTxi))xji]=1m∑i=1m[(hθ(xi)−yi)xji] \\begin{align*} \\frac{\\partial J(\\boldsymbol{\\theta})}{\\partial \\theta_j} &=-\\frac{1}{m}\\sum_{i=1}^{m}[(y^i\\frac{1}{g(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i)}-(1-y^i) \\frac{1}{1-g(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i)}) \\cdot g(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i)(1-g(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i)) \\cdot \\frac{\\partial}{\\partial \\theta_j}\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i] \\\\ &=-\\frac{1}{m}\\sum_{i=1}^{m}[(y^i(1-g(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i))-(1-y^i)g(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i)) \\cdot \\boldsymbol{x}_j^i] \\\\ &=-\\frac{1}{m}\\sum_{i=1}^{m}[(y^i-g(\\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}^i))\\boldsymbol{x}_j^i] \\\\ &=\\frac{1}{m}\\sum_{i=1}^{m}[(h_{\\boldsymbol{\\theta}}(\\boldsymbol{x}^i)-y^i)\\boldsymbol{x}_j^i] \\end{align*} ∂θj​∂J(θ)​​=−m1​i=1∑m​[(yig(θTxi)1​−(1−yi)1−g(θTxi)1​)⋅g(θTxi)(1−g(θTxi))⋅∂θj​∂​θTxi]=−m1​i=1∑m​[(yi(1−g(θTxi))−(1−yi)g(θTxi))⋅xji​]=−m1​i=1∑m​[(yi−g(θTxi))xji​]=m1​i=1∑m​[(hθ​(xi)−yi)xji​]​ 对比线性回归均方误差损失函数的梯度，可以看出和逻辑回归损失函数的梯度形式上基本是完全一致的，具体见线性回归的梯度下降法。接下来利用梯度下降法对损失函数进行迭代优化即可找到最优参数 θ\\boldsymbol{\\theta}θ : θj:=θj−α1m∑i=1m[(hθ(xi)−yi)xji] \\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}[(h_{\\boldsymbol{\\theta}}(\\boldsymbol{x}^i)-y^i)\\boldsymbol{x}_j^i] θj​:=θj​−αm1​i=1∑m​[(hθ​(xi)−yi)xji​] 通过sklearn实现逻辑回归 既然逻辑回归损失函数的梯度跟线性回归损失函数的梯度形式上完全一致，我们就不手写代码实现逻辑回归的梯度下降算法了，可以参考线性回归梯度下降算法实现。直接尝试调用sklearn中逻辑回归相关的API。 如果去翻看sklearn的官方文档，会发现linear_model中也包含了3个这样的分类器API：LogisticRegression、SGDClassifier以及RidgeClassifier，和之前线性回归的3个API非常相似。但其实它们没有对应关系。 主要实现逻辑回归的就是LogisticRegression这个接口，但是这个接口中没有包含SGD的实现。而SGDClassifier接口实现了不同损失函数下的SGD算法，其中甚至包含SVM，虽然把损失函数设置为log_loss即为SGD的逻辑回归，但该接口不是专门为逻辑回归设计的。而RidgeClassifier根本就不是逻辑回归，没有用到sigmoid函数，只是用L2正则的线性回归实现了分类的效果，具体可以参考sklearn官方文档。 本实验采用乳腺癌分类预测数据集。数据集描述： 699条样本，共11列数据，第1列为id，后9列是与肿瘤相关的医学特征。 最后一列表示肿瘤类型的数值：2表示良性，4表示恶性。这一列就是分类的目标。 包含16个缺失值，用?标出。 既然数据有一些缺失，在逻辑回归预测之前肯定需要做一些预处理。 import numpy as np import pandas as pd from sklearn import linear_model from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler import matplotlib.pyplot as plt column_name = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'Class'] path = \"http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\" original_data = pd.read_csv(path, names=column_name) # 从源地址下载数据集 def preprocess_data(): data = original_data.replace(to_replace=\"?\", value=np.nan) # 将缺失值\"?\"替换为NaN data.dropna(inplace=True) # 简单处理，把缺失值所在行丢弃 print(\"Is there any null?\\n\", data.isnull().any()) # 检测是否还有缺失值 x = data.iloc[:, 1:-1] # 选择数据集第一行到倒数第二行作为样本数据 y = data[\"Class\"] # 选择Class列作为标签数据 x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, # 设置测试集的比例占全体数据集的比例 random_state=16) # 设置随机数种子，相同的随机数种子每次都会产生完全一样的划分结果 transfer = StandardScaler() x_train = transfer.fit_transform(x_train) # 对训练集数据进行标准化处理 x_test = transfer.transform(x_test) # 注意：这里是基于训练集标准化后的均值和方差，对测试集进行标准化 return x_train, x_test, y_train, y_test x_train, x_test, y_train, y_test = preprocess_data() def LogisticRegression(): classifier = linear_model.LogisticRegression(penalty=\"l2\", # 选择L2正则化 solver=\"sag\") # 选择sag作为梯度下降迭代算法 classifier.fit(x_train, y_train) print(\"regr.coef_: {}\".format(classifier.coef_)) # 输出回归权重系数 print(\"regr.intercept_: {}\".format(classifier.intercept_)) # 输出回归偏置 y_predict = classifier.predict(x_test) # 预测分类结果，给出每个样本具体的类别的判定 y_predict_proba = classifier.predict_proba(x_test) # 预测各分类的概率值，给出具体每个样本不同类别的概率 y_score = y_predict_proba[:,1] # 取第2列，也就是\"恶性\"（正例）的概率 score = classifier.score(x_test, y_test) print(\"mean accuracy: {}\".format(score)) # 准确率 report = classification_report(y_test, y_predict, labels=[2, 4], target_names=[\"良性\", \"恶性\"]) print(report) # 精确率，召回率，F1-score y_true = np.where(y_test > 3, 1, 0) # roc_auc_score函数要求传入的y_true:必须为0或1，0为反例，1为正例（貌似高版本sklearn没有这个要求） AUC_value = roc_auc_score(y_true, y_score) # AUC指标 print(\"AUC value: {}\".format(AUC_value)) fpr, tpr, thresholds = roc_curve(y_true, y_score) # roc_curve函数对y_true的取值是有规范要求的 print(\"AUC value again: {}\".format(auc(fpr, tpr))) # 已知FPR和TPR的情况下求AUC指标 plt.plot(fpr, tpr, label=\"AUC = {:.2f}\".format(AUC_value)) plt.xlabel(\"False Positive Rate\") plt.ylabel(\"True Positive Rate\") plt.title(\"ROC Curve\") plt.legend(loc=\"lower right\") plt.show() LogisticRegression() regr.coef_: [[1.28874969 1.07409426 1.11121314 0.61213973 0.11802989 1.34477968 0.69445954 0.93152164 0.50223701]] regr.intercept_: [-0.83654326] mean accuracy: 0.9590643274853801 precision recall f1-score support 良性 0.98 0.96 0.97 117 恶性 0.91 0.96 0.94 54 accuracy 0.96 171 macro avg 0.95 0.96 0.95 171 weighted avg 0.96 0.96 0.96 171 AUC value: 0.9893953782842672 AUC value again: 0.9893953782842672 分类问题的模型评估 以上sklearn代码包含了不少模型评估相关的内容。线性回归的模型评估相对简单，直接利用均方误差即可。但分类问题的模型评估就相对复杂。 混淆矩阵 在分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵(适用于多分类)，如下所示： #### 精确率(Precision)与召回率(Recall) **精确率(Precision)**：预测结果为正例的样本中真正为正例的比例 Precision=TPTP+FP Precision = \\frac{TP}{TP+FP} Precision=TP+FPTP​ 召回率(Recall)：真实为正例的样本中预测结果为正例的比例 Recall=TPTP+FN Recall = \\frac{TP}{TP+FN} Recall=TP+FNTP​ F1-score：综合考量了模型的精确率和召回率，相对比较稳健的一个评估指标 F1=2TP2TP+FN+FP=2⋅Precision⋅RecallPrecision+Recall F1 = \\frac{2TP}{2TP+FN+FP}=\\frac{2 \\cdot Precision \\cdot Recall}{Precision+Recall} F1=2TP+FN+FP2TP​=Precision+Recall2⋅Precision⋅Recall​ ROC曲线与AUC指标 真正率（召回率）：正确的样本判断为正例的比例。所有真实类别为1的样本中，预测类别为1的比例。 TPR=TPTP+FN TPR = \\frac{TP}{TP+FN} TPR=TP+FNTP​ 假正率：错误的样本判断为正例的比例。所有真实类别为0的样本中，预测类别为1的比例。 FPR=FPFP+TN FPR=\\frac{FP}{FP+TN} FPR=FP+TNFP​ 真正率和假正率是ROC曲线会用到的两个数值指标。 逻辑回归分类器的输出实际并非样本的类别标签0或1，而是(0, 1)之间的一个”概率值”，表示样本属于该类别的可能性。如果想要获得样本的标签，就需要设定一个阈值，如果概率值高于阈值则标签为1，反之则标签为0。如果调整阈值的大小，那么相应的，一部分样本的标签值可能会发生变化。 [!NOTE] 这个理解其实跟真实逻辑回归的计算不太一样。对于二分类问题，逻辑回归分类器做预测时最终会输出两个类别的概率值，哪个概率值高，样本就属于哪个类别。 假设分类器预测一个样本标签为1的概率为 AAA ，则标签为0的概率分类器实际就是通过 1−A1-A1−A 计算而来。然后再来判断 AAA 和 1−A1-A1−A 到底哪个大，然后决定最终的标签。 这样一来，对于二分类问题，阈值其实始终都是0.5，跟刚才说的调整阈值大小有点不太一样。如何理解这一点呢？实际在逻辑回归训练的过程中，可以理解为分类器将阈值设定为0.5，通过不断训练调整输出的概率值，以满足这个概率最终能被判定为相应的标签。简单说就是确定阈值，调整概率值，来达到最优化的效果。 如果我们将阈值Threshold从0开始逐步调整到1，每调整一步，就可以获得一对真正率和假正率构成的二维坐标 (FPR,TPR)(FPR,TPR)(FPR,TPR) ，将这些坐标绘制在二维坐标系中，就会形成一条曲线。这条曲线就是ROC曲线。ROC曲线下所包含的面积即为AUC指标（Area Under the Curve of ROC），是一个数值。 对于ROC曲线，曲线的凸起程度越高就代表着模型的性能越好，而对角线意味着一个随机瞎猜的分类器模型。从下图可以看出，三个模型 M1M1M1 优于 M2M2M2 ，M2M2M2 优于 M3M3M3 。 对于AUC指标，是ROC曲线下的面积，也即是被限制在1乘1方格中一部分的面积值，所以AUC指标的取值也在0到1之间。 AUC=1AUC = 1AUC=1 ，完美的分类器，该模型至少存在一个阈值，可以将正负样本完美的划分开。 0.5AUC10.50.5AUC1 ，优于随机猜测，数值越大，分类器越好 AUC=0.5 AUC = 0.5AUC=0.5 ，相当于随机猜测，模型没有预测价值 AUC0.5AUCAUC0.5 ，比随机猜测还要差，然而若反向预测，该模型即可优于随机猜测 ROC曲线的绘制步骤： 将全部样本按分类器输出的概率值降序排列。 将阈值Theshold从1到0逐步减小，计算各阈值下对应的(FPR,TPR)(FPR,TPR)(FPR,TPR)数值对。实际操作中，阈值Theshold的取值可以从1开始，逐个选取按降序排列的样本概率值，直至0。 将数值对绘于直角坐标系中，形成曲线。实际操作中，ROC曲线不是一个曲线，而是阶梯线，见代码示例。只有当样本量巨大时，会趋近于曲线。 用PyTorch实现逻辑回归 现在尝试用PyTorch解决乳腺癌分类预测数据集数据集的逻辑回归二分类问题。和PyTorch实现的线性回归代码做比较，可以看到，两者的差异非常小。差异只存在于： 模型构建的时候增加了sigmoid函数 损失函数采用了交叉熵损失函数BCE（Binary Cross Entropy Loss）。这里的BCE指的就是前面所说的负对数似然函数。 代码如下： import numpy as np import pandas as pd import torch import torch.nn as nn from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler column_name = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'Class'] path = \"http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\" original_data = pd.read_csv(path, names=column_name) # 从源地址下载数据集 def preprocess_data(): data = original_data.replace(to_replace=\"?\", value=np.nan) # 将缺失值\"?\"替换为NaN data.dropna(inplace=True) # 简单处理，把缺失值所在行丢弃 print(\"Is there any null?\\n\", data.isnull().any()) # 检测是否还有缺失值 x = data.iloc[:, 1:-1].to_numpy() # 选择数据集第一行到倒数第二行作为样本数据 y = data[\"Class\"].apply(lambda x: 1 if x == 4 else 0).to_numpy() # 选择Class列作为标签数据，将标签为4的转化为1，标签为2的转化为0（sklearn中不需要做这一操作） x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, # 设置测试集的比例占全体数据集的比例 random_state=16) # 设置随机数种子，相同的随机数种子每次都会产生完全一样的划分结果 transfer = StandardScaler() x_train = transfer.fit_transform(x_train) # 对训练集数据进行标准化处理 x_test = transfer.transform(x_test) # 注意：这里是基于训练集标准化后的均值和方差，对测试集进行标准化 return x_train, x_test, y_train, y_test x_train, x_test, y_train, y_test = preprocess_data() device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') class LogisticRegression(torch.nn.Module): def __init__(self, input_dim, output_dim): super(LogisticRegression, self).__init__() self.linear = torch.nn.Linear(input_dim, output_dim) def forward(self, x): outputs = torch.sigmoid(self.linear(x)) # 全连接增加sigmoid函数 return outputs epochs = 1000 input_dim = 9 output_dim = 1 learning_rate = 0.01 LRModel = LogisticRegression(input_dim, output_dim) LRModel.to(device) # 如果有能用GPU计算，就把model放到GPU上去，后续就会用GPU计算 criterion = torch.nn.BCELoss() # 二分类交叉熵损失函数（Binary Cross Entropy Loss） optimizer = torch.optim.SGD(LRModel.parameters(), lr=learning_rate) # ndarray转化成tensor；数据类型变为float32和模型权重参数的数据类型保持一致；如果有能用的GPU，就把数据放到GPU上去 inputs = torch.from_numpy(x_train).to(torch.float32).to(device) # ndarray转化成tensor；形状变成n行1列的矩阵；数据类型变为float32和模型权重参数的数据类型保持一致；如果有能用的GPU，就把数据放到GPU上去 labels = torch.from_numpy(y_train).reshape(-1, 1).to(torch.float32).to(device) for epoch in range(epochs): optimizer.zero_grad() # 每一次迭代梯度要清零 outputs = LRModel(inputs) # 前向传播 loss = criterion(outputs, labels) # 计算损失 loss.backward() # 反向传播 optimizer.step() # 梯度下降迭代 if epoch % 100 == 0: print(\"epoch {}, loss {}\".format(epoch, loss.item())) epoch 0, loss 0.6377063393592834 epoch 100, loss 0.2625509202480316 epoch 200, loss 0.1790274977684021 epoch 300, loss 0.14283452928066254 epoch 400, loss 0.12268733233213425 epoch 500, loss 0.109866663813591 epoch 600, loss 0.10099361836910248 epoch 700, loss 0.09448736160993576 epoch 800, loss 0.08951068669557571 epoch 900, loss 0.08557910472154617 softmax回归实现多分类 逻辑回归主要用来解决二分类问题，多分类问题转化为二分类再用逻辑回归显然不是一个高效的解决方案。在多分类问题中我们通常使用softmax回归。 先给出softmax函数的定义： σ(z)k=ezk∑l=1Kezlfor k=1,⋯ ,K \\sigma(\\boldsymbol{z})_k = \\frac{e^{z_k}}{\\sum_{l=1}^K e^{z_l}} \\quad for\\ k=1,\\cdots,K σ(z)k​=∑l=1K​ezl​ezk​​for k=1,⋯,K softmax函数的输入是一个向量 z\\boldsymbol{z}z ，输出也是一个向量，将输出向量的第 kkk 个分量的值表示为 σ(z)k\\sigma(\\boldsymbol{z})_kσ(z)k​ 。 softmax回归的目标函数 再次重新描述一下我们的问题：假设已知数据集 X\\boldsymbol{X}X ，其中有 mmm 个 nnn 维样本 xi\\boldsymbol{x}^ixi ，iii 代表第 iii 个样本。每个样本 xi\\boldsymbol{x}^ixi 对应着一个标签 yiy^iyi ，与线性回归和逻辑回归都不同的是，这里 yiy^iyi 的值既不是任意实数，也不仅仅只是0或1的类别标签，而是 KKK 个类别的标签。把标签值 yiy^iyi 的集合（向量）记作 Y\\boldsymbol{Y}Y 。 在实际操作过程中，会将 yiy^iyi 表示为One-Hot编码，变成一个 KKK 维向量 yi\\boldsymbol{y}^iyi ，即第一个类别表示为 (1,0,⋯ ,0)(1,0,\\cdots,0)(1,0,⋯,0) ， 第二类别为 (0,1,⋯ ,0)(0,1,\\cdots,0)(0,1,⋯,0) ，以此类推，最后一个类别为(0,0,⋯ ,1)(0,0,\\cdots,1)(0,0,⋯,1) 。 在具体求解时，类似逻辑回归，需要将 f(x)=θTxf(\\boldsymbol{x}) = \\boldsymbol{\\theta}^\\mathsf{T} \\boldsymbol{x}f(x)=θTx 带入softmax函数作为目标函数，利用极大似然估计求得损失函数，对损失函数做梯度下降优化，最终解得回归方程的权重系数 θ\\boldsymbol{\\theta}θ 。这一套已经反复在线性回归，逻辑回归的学习中不断重复过。 与逻辑回归不同的时，此时求解的 θ\\boldsymbol{\\theta}θ 不再是一个 nnn 维向量，而是 KKK 个 nnn 维向量构成的二维矩阵 θn×K\\boldsymbol{\\theta}_{n\\times K}θn×K​ 。每一个样本 x\\boldsymbol{x}x 对应 KKK 套权重系数，每一套权重系数 θk\\boldsymbol{\\theta}_kθk​ 与 x\\boldsymbol{x}x 相乘后输入softmax函数得到一个相应类别的概率值。对于每个样本 x\\boldsymbol{x}x 最终会输出有 KKK 个概率值对应 KKK 个类别。其中，哪个类别对应的概率值大，则预测该样本属于这个类别。 所以对于样本 x\\boldsymbol{x}x ，对应第 kkk 个类别（需要使用第 kkk 套权重系数 θk\\boldsymbol{\\theta}_kθk​ ）的目标函数为： hθ(x)k=exp⁡(θkTx)∑l=1Kexp⁡(θlTx)for k=1,⋯ ,K h_{\\boldsymbol{\\theta}}(\\boldsymbol{x})_k = \\frac{\\exp(\\boldsymbol{\\theta}_k^\\mathsf{T} \\boldsymbol{x})}{\\sum_{l=1}^K \\exp(\\boldsymbol{\\theta}_l^\\mathsf{T} \\boldsymbol{x})} \\quad for\\ k=1,\\cdots,K hθ​(x)k​=∑l=1K​exp(θlT​x)exp(θkT​x)​for k=1,⋯,K 下面还是用流程图来展示整个计算过程： logistic回归与softmax回归的关系 现在我们再回过头来看logistic回归，其实它是softmax回归的特例，即类别数 K=2K=2K=2 ，softmax函数的输入为 z=(z1,z2)\\boldsymbol{z}=(z_1,z_2)z=(z1​,z2​) 。 考虑到最终我们求得的是不同类别的概率值，那么已知 z1z_1z1​ 输出的概率为 ppp ，那么 z2z_2z2​ 输出的概率显然应该是 1−p1-p1−p ，完全不用专门计算。于是 z2z_2z2​ 具体是多少完全不重要，可以取任意值，只不过取值不同，分类器在训练的时候就会自适应这个取值，算得一套合适的 θ1\\boldsymbol{\\theta}_1θ1​ ，最终得到一样的输出概率。那么我们令 z2=0z_2 = 0z2​=0 ，则： σ(z)1=ez1ez1+ez2=ez1ez1+e0=ez1ez1+1=11+e−z1 \\sigma(\\boldsymbol{z})_1 = \\frac{e^{z_1}}{e^{z_1}+e^{z_2}} = \\frac{e^{z_1}}{e^{z_1}+e^0} = \\frac{e^{z_1}}{e^{z_1}+1} = \\frac{1}{1+e^{-z_1}} σ(z)1​=ez1​+ez2​ez1​​=ez1​+e0ez1​​=ez1​+1ez1​​=1+e−z1​1​ 所以可以看出，对于二分类的softmax函数，如果令 z2=0z_2 = 0z2​=0 ，则softmax函数转化为sigmod函数。 同时通过这个例子也可以看出，对于有 KKK 个类别的softmax回归问题，只需要求解 K−1K-1K−1 套 θk\\boldsymbol{\\theta}_kθk​ 即可，最后一组 θk\\boldsymbol{\\theta}_kθk​ 的值可以任意选取。也就是说 KKK 分类的softmax回归问题的自由度是 K−1K-1K−1，参数矩阵 θ\\boldsymbol{\\theta}θ 可以简化为一个 n×(K−1)n\\times (K-1)n×(K−1) 的二维矩阵。 softmax回归的损失函数 通过极大似然估计可以推导出softmax回归的负对数似然损失函数为： J(θ)=−1m∑i=1m∑k=1K(yki⋅logexp⁡(θkTxi)∑l=1Kexp⁡(θlTxi)) J(\\boldsymbol{\\theta})= -\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^K(\\boldsymbol{y}_k^i\\cdot log\\frac{\\exp(\\boldsymbol{\\theta}_k^\\mathsf{T} \\boldsymbol{x}^i)}{\\sum_{l=1}^K \\exp(\\boldsymbol{\\theta}_l^\\mathsf{T} \\boldsymbol{x}^i)}) J(θ)=−m1​i=1∑m​k=1∑K​(yki​⋅log∑l=1K​exp(θlT​xi)exp(θkT​xi)​) 其中，yi\\boldsymbol{y}^iyi 为第 iii 个样本的 KKK 维标签向量，是个One-Hot编码。 "},"DecisionTree.html":{"url":"DecisionTree.html","title":"决策树","keywords":"","body":"决策树案例 先从一个”相亲“案例出发直观认识决策树。已知”相亲对象的条件“样本和”是否去见“的决策，可以建立一个决策树。当有新相亲对象样本来的时候根据决策树来预测”是否去见“的决策。 相亲对象的样本以及决策如下： 白 富 美 行动 是 是 是 去 是 是 否 去 是 否 是 犹豫 是 否 否 犹豫 否 是 是 去 否 是 否 去 否 否 是 犹豫 否 否 否 不去 可以建立的一个符合以上决策的决策树如下： 显然，如果我们先考虑富不富，再考虑白不白，则可以得到不一样的一颗决策树： 所以，可以看到，如果按照不同维度的先后顺序来建立决策树，可能会得到不同的树。 观察以上决策树可以发现其中有一些叶子节点是可以合并的，合并之后，到达某个节点时就不需要进行额外的决策，例如按照“白，富，美”顺序得到的决策树合并后如下： 而按照“富，白，美”顺序的决策树合并后变成： 可以看到上面这棵树只有 4 个叶子节点，少于“白，富，美”顺序的决策树的 5 个节点。这就是决策树间最大的区别，不同决策树合并后得到树叶子节点的个数是不同的，后面我们会看到，叶子节点越少，往往决策树的泛化能力越高，所以可以认为训练决策树的一个目标是减少决策树的叶子节点 。 基于ID3算法的决策树构建 给出样本，建立一颗决策树并不困难。如何建立一颗好的决策树，才是需要考虑的问题。基于不同维度的先后判断顺序可以建立很多不同的树，到底其中哪棵树才是最好的那一颗？ 直观上理解，如果某一个维度在做判断之后能够消除更大的不确定性，则优先判断该维度。以上例子中，如果先判断”富不富“，那么如果结果是”富“则不再需要更多的判断了，完全消除了不确定性。而如果先判断”白不白“，那么不管是”白“还是”不白“后续都后更多的判断，并未完全消除不确定性。 如何用数学语言描述”消除不确定性“？下面引入信息量和信息熵等相关的概念。 信息量 信息量在是信息“多少”的度量。例如，如下两个事件： 事件A：巴西队获得了世界杯冠军 事件B：中国对获得了世界杯冠军 仅凭直觉，事件B的信息量就比事件A的信息量要大。究其原因，是因为事件A发生的概率很大，事件B发生的概率很小。所以当越不可能的事件发生了，我们获取到的信息量就越大。越可能发生的事件发生了，我们获取到的信息量就越小。那么： 信息量和事件发生的概率相关，事件发生的概率越低，传递的信息量越大。 信息量应当是非负的，必然发生的事件的信息量为零（必然事件是必然发生的，所以没有信息量。几乎不可能事件一旦发生，具有近乎无穷大的信息量。） 两个事件的信息量可以相加，并且两个独立事件的联合信息量应该是他们各自信息量的和。 一个随机事件 xxx 发生的概率为 p(x)p(x)p(x) 则其信息量定义如下： I(x)=−log⁡ap(x) I(x) = -\\log_a p(x) I(x)=−loga​p(x) 如果是以2为底数，单位是bit；如果以e为底数，单位是nat；如果以10为底数，单位是det； 例如，今天下雨的概率是0.5，则包含的信息量为 −log20.5=1 −log_2 0.5=1−log2​0.5=1 比特；同理，下雨天飞机正常起飞的概率为0.25，则信息量为 −log20.25=2−log_2 0.25=2−log2​0.25=2 比特。 信息熵 信息熵（Entropy）是接收信息量的平均值，用于确定信息的不确定程度，是随机变量的均值。信息熵越大，信息就越凌乱或传输的信息越多，熵本身的概念源于物理学中描述一个热力学系统的无序程度。信息熵的处理信息是一个让信息的熵减少的过程。 假设 XXX 是一个离散的随机变量，且它的取值分别为 x1,x2,⋯ ,xnx_1,x_2,\\cdots,x_nx1​,x2​,⋯,xn​ ，每一种取值的概率分别是 p(x1),p(x2),⋯ ,p(xn)p(x_1),p(x_2),\\cdots,p(x_n)p(x1​),p(x2​),⋯,p(xn​)，那么 XXX 的信息熵熵定义为: H(X)=∑i=1np(xi)I(xi)=−∑i=1np(xi)log⁡2p(xi) H(X) = \\sum_{i=1}^n p(x_i)I(x_i) = -\\sum_{i=1}^n p(x_i)\\log_2p(x_i) H(X)=i=1∑n​p(xi​)I(xi​)=−i=1∑n​p(xi​)log2​p(xi​) 如何理解信息熵？仍然以世界杯为例，假设世界杯32支球队获得冠军的概率完全相同，则世界杯夺冠这个事件的信息熵为: −∑i=132132⋅log⁡2132=5 -\\sum_{i=1}^{32} \\frac{1}{32}\\cdot \\log_2 \\frac{1}{32} = 5 −i=1∑32​321​⋅log2​321​=5 但实际总有一些球队获得冠军的概率更大，于是计算出来的信息熵的值肯定会小于5。也就是说，在真实世界中，世界杯夺冠这件事的信息量肯定是小于5的。越是均匀的分布，不确定性程度就越大，此时信息熵就越大。越是不均匀的分布，不确定性程度越小，此时信息熵就越小。 信息增益 对于决策树而言，当选择某个特征维度对数据集进行分类时，分类后的数据集信息熵会比分类前的小。因为我们从这个特征维度给出的样本中学习到了一些知识，这些知识帮助我们消除了一部分信息的不确定性。分类前后信息熵的差值称之为信息增益。在生成决策树的过程中，优先选择信息增益大的特征维度做划分判断，因为信息增益大的特征维度能消除更多的不确定性，也就能最终生成的决策树结构更加简单。 数学上，特征 AAA 对于训练数据集 DDD 的信息增益 g(D,A)g(D, A)g(D,A)，定义为集合 DDD 的信息熵 H(D)H(D)H(D) 与特征 AAA 给定条件下 D 的条件熵 H(D∣A)H(D|A)H(D∣A) 之差，即 g(D,A)=H(D)−H(D∣A) g(D,A)=H(D)-H(D|A) g(D,A)=H(D)−H(D∣A) 信息增益的计算方法 设训练数据集为 DDD ，∣D∣|D|∣D∣ 表示其容量，即样本个数。设有 KKK 个类 Ck，k=1,2,⋯ ,KC_k，k=1,2,\\cdots,KCk​，k=1,2,⋯,K，∣Ck∣|C_k|∣Ck​∣ 为属于类 CkC_kCk​ 的样本个数，则 ∑k∣Ck∣=∣D∣\\sum_k|C_k|=|D|∑k​∣Ck​∣=∣D∣。设特征 AAA 有 nnn 个不同的取值 a1,a2,⋯ ,an{a_1,a_2,\\cdots,a_n}a1​,a2​,⋯,an​，根据特征 AAA 的取值将 DDD 划分为 nnn 个子集 D1,D2,⋯ ,DnD_1,D_2,\\cdots,D_nD1​,D2​,⋯,Dn​，∣Di∣|D_i|∣Di​∣ 为 DiD_iDi​ 的样本个数，则 ∑i∣Di∣=D\\sum_i|D_i|=D∑i​∣Di​∣=D 。记子集 DiD_iDi​ 中属于类 CkC_kCk​ 的样本的集合为 DikD_{ik}Dik​ ，∣Dik∣|D_{ik}|∣Dik​∣ 为 DikD_{ik}Dik​ 的样本个数。 则数据集 DDD 的信息熵为： H(D)=−∑k=1K∣Ck∣∣D∣log⁡∣Ck∣∣D∣ H(D)=-\\sum_{k=1}^K\\frac{|C_k|}{|D|}\\log\\frac{|C_k|}{|D|} H(D)=−k=1∑K​∣D∣∣Ck​∣​log∣D∣∣Ck​∣​ 特征 AAA 对数据集 DDD 的条件熵为： H(D∣A)=−∑i,kp(Dk,Ai)log⁡p(Dk∣Ai)=−∑i,kp(Ai)p(Dk∣Ai)log⁡p(Dk∣Ai)=−∑i=1n∑k=1Kp(Ai)p(Dk∣Ai)log⁡p(Dk∣Ai)=−∑i=1n(p(Ai)∑k=1Kp(Dk∣Ai)log⁡p(Dk∣Ai))=−∑i=1n(∣Di∣∣D∣∑k=1K∣Dik∣∣Di∣log⁡∣Dik∣∣Di∣) \\begin{align*} H(D|A) &= -\\sum_{i,k}p(D_k,A_i)\\log p(D_k|A_i) \\\\ &= -\\sum_{i,k}p(A_i)p(D_k|A_i)\\log p(D_k|A_i) \\\\ &= -\\sum_{i=1}^n\\sum_{k=1}^K p(A_i)p(D_k|A_i)\\log p(D_k|A_i) \\\\ &= -\\sum_{i=1}^n(p(A_i)\\sum_{k=1}^K p(D_k|A_i)\\log p(D_k|A_i)) \\\\ &= -\\sum_{i=1}^n(\\frac{|D_i|}{|D|}\\sum_{k=1}^K \\frac{|D_{ik}|}{|D_i|}\\log\\frac{|D_{ik}|}{|D_i|}) \\end{align*} H(D∣A)​=−i,k∑​p(Dk​,Ai​)logp(Dk​∣Ai​)=−i,k∑​p(Ai​)p(Dk​∣Ai​)logp(Dk​∣Ai​)=−i=1∑n​k=1∑K​p(Ai​)p(Dk​∣Ai​)logp(Dk​∣Ai​)=−i=1∑n​(p(Ai​)k=1∑K​p(Dk​∣Ai​)logp(Dk​∣Ai​))=−i=1∑n​(∣D∣∣Di​∣​k=1∑K​∣Di​∣∣Dik​∣​log∣Di​∣∣Dik​∣​)​ 以下用具体例子进行信息增益的演算，并阐释决策树生成的逻辑。 已知如下样本数据集： 考试成绩 作业情况 出勤率 是否通过考试 优 优 高 是 优 良 高 是 良 优 高 是 良 良 高 是 及格 良 高 是 及格 及格 高 是 及格 不及格 低 否 及格 不及格 高 是 不及格 及格 低 否 不及格 不及格 低 否 计算信息增益生成决策树的过程如下： 首先计算样本集合 DDD 的信息熵： H(D)=−710log⁡2710−310log⁡2310=0.881 H(D)=-\\frac{7}{10}\\log_2\\frac{7}{10}-\\frac{3}{10}\\log_2\\frac{3}{10}=0.881 H(D)=−107​log2​107​−103​log2​103​=0.881 计算各维度特征对于数据集合 DDD 的信息增益，分别以 A1、A2、A3A_1、A_2、A_3A1​、A2​、A3​ 为考试成绩、作业完成情况、出勤率，则计算如下： g(D,A1)=H(D)−[210H(D1)+210H(D2)+410H(D3)+210H(D4)] g(D,A_1) = H(D)-[\\frac{2}{10}H(D_1)+\\frac{2}{10}H(D_2)+\\frac{4}{10}H(D_3)+\\frac{2}{10}H(D_4)] g(D,A1​)=H(D)−[102​H(D1​)+102​H(D2​)+104​H(D3​)+102​H(D4​)] 其中 D1,D2,D3,D4D_1,D_2,D_3,D_4D1​,D2​,D3​,D4​ 分别是 DDD 中取值为优、良、及格、不及格的样本自己。所以： H(D1)=−22log⁡222=0H(D2)=−22log⁡222=0H(D3)=−34log⁡234−14log⁡214=0.811H(D2)=−22log⁡222=0 H(D_1) = -\\frac{2}{2}\\log_2\\frac{2}{2}=0 \\\\ H(D_2) = -\\frac{2}{2}\\log_2\\frac{2}{2}=0 \\\\ H(D_3) = -\\frac{3}{4}\\log_2\\frac{3}{4}-\\frac{1}{4}\\log_2\\frac{1}{4}=0. 811 \\\\ H(D_2) = -\\frac{2}{2}\\log_2\\frac{2}{2}=0 \\\\ H(D1​)=−22​log2​22​=0H(D2​)=−22​log2​22​=0H(D3​)=−43​log2​43​−41​log2​41​=0.811H(D2​)=−22​log2​22​=0 于是对于考试成绩这个特征维度的信息增益为： g(D,A1)=H(D)−(210×0+210×0+410×0.811+210×0)=0.539 g(D,A_1) = H(D)-(\\frac{2}{10}\\times0+\\frac{2}{10}\\times0+\\frac{4}{10}\\times0.811+\\frac{2}{10}\\times0) = 0.539 g(D,A1​)=H(D)−(102​×0+102​×0+104​×0.811+102​×0)=0.539 同样的方法计算 A2、A3A_2、A_3A2​、A3​ ，结果如下： g(D,A2)=0.406g(D,A3)=0.881 g(D,A_2)=0.406 \\\\ g(D,A_3)=0.881 g(D,A2​)=0.406g(D,A3​)=0.881 比较各特征的信息增益，由于特征 A3A_3A3​ (出勤率)的信息增益值最大，所以选择 A3A_3A3​ 做为最优特征。根据出勤率把样本分为2个子集，然后把这两个子集和余下的考试成绩和作业完成情况两个属性作为输入，递归执行算法。 这样一套利用信息增益选择最优特征维度构建决策树的算法称之为ID3算法。 代码实现ID3算法 通过sklearn实现决策树 sklearn中决策树相关的实现都在sklearn.tree模块中。这里采用泰坦尼克号生存数据集来做相关的实现。 import numpy as np import pandas as pd from sklearn import feature_extraction, tree from sklearn.model_selection import train_test_split def load_and_preprocess_data(): titanic_data = pd.read_csv(\"https://github.com/powerAmore/DataSciChecklist/blob/master/titanic.csv\") x = titanic_data[[\"Pclass\", \"Age\", \"Sex\"]] y = titanic_data[\"Survived\"] x['Age'].fillna(x['Age'].mean(), inplace=True) # 简单将缺失值填成平均值 x = x.to_dict(orient=\"records\") # 将x转成字典类型，以便利用DictVectorizer做One-Hot编码 dict = feature_extraction.DictVectorizer(sparse=False) # 实例化DictVectorizer x = dict.fit_transform(x) # 利用DictVectorizer做One-Hot编码 feature_names = dict.get_feature_names() # 返回One-Hot编码后的类别名称 x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, # 设置测试集的比例占全体数据集的比例 random_state=16) # 设置随机数种子，相同的随机数种子每次都会产生完全一样的划分结果 return x_train, x_test, y_train, y_test, feature_names x_train, x_test, y_train, y_test, feature_names = load_and_preprocess_data() print(feature_names) # 打印One-Hot编码后的类别名称 dc = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=3) # \"entropy\"代表信息增益，max_depth是决策树最大深度 dc.fit(x_train, y_train) tree.export_graphviz(dc, out_file=\"titanic_tree.dot\", feature_names=feature_names) # 可以用graphviz工具将dot文件转成png print(tree.export_text(dc, feature_names=feature_names)) # 输出文字版的决策树结构 print(\"mean accuracy: \", dc.score(x_test, y_test)) # 准确率 ['Age', 'Pclass', 'Sex=female', 'Sex=male'] |--- Sex=male 2.50 | | | |--- class: 1 | |--- Pclass > 2.50 | | |--- Age 38.50 | | | |--- class: 0 |--- Sex=male > 0.50 | |--- Pclass 18.00 | | | |--- class: 0 | |--- Pclass > 1.50 | | |--- Age 13.00 | | | |--- class: 0 mean accuracy: 0.7713004484304933 "},"ABTest.html":{"url":"ABTest.html","title":"A/B实验","keywords":"","body":"A/B实验 假设检验 原假设（null hypothesis）：是实验者想要收集证据予以反对的假设。A/B实验中的原假设就是指“新策略没有效果”。 备择假设（alternative hypothesis）：是实验者想要收集证据予以支持的假设，与原假设互斥。A/B实验中的备择假设就是指“新策略有效果”。 利用反证法来检验假设，意味着我们要利用现有的数据，通过一系列方法证明原假设是错误的（伪），并借此证明备择假设是正确的（真）。这一套方法在统计学上被称作原假设显著性检验（NHST：null hypothesis significance testing）。 第一类错误和显著性水平( α\\alphaα ) 第一类错误指原假设实际上是正确的（真），但是我们假设检验的结论却显示”原假设错误，备择假设正确“。这一过程中我们拒绝了正确的原假设，所以第一类错误是“弃真”。 在实际操作中表现为：实验结论显示我的新策略有用，但实际上我的新策略没有用。 在统计学中，我们用显著性水平( α\\alphaα )来描述实验者犯第一类错误的概率。 统计显著性：统计显著性 = 1 - 第一类错误的概率( α\\alphaα )，也称“置信水平、置信度、置信系数”，它的存在是为了描述实验结果的可信度。当某个实验组的指标是显著的，说明这个实验结果大概率是可信的。这个概率是95%，也就是说，系统有95%的信心确认这个实验结果是准确的。 p-value：P值就是在原假设为真的前提下随机抽取样本出现极端情况的概率。当p-value 如果P值很小，说明这种情况发生的概率很小，但如果出现了，根据小概率原理，我们就有理由拒绝原假设。P值越小，说明实验发现的差异是因为抽样误差导致的概率越小，极大程度上还是由于本质上存在差异造成，我们拒绝原假设的理由越充分。 第二类错误( β\\betaβ )和统计功效（statistics power） 第二类错误指原假设错误（伪），但是我们假设检验的结论却显示“原假设正确（真）、备择假设是错误的”，这一过程中我们接受了错误的原假设，所以第二类错误是“取伪”。 第二类错误在实际操作中表现为：我的新策略其实有效，但实验没能检测出来。 统计功效：在统计学中，统计功效 = 1 - 第二类错误的概率( β\\betaβ )，统计功效在现实中表现为：我的新策略是有效的，我有多大概率在实验中检测出来。 在上面的图中，浅灰色区域就是我们犯第二类错误的域，其面积占右侧钟形图形面积的比率，就是我们犯第二类错误的概率。这一概率被记作 β\\betaβ 。在图中的例子里， β\\betaβ 的取值为47%（计算过程就不展示了）。也就是说，在上面的例子里，即便我的新策略B有效，我仍有47%的概率没法检验出它是有效的。而统计效力=1−β=53%=1- \\beta =53\\%=1−β=53%。这代表着我有53%的概率可以检测出新策略B是有效的。 "},"TimeSeries.html":{"url":"TimeSeries.html","title":"时间序列","keywords":"","body":"import pandas as pd df = pd.read_csv(\"AirPassengers.csv\", parse_dates=[\"Month\"]).rename(columns={\"Month\":\"ds\", \"#Passengers\":\"y\"}) import matplotlib.pyplot as plt plt.close(\"all\") X_train = df[df.ds=\"19580101\"] plt.plot(X_train['ds'], X_train['y']) plt.plot(X_test['ds'], X_test['y']) [] from prophet import Prophet pro = Prophet() pro.fit(X_train) 18:55:40 - cmdstanpy - INFO - Chain [1] start processing 18:55:40 - cmdstanpy - INFO - Chain [1] done processing pred = pro.predict(X_test) df.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ds y 139 1960-08-01 606 140 1960-09-01 508 141 1960-10-01 461 142 1960-11-01 390 143 1960-12-01 432 future = pro.make_future_dataframe(periods=10, freq = 'M') future.tail(20) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ds 98 1957-03-01 99 1957-04-01 100 1957-05-01 101 1957-06-01 102 1957-07-01 103 1957-08-01 104 1957-09-01 105 1957-10-01 106 1957-11-01 107 1957-12-01 108 1957-12-31 109 1958-01-31 110 1958-02-28 111 1958-03-31 112 1958-04-30 113 1958-05-31 114 1958-06-30 115 1958-07-31 116 1958-08-31 117 1958-09-30 df ds y 0 1949-01-01 112 1 1949-02-01 118 2 1949-03-01 132 3 1949-04-01 129 4 1949-05-01 121 ... ... ... 139 1960-08-01 606 140 1960-09-01 508 141 1960-10-01 461 142 1960-11-01 390 143 1960-12-01 432 144 rows × 2 columns "},"DimensionalModeling.html":{"url":"DimensionalModeling.html","title":"维度建模","keywords":"","body":"实际上不太喜欢研究数仓建模相关的理论。感觉像是在看英文语法书，一点也不Geek。但是真看起来，还是有一些要点值得被记录。 数据仓库领域数据建模通常采用维度建模。维度模型将复杂的业务通过事实和维度两个概念进行呈现。 事实通常对应业务过程，就是具体的用户行为或者发生的事件。电商交易中的下单，取消订单，付款，退单等，都是业务过程。 维度通常对应业务过程发生时所处的环境，就是行为或者事件相关的属性。比如下单这个业务过程包含了以下维度：Date（日期），Customer（顾客），Product（产品），Location（地区）等。 维度建模以数据分析作为出发点，为数据分析服务，因此它关注的重点是如何更快的完成需求分析，以及如何较好的提升大规模复杂查询的性能。 事实表 事实表是包含具体业务过程的表。包含与该业务过程有关的维度引用（维度表外键）以及该业务过程的度量（通常是数字类型字段）。 维度引用可以用来后续关联维度表获知更具体的维度信息。 事实表中也有可能不包含所谓的业务过程度量，比如关注一个作者，点赞一个帖子这种业务过程。而下单，通常会包含商品数量以及订单金额这种业务过程的度量。 事实表有三种类型：分别是事务型事实表、周期快照事实表和累积快照事实表，每种事实表都具有不同的特点和适用场景。 事务型事实表 数仓中最常见的就是事务型事实表。用于分析与各业务过程相关的各项统计指标，由于其保存了最细粒度的记录，可以提供最大限度的灵活性，可以支持无法预期的各种细节层次的统计需求。 设计事务事实表时一般可遵循以下四个步骤：选择业务过程→声明粒度→确认维度→确认事实 选择一个个不可拆分的行为事件作为业务过程。例如电商交易中的下单，取消订单，付款，退单等。 声明粒度，一个业务过程也可能用不同的粒度来表示。比如下单可能包含多个商品，一个订单既可以用一行数据来表达，也可以被拆分成多条数据，每一条数据只记录一个商品。通常应尽可能选择最细粒度，以此来应各种细节程度的需求。 确认维度，确定事实表中应该包含哪些维度信息。维度的丰富程度就决定了维度模型能够支持的指标丰富程度。 确定事实。此处的事实一词，指的是每个业务过程的度量值。可以分为： 可加事实：比如订单金额，观看时长 半可加事实：比如库存，余额。可以按照商品或者用户维度进行累加，但是不能按照时间维度进行累加。 不可加事实：比如比率，转化率。不可加事实通常需要转化为可加事实，例如比率可转化为分子和分母。 事务型事实表的不足 理论上事务型事实表可以支撑与各业务过程相关的各种统计粒度的需求。但对于某些特定类型的需求，其逻辑可能会比较复杂，或者效率会比较低下。 存量型指标 例如商品库存，账户余额等。以账户余额为例，事务型事实表记录的是用户账户每一笔支出和收入。假定现有一个需求，要求统计截至当日的各用户账户的余额。则需要扫描全量用户有史以来的全表数据聚合才能得到统计结果。可以看到事务型事实表对于这个需求而言并不是一个好的方案。 多事务关联统计 例如，现需要统计最近30天，用户下单到支付的时间间隔的平均值。统计思路应该是找到下单事务事实表和支付事务事实表，过滤出最近30天的记录，然后按照订单id对两张事实表进行关联，之后用支付时间减去下单时间，然后再求平均值。 逻辑上虽然并不复杂，但是其效率较低，应为下单事务事实表和支付事务事实表均为大表，大表join大表的操作应尽量避免。 为了解决以上两个问题，引入周期快照事实表和累积快照事实表。 周期快照事实表 周期快照事实表以具有规律性的、可预见的时间间隔来记录事实，主要用于分析一些存量型（例如商品库存，账户余额）或者状态型（空气温度，行驶速度）指标。 对于商品库存、账户余额这些存量型指标，业务系统中通常就会计算并保存最新结果，所以定期同步一份全量数据到数据仓库，构建周期型快照事实表，就能轻松应对此类统计需求，而无需再对事务型事实表中大量的历史记录进行聚合了。 对于空气温度、行驶速度这些状态型指标，由于它们的值往往是连续的，我们无法捕获其变动的原子事务操作，所以无法使用事务型事实表统计此类需求。而只能定期对其进行采样，构建周期型快照事实表。 周期快照事实表设计的时候除了要考虑事务型事实表遵循的4个步骤，另外还需要考虑采样周期。通常采样周期选择”天“，以一天为周期进行快照。当然也有按周，按月，按季度的周期快照。 周期快照事实表和事务型事实表的一个关键区别在于密度。事务事实表本质是稀疏的，当天只有有业务过程发生才会记录相关的数据。而对于周期快照事实表，实际上是稠密的，即使当天没有业务过程发生，仍然会全量记录。比如余额数据，即使用户当天并未有收入或者支出相关的行为发生，周期快照事实表仍然会记录一条数据，即使是与前一天数据完全相同。 注意事项： 事务与快照成对设计 通常在数仓维度建模的时候，为了更好的满足业务过程度量值的分析，往往要求事务型事实表和周期快照事实表成对设计。周期快照事实表主要是为了满足存量或状态型数据分析任务。 附加事实 快照事实表在确定状态度量时， 一般都是保存采样周期结束时的状态度量。但是也有分析需求需要关注上一个采样周期结束时的状态度量，而又不愿意多次使用快照事实表，因此一般在设计周期快照事实表时会附加一些上一个采样周期的状态度量。 累积快照事实表 累计快照事实表是基于一个业务流程中的多个关键业务过程联合处理而构建的事实表，如交易流程中的下单、支付、发货、确认收货业务过程。 累积快照事实表通常具有多个日期字段，每个日期对应业务流程中的一个关键业务过程。 订单ID 用户ID 下单日期 支付日期 发货日期 确认收货日期 订单金额 支付金额 1001 0001 2022-01-01 2022-01-02 2022-01-03 2022-01-04 2000 2000 累积快照事实表主要用于分析业务过程之间的时间间隔等需求。例如前文提到的用户下单到支付的平均时间间隔，使用累积快照事实表进行统计，就能避免两个事务事实表的关联操作，从而变得十分简单高效。 事务型事实表和周期快照事实表数据只存在插入操作，但是累计快照事实表不仅存在插入操作还存在更新操作。 在实际应用中，业务流程中所包含的业务过程可能并不固定，甚至相当复杂。仍以下单为例，可能并非是下单→支付→发货→收货这么简单直接。中间也可能包含退款、申诉、取消订单等等一些不确定的业务过程。所以在设计和使用累计快照事实表的时候需要格外小心。 物理实现 第一种方式是全量表的形式。此全量表一般为日期分区表，每天的分区存储昨天的全量数据和当天的增量数据合并的结果，保证每条记录的状态最新。此种方式适用于全量数据较少的情况。如果数据量很大，此全量表数据量不断膨胀，存储了大量永远不再更新的历史数据，对ETL 和分析统计性能影响较大。 第二种方式是全量表的变化形式。此种方式主要针对事实表数据量很大的情况。较短生命周期的业务实体一般从产生到消亡都有一定的时间间隔，可以测算此时间间隔，或者根据商业用户的需求确定一个相对较大的时间间隔。比如针对交易订单，我们以200 天作为订单从产生到消亡的最大间隔。设计最近200 天的交易订单累积快照事实表，每天的分区存储最近200 天的交易订单；而200 天之前的订单则按照gmt_create 创建分区存储在归档表中。此方式存在的一个问题是200 天的全量表根据商业需求需要保留多天的分区数据，而由于数据量较大，存储消耗较大。 第三种方式是以业务实体的结束时间分区。每天的分区存放当天结束的数据，设计一个时间非常大的分区，比如3000-12-31 ，存放截至当前未结束的数据。由于每天将当天结束的数据归档至当天分区中，时间非常大的分区数据量不会很大， ETL 性能较好；并且无存储的浪费， 对于业务实体的某具体实例，在该表的全量数据中唯一。比如对于交易订单，在交易累积快照事实表中唯一。 针对第三种方式，可能存在极特殊情况，即业务系统无法标识业务实体的结束时间： 使用相关业务系统的业务实体的结束标志作为此业务系统的结束标志。比如针对物流订单，可以使用交易订单。理论上， 交易订单完结了，则物流订单已经完结。 和前端业务系统确定口径或使用前端归档策略。累积快照事实表针对业务实体一般是具有较短生命周期的，和前端业务系统确定口径，确定从业务实体的产生到消亡的最大间隔。 维度表 规范化与反规范化 规范化是指使用一系列范式设计数据库的过程，其目的是减少数据冗余，增强数据的一致性。通常情况下，规范化之后，一张表的字段会拆分到多张表。 反规范化是指将多张表的数据冗余到一张表，其目的是减少join操作，提高查询性能。在设计维度表时，如果对其进行规范化，得到的维度模型称为雪花模型，如果对其进行反规范化，得到的模型称为星型模型。 数据仓库系统的主要目的是用于数据分析和统计，所以是否方便用户进行统计分析决定了模型的优劣。采用雪花模型，用户在统计分析的过程中需要大量的关联操作，使用复杂度高，同时查询性能很差，而采用星型模型，则方便、易用且性能好。所以出于易用性和性能的考虑，维度表在设计时一般存在大量反规范化。 维度变化 维度属性通常不是静态的，而是会随时间变化的，数据仓库的一个重要特点就是反映历史的变化，所以如何保存维度的历史状态是维度设计的重要工作之一。保存维度数据的历史状态，通常有以下两种做法，分别是全量快照表和拉链表。 全量快照表 离线数据仓库的计算周期通常为每天一次，所以可以每天保存一份全量的维度数据。这种方式的优点和缺点都很明显。优点是简单而有效，开发和维护成本低，且方便理解和使用。缺点是浪费存储空间，尤其是当数据的变化比例比较低时。 拉链表 拉链表，记录每条信息的生命周期，一旦一条记录的生命周期结束，就重新开始一条新的记录，并把当前日期放入生效开始日期。如果当前信息至今有效，在生效结束日期中填入一个极大值（如9999-12-31 ）。 用户ID 姓名 手机号码 开始日期 结束日期 0001 李四 136**1111 2022-01-01 2022-01-02 0001 李四 137**2222 2022-01-03 2022-01-09 0001 李四 138**1234 2022-01-10 9999-12-31 拉链表适合于：数据会发生变化，但是变化频率并不高的维度（即：缓慢变化维） 。比如：用户信息会发生变化，但是每天变化的比例不高。如果数据量有一定规模，按照每日全量的方式保存效率很低。 比如：1亿用户*365天，每天做一份全量用户信息表效率就会很低。 拉链表在具体使用过程中可以通过：生效开始日期=某个日期 ，得到某个时间点的数据全量切片： SELECT * FROM user_info WHERE start_date = '2022-01-01' "},"UserBehaviorsAnalysisPlatform.html":{"url":"UserBehaviorsAnalysisPlatform.html","title":"用户行为分析平台","keywords":"","body":"用户行为分析平台的出现完全是为了满足产品研发团队对数据分析简单、灵活、高频、快速的实际需求，直接面向数据驱动、增长营销、智能运营的集成数据分析平台。 相比传统数仓复杂的数据分层和数据模型，用户行为分析平台建模的思想虽然也源自维度建模，但将整个数据模型进行了极大的简化。降低了数据使用者对数据的理解门槛。在数据质量和数据实时性上也因为模型本身的简洁得到了有效的保证。 相比传统BI，数据源相对分散，数据使用过于灵活，用户行为分析平台统一了数据来源（统一的用户行为数据上报），根据实际产品运营研发团队对数据分析的要求提炼出了数据的特定使用模式（事件分析，留存分析，漏斗分析，分布分析，路径分析等等）。源自产品运营研发团队实际工作的分析方法，自然使得产品运营研发人员在使用数据时更加的得心应手，能够将目光聚焦于具体数据分析和使用本身。 用户行为分析平台从数据上报到数据可用可以做到分钟级。数据虽然并非实时，但是对于满足产品运营研发团队的数据分析已经足够了，完全满足数据驱动的要求。对于分析的场景，其实完全没有必要追求实时，够用才是最好的，没必要去承担实时系统的高成本。另一方面基于离线数仓的天级数据对于一个数据驱动的团队来说又完全不够。量变引起质变，只有足够快的出图速度才能达到连贯的数据思维。 用户行为分析平台完全基于用户行为数据的上报，分析的也是用户行为数据。缺失业务数据库中的数据。于是针对一些业务数据库中的数据无法有效分析。不过不断进化的用户行为分析平台允许用户自行上报一些维表数据补充部分业务数据的缺失。但仍然的，一些业务数据库相关的累计数据或者状态数据，分析起来仍然不方便。但是剩下这些搞不定的分析真的不多了。只需要稍微再针对这些数据做一些BI报表即可。 用户行为分析平台尽可能用最少的资源最低的成本最简单的逻辑最快的速度满足产品运营研发团队最多的数据分析需求。让团队中的每个人都能轻松玩转数据。这就是它巨大的价值。 对于中小公司，如果开始考虑搭建自己的数据体系，务必优先考虑搭建用户行为分析平台。对于中小公司的业务体量而言，真正可以称之为大数据的可能只有用户行为数据，业务数据库中的数据总体来说不会太大，这就更凸显了用户行为分析平台的价值。市面上也有着一批优秀的第三方公司提供基于SaaS或者私有化部署的用户行为分析平台。国内的比如：GrowingIO，神策以及字节跳动火山引擎的DataFinder；国外的比如：Mixpanel。虽然Google Analytics和Firebase Analytics是来自于巨头的产品，但其实做的不太好，所以也不太推荐。 用户行为分析平台的数据模型 用户行为分析平台的数据模型并没有统一的命名，有些称之为事件模型，有些称之为事件-用户模型。但核心要素就是用户（User）和事件（Event）。对应着基于用户标识的行为事件埋点上报，以及最终分析性数据库中存储的事件宽表和用户宽表。这样一个数据模型也决定了最终的数据分析的核心就是围绕着用户和事件展开的。 显然的，就像一句完整的话是由主-谓-宾构成的，用户产生的行为事件必然包含一个目标，将其称之为物品（Item）。早期用户行为分析平台并未太关注物品（Item）。导致物品相关的维度分析存在缺失。但随着用户行为分析平台不断的发展完善，物品（Item）也被纳入其中，成为数据模型的重要补充。在用户行为分析平台中与之对应的是相关的物品（Item）维度表。 目前，一个完善的用户行为分析平台的数据模型就是由：用户（User）-- 事件（Event）-- 物品（Item），这三个核心要素组成。随着用户行为事件的上报，系统尽可能快速的将上报事件数据解析为对应的事件表、用户表以及物品维度表的字段，供下游数据分析使用。同时，用户表中的一些属性字段以及一些物品维度表也可以通过专门的同步机制，从业务数据库中同步相应的数据，比如用户等级表，商品信息表，帖子状态表等等。用户行为分析平台通常由一张事件表、一张用户表和N张物品维度表组成。 用户行为分析平台数据上报一般采用json格式，一条典型的数据上报格式如下：event_name为事件名称，local_time_ms为事件发生的时间；user为触发事件的用户以及相应的用户信息，用于关联用户表；params为事件包含的属性信息；item为与事件相关的物品信息，用于关联物品（Item）维度表。 { \"user\": { \"user_id\": \"0001\", \"device_id\": \"asdf123\", \"app_version\": \"9.13.5\", \"network_type\": \"wifi\", \"platform\": \"Android\" }, \"params\": { \"author_id\": \"809654\", \"article_type\": \"video\", \"view_list\": \"main\" }, \"items\":{ \"article\": [{ \"id\": \"789\" }] }, \"event_name\": \"like_article\", \"app_id\": \"4567\", \"app_name\": \"buybuy\", \"local_time_ms\": 1671593592638 } 以上示例为一条给视频内容点赞的用户行为数据。通过这条数据上报我们可以多维度的分析”内容点赞“这个事件，不同网络环境下给视频内容的点赞情况，比如不同内容类型的点赞情况等等。但一旦我们要分析内容的点赞总量或者我们想分析帖子状态信息（审核是否通过），仅仅通过行为数据的上报是很困难的，这时就要关联物品维度表中相关的内容属性。 用户行为分析平台的系统架构 下图展示的是神策的用户行为分析平台系统架构，以此为例，可以看到用户行为分析平台的整体技术架构大致可以分为以下几个部分： 数据采集子系统。这块儿以前端后端的数据采集SDK为主，用来提供用户行为数据的采集功能和上报功能。因为用户行为分析平台天然包含了数据采集模块儿，有时候会有同学直接把用户行为分析平台叫做数据采集工具。 数据接入子模块。用户行为数据通过Http请求进行上报，由Nginx作为数据接收端将用户行为数据写入日志。用Flume或者Logstash这类工具从日志读取数据并写入消息队列Kafka，供下游读取并处理。 数据导入与存储模块。Spark或者Flink从Kafka中读取数据进行ETL之后写入存储。神策的OLAP模块使用的是Parquet+Kudu+Impala的模式，存储和查询分离。GrowingIO早期是使用HBase自研了一套OLAP引擎，后面重构之后采用的是ClickHouse。字节火山引擎的DataFinder从一开始就是使用自研的ByteHouse作为OLAP引擎，ByteHouse是基于ClickHouse的魔改版本。 最后就是各种数据应用模块。包括数据可视化分析，智能运营等等。 整个系统架构摒弃了传统数仓的多层分层架构，数据接入之后以最短路径进入OLAP存储成大宽表。提升数据实时性的同时也降低了中间环节出错的可能性，尽量提升数据质量。 用户行为分析平台的数据能力 用户细查 上报的是用户行为数据，自然地，如果按照时间线展开每个用户的行为事件，就能够清晰的还原用户使用网站或者应用的行为细节以及所处的环境。能够帮助理解特定用的户行为以及定位线上问题。 用户细查需要包含的最基本功能就是能够按照用户ID或者设备ID搜索到该用户，并且展示该用户的基本用户属性信息。同时能够查询特定时间范围内的该用户的行为事件。如果做的更细致一些，可以提供更丰富的过滤条件，比如：可以选择需要包含的事件或者不想要包含的事件。 用户分群 用户分群是精细化运营的重要支撑手段之一，产品运营同学可以基于细分后的用户群开展用户画像、精细化分析、精准触达等工作。用户行为分析平台利用上报的用户行为数据以及用户属性相关的数据，可以非常容易的根据具体的行为事件和用户属性创建规则将用户进行分群，并固化下来。固化下来的用户分群不仅可以选择具体的用户进行用户细查，也可以作为目标用户进行事件分析，留存分析，漏斗分析等一系列的数据分析工作。 用户分群也可以通过直接上传用户ID列表文件形成分群，让分群方式更加灵活。 事件分析 事件分析应该是用户行为分析平台使用最广泛的功能。事件分析是针对特定事件特定目标用户的某一指标的分析。事件分析的大致步骤： 选择想要分析的事件。例如：”内容点赞“事件。 选择想要分析的指标。例如：”内容点赞“事件量，”内容点赞“事件人数，”内容点赞“事件人均次数等等。对于一些包含数值信息的事件，比如“下单”事件，还可以选择按照“下单金额”求和，求平均，求最大最小值等等。 对事件增加过滤条件：对事件属性或者属性的组合进行相应的过滤，选择想要分析的特定属性条件。比如：通过过滤选择”内容类型”为“视频”，“内容出现位置”为“推荐列表”的”内容点赞“事件。 选择目标用户：比如：目标用户为“新用户”或者使用其它用户属性来筛选。甚至可以做到更为复杂的用户圈选，比如：“过去30天访问过7天以上并且关注了3个以上up主的用户” 针对以上这样一个条件组合生成最终指标的可视化图表。实际中还可以做多个事件指标的计算，不同属性维度的对比展示，不同用户群组的对比展示等等。 留存分析 留存基本是产品运营最关注的指标之一。留存分析可以通过方便的选择起始事件和回访事件生成针对目标用户成留存曲线和留存率变化图。 留存曲线是指针对一群特定的目标用户由其次日留存，2日留存，3日留存，4日留存……构成的随时间逐渐衰减的曲线。从留存率曲线中可以观察一群用户留存衰减的情况。 留存率变化图是指由每天用户的次日（也可以是7日，14日等）留存构成的次日留存率曲线。从留存率变化图中我们可以观察随着时间的推移次日留存变化的曲线。是产品运营做增长最重要的图表之一。 我们通常意义下的用户留存，指的是全量用户针对全量事件的留存，起始事件是任意事件，回访事件也是任意事件。新用户留存，就是把目标用户设定为新用户，起始事件和回访事件设置为任意事件的留存。 之所以需要能够任意设置留存的起始事件和回访事件，是因为我们同样会关注某一些特定事件的留存。比如：买过了再买的复购率，比如某些功能模块的留存率，来了又来。 留存分析同样可以选择需要的目标用户群进行留存分析。方便产品运营进行精细化的用户运营。 留存分析有可能还包含一个关联属性的设置。关联属性用于让起始事件和回访事件的某个属性值保持一致，常用于活动名称、页面标题或商品名称等。 漏斗分析 漏斗分析主要用来分析用户在流程中每一步的转化情况。典型的分析场景比如：我们会分析新用户打开APP到注册登录进入主页面这一重要留存每个步骤的转化情况。亦或：电商场景下，用户浏览商品到加入购物车到提交订单完成支付这一系列动作每一步的转化情况。 漏斗分析首先要设计漏斗：根据需要分析的流程选择每一步的转化事件，以及转化周期。转化周期是指事件与事件之间转化的时间间隔。然后同样的，可以选择特定目标用户群，针对特定群体的用户进行转化分析。 漏斗分析最终会给出流程中每一步之间的转化率漏斗，从中可以分析出核心的转化问题存在于哪个环节之中。同时也可以获取到具体流失用户的用户ID，将相应的流失用户生成用户分群，做进一步的流式用户分析。 同样，漏斗分析也可以设置关联属性。 漏斗分析不仅可以分析事件之间的转化率。也可以生成某一转化率随时间变化的转化趋势图，类似留存率变化图。另外，事件转化的时间间隔也可以生成基于转化周期的分布分析图。 分布分析 分布分析指的是事件在整体或某一维度下，按照计算结果划分出一些区间，查看对应人数在各区间内的分布情况。分布分析有很多种类，比如按事件发生频次查看人数分布、按属性值计算结果查看人数分布、按一段时间内累计发生的时长或天数查看人数分布等。 分布分析在使用时通常需要自定义分布区间。自动划分的分布区间往往并不能符合我们实际的分析需要。 分布分析在用户行为分析平台的使用过程中需要注意的是，它一定是针对人的分布。这一点是对数据理解不够深入的产品运营同学在使用时经常忽视的一个问题，总是试图在用户行为分析平台做非人的分布分析。之所以一定是对人的分布，还是因为用户行为分析平台所选择的事件-用户模型所致。 路径分析 路径分析多少有点类似漏斗分析。漏斗分析往往用于分析明确的转化路径，但在一些情况下路径分叉比较多，想要从整体上把控用户的流向，这时候路径分析就会起到作用。而且路径分析展示的是连续的用户行为，可能会出现A事件后接着是B事件，然后又回到A事件的情况。 路径分析第一步是设置起始事件或者终止事件。起始事件用于分析用从这个事件开始用户去到了哪里。终止事件用于分析用户从哪里来到这个事件。 第二步是设置N个想要分析的用户可能流向的事件或者排除不想分析的事件。在探索分析或者对用户流向不清楚的时候，刚开始通常也可以不设置具体的事件，默认包含全部埋点事件。这样就可以有个用户流向的全貌展示，之后再选择关注的事件进行相应分析或者配合漏斗分析。 LTV分析 LTV是指用户生命周期价值(life time value)。通过分析用户从开始进入应用到最终流失全生命周期的消费数据，衡量单个用户的平均价值，进而帮助产品运营了解产品营收情况以及确立拉新成本上限。 LTV的计算规则如下：用户进入应用第n天的人均LTV称为LTVn，LTVn = 新用户同期群在随后n天内花费的总金额 ÷\\div÷ 该同期群用户数。 同期群（cohort）的含义是在规定时间内对具有共同行为特征的用户群。新用户同期群通常指在同一时间段（同一天，或者同一星期等）开始使用应用的新用户的总体。 通过计算第0天，第1天，第2天，第3天……的LTVn绘制成相应的LTV趋势图。LTV趋势图是一个逐步上升然后趋缓最终水平的一条曲线。因为用户进入应用后开始消费，随着消费增加LTV的值越来越大，但是用户也会随着时间逐步流失，直到某天的该新用户同期群的用户全部流失掉，LTV的值将保持恒定不再增加。 同样类似留存分析中的留存率变化图，LTV分析不仅可以展示LTV趋势图，可以展示具体某一个LTVn的LTV对比图，用于分析随时间推移的不同同期群的LTVn的变化情况。 LTV的概念还可以进一步抽象化。不一定特指用户生命周期的消费。LTV计算的分子，可以抽象为同期群的任意数值度量。比如：可以考察用户生命周期人均获得的金币量，用户生命周期人均的阅读量等等。 用户行为分析平台的高级能力 基于以上的分析能力，产品运营研发团队能够解决绝大多数数据分析需求。但这并未发挥出用户行为分析平台的全部潜力。 SQL分析 上面提到的基本的分析能力都是确定套路的分析方法。虽然足够高频，但也存在不够灵活的缺陷。用户行为分析平台的事件-用户模型本身是由事件宽表、用户宽表以及若干的维度表组成的。开放基于这些表的SQL查询，自然能够获得更灵活的分析能力。 可能普通产品运营同学不具备足够的SQL能力，SQL分析更多的是提供给数据分析师以及研发同学使用。SQL的结果同样也可以固化为相应的分析报表提供给产品运营同学使用。 在实际工作中，数据产品经理或者数据分析师也会经常性的利用SQL分析来分析问题保障数据质量。毕竟，只有通过SQL分析系才能完整明确的看到数据表中真实存储的原始数据。不断提升数据质量才是业务部门愿意使用数据平台的基础。 智能运营 产品运营在日常工作中通常都会希望通过各种方式触达用户。比如给用户发送短信，推送消息，发放礼券等等。触达的逻辑通常是在特定的时机给满足某些条件的用户进行相应的”推送“。这里有两个关键点，一个是特定时机，一个是满足条件的用户。以往，这样一个逻辑通常需要产品运营提出需求，开发同学写代码来实现。但是基于用户行为分析平台，这样一套逻辑似乎并不需要每次都有开发介入。 所谓满足条件的用户实际上对应着用户行为分析平台的用户分群功能。所谓特定时机可以是特定事件的触发，也可以是特定用户路径，或者是一些组合逻辑。不管怎样，最基本的用户行为事件已经上传进入了用户行为分析平台。用户行为分析平台需要实现一套流程画布系统来编辑触发的逻辑。比如一个简单的逻辑：针对一周内的新用户，如果进入了商品购买页面，就给他们发放一张8折券。一周内的新用户可以利用用户分群功能圈选出来，每天自动更新。进入商品购买页面，直接利用用户行为上报的事件进行触发。最后需要实现发放优惠券的逻辑。 礼券系统应该是一个独立的与其它系统解耦的后端服务系统。对外应该有提供通过RPC或者http的接口调用服务。那么用户行为分析平台应当提供一个推送通道管理的界面，用于注册推送调用的接口，实际上就是我们通常所说的Webhook。针对礼券接口的例子，就应该是在用户行为分析平台的推送通道管理界面注册一个调用礼券发放礼券的接口。推送通道管理的界面可以注册不同的推送通道，比如消息推送，短息推送，私聊消息，弹窗，礼券发放等等。 一旦完成这样一套智能运营系统，运营人员即可自行编辑相关的逻辑，完成自动化的用户触达能力，实现个性化的用户运营。同时极大的降低了开发人员的参与，提升了运营的效率。 数据开放 正如上文提到过的，有些同学会把用户行为分析平台称之为数据采集系统。显然的，能够全量准实时获取到用户行为数据的用户行为分析系统一定是可以对外提供数据的。 用户行为分析平台对外开放数据可以存在于4个不同的阶段： 最原始的数据。用户行为数据上报到用户行为分析系统之后，在ETL之前会位于Kafka消息队列之中。这些都是最原始的未经加工过的数据。如果对外开放Kafka中存储原始数据的Topic的订阅能力，外部系统就可以以最快速度消费到最原始的用户行为数据。 经过ETL之后的数据。用户行为分析系统会对用户上报的行为数据进行ETL处理。经过ETL处理之后未落表之前，这些数据可以推入一个旁路的Kafka消息队列通道。将这个通道对外开发。则外部系统可以获取到相对实时，同时数据质量不错的用户行为数据。 落表之后的数据。用户行为数据最终会落到事件表中。可以直接对外开放事件表的访问权限。当然，落表数据往往数据延时会更大，提供的数据往往也是数据片段，例如若干特定事件。订阅这些特定数据的方式通常也是通过Webhook。 统计数据。产品运营已经在用户行为分析平台上制作了相应的事件分析表，漏斗分析表，分布分析表等等数据表。这些数据表都可以开放给外部系统进行订阅。这些数据的数据量相对较小，可以直接通过Http请求获取。 "},"SQLKeypoints.html":{"url":"SQLKeypoints.html","title":"SQL要点","keywords":"","body":"只包含DML（Data Manipulation Language，数据操纵语言）的内容，不包含DDL（Data Definition Language，数据定义语言）和DCL（Data Control Language，数据控制语言） 基础语法 仅举例说明，不做过多解释 其中Student表为学生信息表，Score表为学生成绩表，建表语句为： CREATE TABLE Student ( s_id VARCHAR(20) COMMENT 'student ID', s_name VARCHAR(20) NOT NULL DEFAULT '' COMMENT 'student name', s_birth VARCHAR(20) NOT NULL DEFAULT '' COMMENT 'student birthday', s_sex VARCHAR(10) NOT NULL DEFAULT '' COMMENT 'student sex', PRIMARY KEY (s_id) ); CREATE TABLE Score ( s_id VARCHAR(20) COMMENT 'student ID', c_id VARCHAR(20) COMMENT 'course ID', s_score INT(3) COMMENT 'student score', PRIMARY KEY (s_id, c_id) ); 以下为包含基础语法的查询语句： select * from Student where s_name like \"李%\"； select s_id, s_score from Score where c_id in (\"0001\", \"0002\"); select count(distinct s_id) from Score where s_score >= 60; select c_id from Score where s_score60; select s_id, s_name, s_birth from Student where year(s_birth) = 2010 limit 10; select * from Student where month(s_birth) = month(now()); # 求年纪 select s_id, s_name, s_birth, floor(timestampdiff(month, s_birth, now())/12) from Student; 进阶语法 条件表达式 case表达式 /* 语法 CASE WHEN THEN WHEN THEN . ELSE END */ # 将s_sex字段的数值转成'male'、'female'和'null'的表示形式 select s_id, s_name, (case s_sex when 1 then \"male\" when 2 then \"female\" else \"uncertain\" end) as gender from Student; # 行转列 select sum(case s_sex when 1 then 1 else 0 end) as \"male\"， sum(case s_sex when 2 then 1 else 0 end) as \"female\" from Student; if表达式 /* 语法 IF(expr1, expr2, expr3) expr1的值为 TRUE，则返回值为expr2 expr1的值为FALSE，则返回值为expr3 */ select if(s_sex = 1, \"male\", \"female\") as gender from Student where s_sex != \"\"; /* 语法 IFNULL(expr1, expr2) 判断expr1是否为NULL： 如果expr1不为空，返回expr1； 如果expr1为空， 返回expr2 */ select ifnull(s_sex, \"uncertain\") as gender from Student; 子查询 子查询指一个查询语句嵌套在另一个查询语句内部的查询，子查询的结果可以作为一个表，也可以作为一个过滤条件 # 作为一个表 # 前10名从低到高排序 select * from (select * from Score where c_id = \"0001\" order by s_score desc limit 10) order by s_score; # 作为一个过滤条件 # 高于平均分 select * from Score where s_score >= (select avg(s_score) from Score); 表关联 UNION UNION 操作符用于合并两个或多个 SELECT 语句的结果集。 UNION 内部的每个 SELECT 语句必须拥有相同数量的列。列也必须拥有相似的数据类型。同时，每个 SELECT 语句中的列的顺序必须相同。 UNION 是去重的，UNION ALL 是不去重的，只能使用一条ORDER BY子句，它必须出现在最后一条SELECT语句之后。 # UNION去重 select u_id from Score where c_id = \"0001\" union select u_id from Score where c_id = \"0002\" order by s_score; # UNION ALL不去重 select u_id from Score where c_id = \"0001\" union all select u_id from Score where c_id = \"0002\" order by s_score; JOIN 常见的7种JOIN方式见下图： # 列出学生ID、学生姓名的同时也把对应学生的分数列出来 select st.s_id, st.s_name, sc.c_id, sc.s_score from Student as st left join Score as sc on st.s_id = sc.s_id; s_id s_name c_id s_score 01 李四 03 99 01 李四 02 90 01 李四 01 80 02 王二 03 80 02 王二 02 60 查询语句运行结果中需要注意，Student表中原本的一条数据被重复了N次 窗口函数 窗口函数的语法为： OVER ([PARTITION BY ] ORDER BY ) MySQL中只有8.0版本以上才支持窗口函数 # 按照c_id分组，按照s_score排序 select s_id, c_id, s_score, rank() over (partition by c_id order by s_score desc) as ranking from Score; s_id c_id s_score ranking 01 01 80 1 03 01 80 1 05 01 76 3 02 01 70 4 04 01 50 5 06 01 31 6 01 02 90 1 02 02 60 2 04 02 30 3 01 03 99 1 排序函数 三种不同排序函数：rank()，dense_rank()，row_number() # 按成绩排序，展示三种不同排序函数的差异 select s_id, c_id, s_score, rank() over (order by s_score desc) as ranking, dense_rank() over (order by s_score desc) as denserank, row_number() over (order by s_score desc) as rownumber from Score; s_id c_id s_score ranking denserank rownumber 01 03 99 1 1 1 07 03 98 2 2 2 01 02 90 3 3 3 01 01 80 4 4 4 02 03 80 4 4 5 03 01 80 4 4 6 05 01 76 7 5 7 02 01 70 8 6 8 rank()：有相同分数时排名相同，且会占用后一名的位置，一些名次不存在，存在名次跳跃 dense_rank()：有相同分数时排名相同，但不会占用后一名的位置，名次是连续的 row_number()：有相同分数时排名也会不相同，名次是连续的 聚合函数 在窗口函数中使用聚合函数（sum，avg，count，max，min）的含义为：根据order by字段的排序，求”到目前为止“的聚合值，比如sum就是累加的含义。语言描述起来有点困难，直接看查询语句的结果理解起来就不难了。 按照s_score字段降序排列。值得注意的是：相同分数count值相同。因为降序，最大分数值出现在第一位，所以每一行的max都等。 #只有order by，没有partition的查询 select s_id, c_id, s_score , sum(s_score) over (order by s_score desc) as current_sum, avg(s_score) over (order by s_score desc) as current_avg, count(s_score) over (order by s_score desc) as count_, max(s_score) over (order by s_score desc) as max_score, min(s_score) over (order by s_score desc) as min_score from Score s_id c_id s_score current_sum current_avg count_ max_score min_score 01 03 99 99 99.0000 1 99 99 07 03 98 197 98.5000 2 99 98 01 02 90 287 95.6667 3 99 90 01 01 80 527 87.8333 6 99 80 02 03 80 527 87.8333 6 99 80 03 01 80 527 87.8333 6 99 80 05 01 76 603 86.1429 7 99 76 02 01 70 673 84.1250 8 99 70 02 02 60 733 81.4444 9 99 60 04 01 50 783 78.3000 10 99 50 06 03 34 817 74.2727 11 99 34 06 01 31 848 70.6667 12 99 31 04 02 30 878 67.5385 13 99 30 04 03 20 898 64.1429 14 99 20 先按照c_id字段分组，分组之后按照s_score升序排列。注意所有的聚合操作都是在分组内进行，组与组之间没有联系。 #既有order by，又有partition的查询 select s_id, c_id, s_score , sum(s_score) over (partition by c_id order by s_score) as current_sum, avg(s_score) over (partition by c_id order by s_score) as current_avg, count(s_score) over (partition by c_id order by s_score) as count_, max(s_score) over (partition by c_id order by s_score) as max_score, min(s_score) over (partition by c_id order by s_score) as min_score from Score s_id c_id s_score current_sum current_avg count_ max_score min_score 04 01 50 81 40.5000 2 50 31 06 01 31 31 31.0000 1 31 31 02 01 70 151 50.3333 3 70 31 05 01 76 227 56.7500 4 76 31 01 01 80 387 64.5000 6 80 31 03 01 80 387 64.5000 6 80 31 04 02 30 30 30.0000 1 30 30 02 02 60 90 45.0000 2 60 30 01 02 90 180 60.0000 3 90 30 04 03 20 20 20.0000 1 20 20 06 03 34 54 27.0000 2 34 20 02 03 80 134 44.6667 3 80 20 07 03 98 232 58.0000 4 98 20 01 03 99 331 66.2000 5 99 20 滑动窗口 # 对前n行（包括当前行），求的值 OVER (ORDER BY ROWS n PRECEDING) # 对前n行到后n行，求的值 OVER (ORDER BY ROWS BETWEEN n PRECEDING AND n FOLLOWING) # UNBOUNDED PRECEDING表示分组内的第一行，UNBOUNDED FOLLOWING表示分组内的最后一行 OVER (PARTITIOIN BY ORDER BY ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) "},"SQLCases.html":{"url":"SQLCases.html","title":"SQL案例","keywords":"","body":"在互联网行业，会有一些常用的数据指标和常见的数据分析方法包括：事件分析，留存分析，漏斗分析，分布分析，路径分析，LTV分析等。在做数据分析，数据报表时，通常用SQL来计算这些数据指标或者实现这些数据分析方法。 下面模拟一个简单的用户行为事件表。基于这个事件表，用SQL来实现常见的数据指标和数据分析方法。 # 用户行为事件表 CREATE TABLE Events ( user_id VARCHAR(20) NOT NULL DEFAULT '', event_name VARCHAR(20) NOT NULL DEFAULT '', event_time VARCHAR(20) NOT NULL DEFAULT '' ); 事件量，用户量 一段时间内的事件量。在真实的大数据分析场景，因为数据量比较大，通常一定是需要限定时间范围的。而且时间字段通常用来作为分区表的分区字段，比如：按天分区。 一段时间内的用户量就是总事件量按用户做去重处理。 # 一段时间内的总事件量 SELECT count(event_name) FROM Events WHERE event_time >= '2022-01-01 00:00:00' AND event_time = '2022-01-01 00:00:00' AND event_time DAU，UV，PV 数据分析中经常需要按天展示DAU，在Web侧通常称为UV。或者按天展示PV，APP侧不太讲PV这个概念。 # 一段时间内特定事件的按天DAU SELECT date(event_time) AS day, COUNT(DISTINCT user_id) FROM Events WHERE event_name = \"play\" AND event_time >= \"2022-01-01 00:00:00\" AND event_time = \"2022-01-01 00:00:00\" AND event_time 以上SQL直接通过GROUP BY按天展示数据的一个问题是：如果某一天没有数据，那么在SQL查询的结果中就不会出现该天的数据。但因为最终的数据是要拿来做展示的，所以如果某天没有数据，也需要增加该日期的数据行，用0来补充。具体到项目中，如果用来展示数据的 BI 能够兼容缺失日期的数据当然最好。如果 BI 无法兼容，用于请求数据库或者数仓的后端服务也可以兼容该逻辑。如果后端服务也不愿意兼容，最后只能还是利用SQL实现，不过SQL实现这个需求并不简单且不直观。 SQL实现该需求需要分两步进行： 生成按行的连续日期。根据所用数据库或者数仓的不同，应该有不同的实现方式。 按行的连续日期和原数据做LEFT JOIN。利用LEFT JOIN时右表某行不存在则用NULL替代的特性，将NULL替换为0即可。 # 给定时间范围，生成按行的连续日期 SELECT @i := @i + 1 AS 'NO', DATE( DATE_ADD('2022-01-01',INTERVAL @i DAY)) as date # 开始时间 FROM Events, # 随便给表，需要保证表的行数比需要的天数多 (SELECT @i := - 1) t # 设置初始值为-1的变量i WHERE @i 以时间范围内的按行连续日期表为基础，结合之前查询DAU的数据，进行联表查询，获得每个日期的DAU数据。观察一下数据查询结果中，2022-01-08~2022-01-10三天的DAU数据均为0。 SELECT t1.day, ifnull(t2.DAU, 0) as DAU FROM (SELECT @i := @i + 1 AS NO, DATE( DATE_ADD( '2022-01-01', INTERVAL @i DAY )) AS day # 开始时间 FROM Events, ( SELECT @i := - 1 ) t WHERE @i = \"2022-01-01 00:00:00\" AND event_time day DAU 2022-01-01 3 2022-01-02 3 2022-01-03 3 2022-01-04 1 2022-01-05 2 2022-01-06 2 2022-01-07 2 2022-01-08 0 2022-01-09 0 2022-01-10 0 留存分析，留存率 A日期的活跃用户在B日期的留存率计算可以分为一下步骤： 计算A日期活跃的用户ID 计算A日期活跃的用户哪些在B日期也活跃过 将以上两步的计算结果做join，算出留存率 对于复杂的SQL，可读性通常很差。可以利用WITH AS来相对提升可读性。t1用来计算起始天活跃的用户 ，t2用来计算起始天用户在留存天留存的用户。 WITH t1 AS ( # 计算起始天的活跃用户 SELECT DISTINCT user_id FROM Events WHERE date(event_time) = '2022-01-01' ), t2 AS ( # 计算留存天的活跃用户 SELECT DISTINCT user_id FROM Events WHERE user_id IN (SELECT * FROM t1) AND date(event_time) = '2022-01-02' ) SELECT COUNT(t2.user_id) / COUNT(t1.user_id) AS retention # COUNT不会包含NULL FROM t1 LEFT JOIN t2 ON t1.user_id = t2.user_id; 以上SQL只是求得了某一日期的次日留存率。但在数据分析中，通常我们希望看到的是如下的留存曲线。留存曲线描绘了特定日期之后一段时间每天的留存数据。我们需要用SQL来实现绘制该留存曲线所需要的数据。 和求单日留存率最大的不同点在于需要同时对用户ID和日期做GROUP BY。LEFT JOIN的主要目的是为了求时间差。以下SQL展示了如何求得时间差diff这一中间结果。 WITH t1 AS ( SELECT user_id, date(event_time) AS day FROM Events WHERE date(event_time) = '2022-01-01' GROUP BY user_id, date(event_time) ), t2 AS ( SELECT user_id, date(event_time) AS day FROM Events WHERE event_time >= \"2022-01-01 00:00:00\" AND event_time user_id start_day retent_day diff 01 2022-01-01 2022-01-07 6 01 2022-01-01 2022-01-05 4 01 2022-01-01 2022-01-03 2 01 2022-01-01 2022-01-02 1 01 2022-01-01 2022-01-01 0 03 2022-01-01 2022-01-07 6 03 2022-01-01 2022-01-06 5 03 2022-01-01 2022-01-05 4 03 2022-01-01 2022-01-01 0 04 2022-01-01 2022-01-03 2 04 2022-01-01 2022-01-02 1 04 2022-01-01 2022-01-01 0 以上结果中retent_day表示该用户有留存的日期，diff就是留存日期到起始日期的差值，1就是次日留存，2就2日留存，3就是3日留存，以此类推。要求留人数存就只需要按照diff进行GROUP BY求COUNT。如果要求留存率就还要增加一步。而且这里同样存在某天如果是留存人数为0，则结果中不存在该天的问题，这里就不再赘述。 SELECT datediff(t2.day, t1.day) AS diff, COUNT(*) FROM t1 LEFT JOIN t2 ON t1.user_id = t2.user_id GROUP BY diff ORDER BY diff; diff count(*) 0 3 1 2 2 2 4 2 5 1 6 2 漏斗分析，转化率 漏斗分析中的转化率其实和留存率有相似之处。留存率可以看成是相对于日期的转化率，而漏斗分析中的转化率可以看成是相对于具体事件的转化率。但是通常在漏斗分析时，我们会对转化的时间做出限制，比如转化必须发生在1小时之内。 选取Events表中的login、play、like三个事件做转化分析。用Events表和自身进行JOIN，筛选其中间隔小于1小时的两个不相同的事件。每个转化，同一个用户只记录一次。 SELECT event1.event_name AS event1, event2.event_name AS event2, COUNT(DISTINCT event1.user_id) AS num_conversions FROM Events event1 INNER JOIN Events event2 ON event1.event_name <> event2.event_name AND event1.user_id = event2.user_id AND TIMESTAMPDIFF(SECOND, event1.event_time, event2.event_time) 0 WHERE event1.event_name IN ('login', 'play', 'like') AND event2.event_name IN ('play', 'like') GROUP BY event1.event_name, event2.event_name event1 event2 num_conversions login like 3 login play 4 play like 3 以上可以看出login到like发生了3人次的转化，login到play发生了4次转化，play到like发生了3次转化。如果最终要求出转化率，还要先把起始事件的人数求出。对以上SQL做一点改动。 SELECT event1.event_name AS event1, event2.event_name AS event2, COUNT(DISTINCT event1.user_id) AS num_conversions, CAST(COUNT(DISTINCT event1.user_id) AS FLOAT) / (SELECT COUNT(DISTINCT user_id) FROM Events AS e WHERE e.event_name = event1.event_name) AS conversion_rate FROM Events AS event1 INNER JOIN Events AS event2 ON event1.event_name <> event2.event_name AND event1.user_id = event2.user_id AND TIMESTAMPDIFF(SECOND, event1.event_time, event2.event_time) 0 where event1.event_name in ('login', 'play', 'like') and event2.event_name in ('play', 'like') GROUP BY event1.event_name, event2.event_name event1 event2 num_conversions conversion_rate login like 3 0.75 login play 4 1 play like 3 0.75 用户分群 在实际数据分析中，我们通常需要按照一定的用户属性或者行为特征对用户进行分群分析。一道经典的SQL面试题：求连续登录3天的用户ID，就是一个典型的用户分群案例。用户连续登录N天可以反应用户某种程度的活跃。用户连续N次做某件事情也会经常被应用于运营活动中。 以下就尝试用SQL求解连续登录3天的用户ID： 将事件时间event_time转换为事件天event_day。 利用窗口函数，按用户ID分组，按照事件天event_day排序。给每一天打上一个排序后的行号rn。 将事件天event_day减去对应的行号rn得到一个flag_day。如果是连续登录的，则flag_day应该是同一天。 按照用户ID和flag_day分组，如果同一天出现大于等3，则表示该用户从start_day开始连续登录了3天及以上。 WITH t1 AS ( # 将事件时间event_time转换为事件天event_day SELECT user_id, date(event_time) AS event_day FROM Events GROUP BY user_id, date(event_time) ), t2 AS ( # 利用窗口函数，按用户ID分组，按照事件天event_day排序。给每一天打上一个排序后的行号rn SELECT user_id, event_day, row_number() OVER (PARTITION BY user_id ORDER BY event_day) AS rn FROM t1 ), t3 AS ( # 将事件天event_day减去对应的行号rn得到一个flag_day。如果是连续登录的，则flag_day应该是同一天 SELECT *, date_sub(event_day, INTERVAL rn DAY) AS flag_day FROM t2 ) # 按照用户ID和flag_day分组，如果同一天出现大于等3，则表示该用户从start_day开始连续登录了3天及以上 SELECT date_add(flag_day, INTERVAL 1 DAY) AS start_day, user_id FROM t3 GROUP BY user_id, flag_day HAVING COUNT(user_id) >= 3; start_day user_id 2022-01-01 01 2022-01-02 02 2022-01-04 03 2022-01-01 04 附 以上内容所需的数据库表结构和数据库表数据如下： CREATE TABLE Events( user_id VARCHAR(20) NOT NULL DEFAULT '', event_name VARCHAR(20) NOT NULL DEFAULT '', event_time VARCHAR(20) NOT NULL DEFAULT '' ); insert into Events values('01', 'login', '2022-01-01 11:50:31'); insert into Events values('01', 'click', '2022-01-01 11:51:10'); insert into Events values('01', 'view', '2022-01-01 11:55:59'); insert into Events values('01', 'play', '2022-01-01 12:01:03'); insert into Events values('01', 'like', '2022-01-01 12:06:42'); insert into Events values('01', 'login', '2022-01-02 15:50:31'); insert into Events values('01', 'play', '2022-01-02 16:01:03'); insert into Events values('01', 'like', '2022-01-02 16:06:42'); insert into Events values('01', 'login', '2022-01-03 21:50:31'); insert into Events values('01', 'play', '2022-01-03 22:01:03'); insert into Events values('01', 'login', '2022-01-05 08:50:31'); insert into Events values('01', 'view', '2022-01-05 08:55:59'); insert into Events values('01', 'play', '2022-01-05 09:01:03'); insert into Events values('01', 'like', '2022-01-05 09:06:42'); insert into Events values('01', 'logout', '2022-01-05 09:51:10'); insert into Events values('01', 'login', '2022-01-07 22:50:31'); insert into Events values('02', 'login', '2022-01-02 09:10:31'); insert into Events values('02', 'play', '2022-01-02 09:15:03'); insert into Events values('02', 'like', '2022-01-02 09:20:55'); insert into Events values('02', 'login', '2022-01-03 13:15:31'); insert into Events values('02', 'play', '2022-01-03 13:20:03'); insert into Events values('02', 'logout', '2022-01-03 13:25:55'); insert into Events values('02', 'login', '2022-01-04 01:50:31'); insert into Events values('02', 'click', '2022-01-04 02:51:10'); insert into Events values('02', 'view', '2022-01-04 02:55:59'); insert into Events values('02', 'play', '2022-01-04 03:01:03'); insert into Events values('02', 'like', '2022-01-04 03:06:42'); insert into Events values('02', 'login', '2022-01-06 18:50:31'); insert into Events values('02', 'view', '2022-01-06 18:55:01'); insert into Events values('02', 'like', '2022-01-06 18:59:42'); insert into Events values('03', 'login', '2022-01-01 16:23:07'); insert into Events values('03', 'login', '2022-01-05 23:50:31'); insert into Events values('03', 'play', '2022-01-06 00:01:03'); insert into Events values('03', 'like', '2022-01-06 00:06:42'); insert into Events values('03', 'login', '2022-01-07 13:08:31'); insert into Events values('03', 'click', '2022-01-07 13:10:10'); insert into Events values('03', 'view', '2022-01-07 13:47:59'); insert into Events values('04', 'login', '2022-01-01 19:43:31'); insert into Events values('04', 'play', '2022-01-01 19:47:03'); insert into Events values('04', 'logout', '2022-01-01 19:53:55'); insert into Events values('04', 'login', '2022-01-02 18:43:31'); insert into Events values('04', 'play', '2022-01-02 18:47:03'); insert into Events values('04', 'logout', '2022-01-02 18:53:55'); insert into Events values('04', 'login', '2022-01-03 17:43:31'); insert into Events values('04', 'play', '2022-01-03 17:47:03'); insert into Events values('04', 'logout', '2022-01-03 17:53:55'); "},"ProbabilityAndStatistics.html":{"url":"ProbabilityAndStatistics.html","title":"概率论与数理统计","keywords":"","body":"基本概念定义 概率论基本概念 随机事件：随机实验中可能发生也可能不发生的事情，简称事件 必然事件：随机实验中必然发生写事件，用符号 Ω\\OmegaΩ 表示 不可能事件：随机试验中必然不发生的事件，用符号 ∅\\varnothing∅ 表示 随机实验 EEE 中必然发生一个且仅发生一个的最简单事件为实验 EEE 的基本事件，由若干基本事件组合而成的事件成为复合事件。一个事件是否为基本事件是相对于实验目的而言的。 我们用集合表示事件，对于随机实验 EEE 的每一个基本事件，用一个只包含一个元素 ω\\omegaω 的单元素 {ω}\\{\\omega\\}{ω} 表示；复合事件，则用对应的若干个元素所组成的集合表示； 由全体基本事件所对应的全部元素所组成的集合，称为随机实验 EEE 的样本空间，样本空间仍然用 Ω\\OmegaΩ 表示，和必然事件一样。样本空间的每一个元素 ω\\omegaω 为样本点。 随机变量：设 Ω\\OmegaΩ 是随机试验 EEE 的样本空间，若对于每一个样本点 ω∈Ω\\omega \\in \\Omegaω∈Ω ，都有唯一的实数 X(ω)X(\\omega)X(ω) 与之对应，且对于任意实数 xxx ，都有确定的概率 P{X(ω)≤x}P\\{X(\\omega) \\leq x\\}P{X(ω)≤x} 与之对应，则称 X(ω)X(\\omega)X(ω) 为随机变量，简记为 XXX 。随机变量是一个函数。 概率分布函数：设 Ω\\OmegaΩ 是随机试验 EEE 的样本空间，xxx 是任意实数，称函数 F(x)=P{X≤x}=P{ω:X(ω)≤x} F(x) = P\\{X \\leq x\\}= P\\{\\omega:X(\\omega) \\leq x\\} F(x)=P{X≤x}=P{ω:X(ω)≤x} 为随机变量 XXX 的分布函数 ，F(x)F(x)F(x) 也可以记作 FX(x)F_X(x)FX​(x) 。 概率密度：设 F(x)F(x)F(x) 是随机变量 XXX 的分布函数，若存在非负函数 f(x)f(x)f(x) ，对任意实数 xxx ，有 F(x)=∫−∞xf(u)du F(x) = \\int_{-\\infty}^x f(u)du F(x)=∫−∞x​f(u)du 则称 XXX 是连续型随机变量，称 f(x)f(x)f(x) 为 XXX 的概率密度。 数理统计基本概念 总体：研究对象的全体 个体：组成总体的每个基本元素 赋有一定概率分布的总体称为统计总体，其概率分布称为总体分布。当总体分布为正态分布时，称为正态分布总体或简称正态总体。 总体的概率分布是总体的核心。因此，进一步将总体看成具有相应的概率分布的随机变量，比如 XXX ，称作总体 XXX ，则随机变量 XXX 的概率分布就是总体分布。 样本是按一定的规定从总体中抽出的一部分个体。这里的”按一定的规定“，是指为保证总体中的每一个个体有同等的被抽出的机会而采取的一些措施。取得样本的过程，称为抽样。 样本是一组随机变量，记为 X1,X2,⋯ ,XnX_1,X_2,\\cdots,X_nX1​,X2​,⋯,Xn​ ，其中 nnn 称为样本容量或样本大小或样本量。实施抽样后得到的具体数据 x1,x2,⋯ ,xnx_1,x_2,\\cdots,x_nx1​,x2​,⋯,xn​ 称为样本观测值。 简单随机样本： 样本 X1,X2,⋯ ,XnX_1,X_2,\\cdots,X_nX1​,X2​,⋯,Xn​ 满足以下要求的称之为简单随机样本，如果没有特别说明，通常都是简单随机样本： 代表性。每个 XiX_iXi​ 应该与总体 XXX 有相同的分布； 独立性。 X1,X2,⋯ ,XnX_1,X_2,\\cdots,X_nX1​,X2​,⋯,Xn​ 应该是相互独立的随机变量。 统计量：设 X1,X2,⋯ ,XnX_1,X_2,\\cdots,X_nX1​,X2​,⋯,Xn​ 为来自总体 XXX 的一个样本，若样本函数 g(X1,X2,⋯ ,Xn)g(X_1,X_2,\\cdots,X_n)g(X1​,X2​,⋯,Xn​) 中不含任何未知参数，则称 g(X1,X2,⋯ ,Xn)g(X_1,X_2,\\cdots,X_n)g(X1​,X2​,⋯,Xn​) 为一个统计量。常用的统计量有：样本均值，样本方差，样本标准差。 将样本观测值 x1,x2,⋯ ,xnx_1,x_2,\\cdots,x_nx1​,x2​,⋯,xn​ 带入统计量公式中得到的值称之为统计值。 统计量也是随机变量，统计量的分布称为抽样分布，比如样本均值的抽样分布。 抽样分布定理 设 X1,X2,⋯ ,XnX_1,X_2,\\cdots,X_nX1​,X2​,⋯,Xn​ 是正态总体 N(μ,σ2)N(\\mu,\\sigma^2)N(μ,σ2) 的样本，Xˉ\\bar{X}Xˉ 、S2S^2S2 分别是样本均值和样本方差，则有： (1)Xˉ与S2相互独立(2)Xˉ∼N(μ,σ2n)(3)n−1σ2S2∼χ2(n−1)(4)Xˉ−μS/n∼t(n−1) \\begin{align*} &(1)\\quad \\bar{X} 与 S^2 相互独立\\\\ &(2)\\quad \\bar{X}\\sim N(\\mu,\\frac{\\sigma^2}{n}) \\\\ &(3)\\quad \\frac{n-1}{\\sigma^2}S^2\\sim \\chi^2(n-1) \\\\ &(4)\\quad \\frac{\\bar{X}-\\mu}{S/\\sqrt{n}}\\sim t(n-1)\\\\ \\end{align*} ​(1)Xˉ与S2相互独立(2)Xˉ∼N(μ,nσ2​)(3)σ2n−1​S2∼χ2(n−1)(4)S/n​Xˉ−μ​∼t(n−1)​ 参数估计 参数估计在机器学习导出损失函数以及A/B实验的计算中起着重要的作用。 参数估计是统计推断的基本问题之一。在实际问题中，往往遇到总体的分布类型已知，而所依据的几个参数未知的情形。针对未知参数，借助于总体的样本对其做出估计。 参数的点估计 矩估计法和极大似然估计法是常用的参数点估计方法。 跟据矩估计法和极大似然估计法均可得出，若 X1,X2,⋯ ,XnX_1,X_2,\\cdots,X_nX1​,X2​,⋯,Xn​ 是正态总体 N(μ,σ2)N(\\mu,\\sigma^2)N(μ,σ2) 的样本，则均值 μ\\muμ 和方差 σ2\\sigma^2σ2 的估计为： μ^=Xˉ,σ^2=1n∑i=1n(Xi−Xˉ)2 \\hat{\\mu}=\\bar{X},\\quad \\hat{\\sigma}^2=\\frac{1}{n}\\sum_{i=1}^n(X_i-\\bar{X})^2 μ^​=Xˉ,σ^2=n1​i=1∑n​(Xi​−Xˉ)2 但基于参数估计无偏性的准则，由矩估计法和极大似然估计法求得的 σ^2\\hat{\\sigma}^2σ^2 并不是无偏的。需要将分母 nnn 修正为 n−1n-1n−1，也就是 S2=1n−1∑i=1n(Xi−Xˉ)2 S^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X})^2 S2=n−11​i=1∑n​(Xi​−Xˉ)2 为 σ2\\sigma^2σ2 的无偏估计。 区间估计 利用枢轴变量法构造置信区间 一个正态总体参数的置信区间 设 X1,X2,⋯ ,XnX_1,X_2,\\cdots,X_nX1​,X2​,⋯,Xn​ 是正态总体 N(μ,σ2)N(\\mu,\\sigma^2)N(μ,σ2) 的样本，求未知参数 μ\\muμ 的置信度为 1−α1-\\alpha1−α 的置信区间： σ2\\sigma^2σ2 已知 因为样本均值 Xˉ\\bar{X}Xˉ 是 μ\\muμ 的无偏估计，且根据抽样分布定理 Xˉ∼N(μ,σ2n)\\bar{X}\\sim N(\\mu,\\frac{\\sigma^2}{n})Xˉ∼N(μ,nσ2​) ，所以： U=Xˉ−μσ/n∼N(0,1) U=\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1) U=σ/n​Xˉ−μ​∼N(0,1) 于是由标准正态分布的上侧分位数的定义可知，对于给定的置信度 1−α1-\\alpha1−α ，有 P{∣U∣≤uα2}=1−αP\\{|U|\\leq u_\\frac{\\alpha}{2}\\}=1-\\alphaP{∣U∣≤u2α​​}=1−α，即： P{−uα2≤Xˉ−μσ/n≤uα2}=P{Xˉ−σnuα2≤μ≤Xˉ+σnuα2}=1−α \\begin{align*} & P\\{-u_\\frac{\\alpha}{2} \\leq \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\leq u_\\frac{\\alpha}{2}\\} \\\\ =& P\\{\\bar{X}-\\frac{\\sigma}{\\sqrt{n}}u_\\frac{\\alpha}{2} \\leq \\mu \\leq \\bar{X}+\\frac{\\sigma}{\\sqrt{n}}u_\\frac{\\alpha}{2}\\} \\\\ =& 1-\\alpha \\end{align*} ==​P{−u2α​​≤σ/n​Xˉ−μ​≤u2α​​}P{Xˉ−n​σ​u2α​​≤μ≤Xˉ+n​σ​u2α​​}1−α​ 从而得到 μ\\muμ 的置信度为 1−α1-\\alpha1−α 的置信区间为 [Xˉ−σnuα2,Xˉ+σnuα2][\\bar{X}-\\frac{\\sigma}{\\sqrt{n}}u_\\frac{\\alpha}{2},\\bar{X}+\\frac{\\sigma}{\\sqrt{n}}u_\\frac{\\alpha}{2}][Xˉ−n​σ​u2α​​,Xˉ+n​σ​u2α​​] σ2\\sigma^2σ2 未知 此时 U=Xˉ−μσ/nU=\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}U=σ/n​Xˉ−μ​ 不再构成枢轴变量，因为 σ2\\sigma^2σ2 未知，故用 S2S^2S2 代替 σ2\\sigma^2σ2 。根据抽样分布定理，枢轴变量 T=Xˉ−μS/n∼t(n−1)T=\\frac{\\bar{X}-\\mu}{S/\\sqrt{n}}\\sim t(n-1)T=S/n​Xˉ−μ​∼t(n−1) 。因为 ttt 分布也是关于 YYY 轴对称，于是有： P{−tα2(n−1)≤Xˉ−μS/n≤tα2(n−1)}=1−α P\\{-t_\\frac{\\alpha}{2}(n-1) \\leq \\frac{\\bar{X}-\\mu}{S/\\sqrt{n}} \\leq t_\\frac{\\alpha}{2}(n-1)\\}=1-\\alpha P{−t2α​​(n−1)≤S/n​Xˉ−μ​≤t2α​​(n−1)}=1−α 经过恒等变形，得到参数 μ\\muμ 的置信度为 1−α1-\\alpha1−α 的置信区间是 [Xˉ−Sntα2(n−1),Xˉ+Sntα2(n−1)][\\bar{X}-\\frac{S}{\\sqrt{n}}t_\\frac{\\alpha}{2}(n-1),\\bar{X}+\\frac{S}{\\sqrt{n}}t_\\frac{\\alpha}{2}(n-1)][Xˉ−n​S​t2α​​(n−1),Xˉ+n​S​t2α​​(n−1)] 两个正态总体的区间估计 大样本方法构造置信区间 单侧置信区间 "},"GitCheatSheet.html":{"url":"GitCheatSheet.html","title":"Git Cheat Sheet","keywords":"","body":" 创建仓库提交文件 创建一个版本库 $ git init Initialized empty Git repository in /Users/xxx/xxx/.git/ 把文件添加进暂存区 $ git add readme.txt $ git add file1.txt file2.txt # 添加当前目录下的所有文件进暂存区 $ git add . 把文件提交到本地仓库 $ git commit -m \"wrote a readme file\" 修改上一次commit。比如修改之前提交时的日志，或者追加提交文件等。 $ git commit --amend # 会弹出一个vim界面，在vim界面上做相应的修改即可 查看当前状态，也可以查看到位于哪个分支 $ git status # On branch master # Your branch is ahead of 'origin/master' by 202 commits. # # Untracked files: # (use \"git add ...\" to include in what will be committed) # # ../profile.svg # ../templates/customer_service.html nothing added to commit but untracked files present (use \"git add\" to track) 查看修改内容 $ git diff readme.txt diff --git a/readme.txt b/readme.txt index 46d49bf..9247db6 100644 --- a/readme.txt +++ b/readme.txt @@ -1,2 +1,2 @@ -Git is a version control system. +Git is a distributed version control system. Git is free software. 本地版本控制 版本查看 $ git log commit 1094adb7b9b3807259d8cb349e7df1d4d6477073 (HEAD -> master) Author: Michael Liao Date: Fri May 18 21:06:15 2018 +0800 append GPL commit e475afc93c209a690c39c13a46716e8fa000c366 Author: Michael Liao Date: Fri May 18 21:03:36 2018 +0800 add distributed 回退到上一个版本，回退到上上个版本 $ git reset --hard HEAD^ HEAD is now at e475afc add distributed $ git reset --hard HEAD^ 通过版本号更改到指定版本，可以是之后的版本，也可以是之前的版本 $ git reset --hard 1094a HEAD is now at 83b0afe append GPL 回退到之前的版本之后，无法通过git log查看到后面的版本，可以通过git relog来查看每一次输入的命令，查看到后面版本的版本号 $ git reflog e475afc HEAD@{1}: reset: moving to HEAD^ 1094adb (HEAD -> master) HEAD@{2}: commit: append GPL e475afc HEAD@{3}: commit: add distributed eaadf4e HEAD@{4}: commit (initial): wrote a readme file 远程仓库 配置本地全局用户名和Email $ git config --global user.name \"Your Name\" $ git config --global user.email \"email@example.com\" # 查看配置信息 git config --list 给本地特定仓库设置特定的用户名和Email，需要去到该仓库的根目录下利用git config进行设置。去掉--global参数。可以在该仓库的.git/config查看具体的配置信息。 $ git config user.name \"Your Name\" $ git config user.email \"email@example.com\" 验证本地git和github的连通性 $ ssh -T git@github.com Hi powerAmore! You've successfully authenticated, but GitHub does not provide shell access. 关联远程仓库，并把远程仓库取名为origin $ git remote add origin git@github.com:xxx/learngit.git 第一次将本地仓库master分支推送到远程origin仓库。-u参数会将本地master分支和远程master分支做关联。 $ git push -u origin master Counting objects: 20, done. Delta compression using up to 4 threads. Compressing objects: 100% (15/15), done. Writing objects: 100% (20/20), 1.64 KiB | 560.00 KiB/s, done. Total 20 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), done. To github.com:michaelliao/learngit.git * [new branch] master -> master Branch 'master' set up to track remote branch 'master' from 'origin'. 后续将本地仓库master分支内容推送到远程仓库 $ git push origin master 查看远程仓库信息 $ git remote -v origin git@github.com:xxx/learn-git.git (fetch) origin git@github.com:xxx/learn-git.git (push) 解除和远程仓库origin的关联。如果要删除远程仓库，需要去到远程仓库所在地址进行删除。 $ git remote rm origin 从远程仓库克隆一个本地库 $ git clone git@github.com:xxx/gitskills.git Cloning into 'gitskills'... remote: Counting objects: 3, done. remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 3 Receiving objects: 100% (3/3), done. 从远程仓库拉取代码并合并本地版本 $ git pull $ git pull origin git pull其实就是 git fetch 和git merge FETCH_HEAD 的简写。格式如下： # git pull : # 拉取远程仓库origin的master分支，与本地的brantest分支合并 $ git pull origin master:brantest # 如果远程分支是与当前分支合并，则冒号后面的部分可以省略 $ git pull origin master 分支操作 创建dev分支，然后切换到dev分支 $ git checkout -b dev Switched to a new branch 'dev' git checkout命令加上-b参数表示创建并切换，相当于以下两条命令。其中git branch dev为创建本地分支，git checkout dev为切换分支 $ git branch dev $ git checkout dev Switched to branch 'dev' switch也可以用来切换分支，相比checkout更容易理解。因为撤销修改是git checkout -- ，同一个命令，有两种作用，确实有点令人迷惑。 创建并切换到新的dev分支 $ git switch -c dev 直接切换到已有的master分支 $ git switch master 用git branch命令查看当前分支 $ git branch * dev master 把dev分支的工作成果合并到当前所处的master分支。Fast-forward信息表示这次合并是“快进模式”，也就是直接把master指向dev的当前提交。也不是每次合并都能Fast-forward。 $ git merge dev Updating d46f35e..b17d20e Fast-forward readme.txt | 1 + 1 file changed, 1 insertion(+) 删除dev分支 $ git branch -d dev Deleted branch dev (was b17d20e). "}}