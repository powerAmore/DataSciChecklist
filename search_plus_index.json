{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction 工作中涉及到了数据领域的方方面面，包括大数据技术、机器学习、深度学习、推荐系统、数据产品、数据分析等非常多的领域。然而由于工作重心不断变换，一段时间不接触，很多领域的基础知识非常容易遗忘，非常可惜。重新查阅相关资料非常耗时，有些基于自身理解的概念，即使查阅资料也并不容易找回。 对于查阅资料。网络上各种资料教程非常多，也不乏高质量的课程。但是问题也很明显，多数教程面向初学者，极其细致，看完往往需要很长时间，学习效率较低。尤其视频教程，甚至很难快速直接找到关注点。对于在该领域已经建立整体概念框架的人来说，需要的往往是提纲挈领，直击要害，把书读薄。 故做此Checklist，帮助自己也帮助需要复习相关领域知识点的小伙伴快速恢复记忆。务必做到言简意赅，突出重点。 "},"GradientDescent.html":{"url":"GradientDescent.html","title":"梯度下降","keywords":"","body":"梯度下降法 机器学习深度学习中用梯度下降法来优化损失函数，试图求解损失函数的最小值以及其对应的参数。要搞清楚梯度下降法，我们从方向导数的概念引入。 方向导数 方向导数就是曲面切线的斜率。曲面的切线不是唯一的，360∘360^{\\circ}360∘ 各个方向都有，所以不同方向的切线斜率也不一定相同，方向导数也不是唯一的。 例如二元函数 f(x,y)=x2+y2f(x,y)=x^2+y^2f(x,y)=x2+y2 的方向导数如图所示： 函数 f(x,y)f(x,y)f(x,y) 在 PPP 点沿着切线 lll 方向的方向导数为： lim⁡ρ→0f(x+Δx,y+Δy)−f(x,y)ρ {\\lim_{\\rho\\to0}\\frac{f(x+\\Delta x, y+\\Delta y)-f(x,y)}{\\rho}} ρ→0lim​ρf(x+Δx,y+Δy)−f(x,y)​ 其中 ρ=(Δx)2+(Δy)2\\rho=\\sqrt{(\\Delta x)^2+(\\Delta y)^2}ρ=(Δx)2+(Δy)2​ 。把该极限记作方向导数 ∂f∂l\\frac{\\partial f}{\\partial l}∂l∂f​ 。其计算公式为： ∂f∂l=∂f∂xcosφ+∂f∂ysinφ \\frac{\\partial f}{\\partial l} = \\frac{\\partial f}{\\partial x}cos\\varphi + \\frac{\\partial f}{\\partial y}sin\\varphi ∂l∂f​=∂x∂f​cosφ+∂y∂f​sinφ 其中 φ\\varphiφ 为x轴正方向到 lll 的角度。 梯度 梯度是个向量，它的模为曲面上该点取值最大的方向导数的值，方向为最大方向导数对应的切线的投影方向。也是该点所处的等高线的法向量，也就是函数值变化最快的方向。 [!NOTE] 梯度的方向并不是最大方向导数对应的切线方向，而是切线的投影方向。但即使认为是切线方向，对理解梯度的概念也不会产生太大的影响。 图中绿色箭头所示向量即为梯度： 对于二元函数 f(x,y)f(x,y)f(x,y) ，其曲面上任意点 P(x,y)P(x,y)P(x,y) 的梯度，记作 gradf(x,y)gradf(x,y)gradf(x,y) 或 ∇f(x,y)\\nabla f(x,y)∇f(x,y) ，定义为： ∇f(x,y)=(∂f∂x,∂f∂y)=fx(x,y)i⃗+fy(x,y)j⃗ \\nabla f(x,y)=(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y})=f_x(x,y)\\vec i+f_y(x,y)\\vec j ∇f(x,y)=(∂x∂f​,∂y∂f​)=fx​(x,y)i+fy​(x,y)j​ 仍然以二元函数 f(x,y)=x2+y2f(x,y)=x^2+y^2f(x,y)=x2+y2 为例。设 e⃗=(cosφ,sinφ)\\vec e=(cos\\varphi, sin\\varphi)e=(cosφ,sinφ) 为曲面某点切线 lll 方向导数所对应方向的单位向量，则该方向导数为： ∂f∂l=∂f∂xcosφ+∂f∂ysinφ=(∂f∂x,∂f∂y)⋅(cosφ,sinφ)=∣∇f(x,y)∣⋅∣e⃗∣⋅cos⟨∇f(x,y),e⃗⟩=∣∇f(x,y)∣⋅1⋅cos⟨∇f(x,y),e⃗⟩ \\begin{align*} \\frac{\\partial f}{\\partial l} =\\frac{\\partial f}{\\partial x}cos\\varphi + \\frac{\\partial f}{\\partial y}sin\\varphi &=(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y})\\cdot(cos\\varphi, sin\\varphi) \\\\ &=|\\nabla f(x,y)|\\cdot |\\vec e| \\cdot cos\\langle\\nabla f(x,y),\\vec e\\rangle \\\\ &=|\\nabla f(x,y)|\\cdot 1 \\cdot cos\\langle\\nabla f(x,y),\\vec e\\rangle \\end{align*} ∂l∂f​=∂x∂f​cosφ+∂y∂f​sinφ​=(∂x∂f​,∂y∂f​)⋅(cosφ,sinφ)=∣∇f(x,y)∣⋅∣e∣⋅cos⟨∇f(x,y),e⟩=∣∇f(x,y)∣⋅1⋅cos⟨∇f(x,y),e⟩​ 其中，⟨∇f(x,y),e⃗⟩\\langle\\nabla f(x,y),\\vec e\\rangle⟨∇f(x,y),e⟩ 为 ∇f(x,y)\\nabla f(x,y)∇f(x,y) 和 e⃗\\vec ee 的夹角。当 cos⟨∇f(x,y),e⃗⟩=1cos\\langle\\nabla f(x,y),\\vec e\\rangle=1cos⟨∇f(x,y),e⟩=1 时，方向导数 ∂f∂l\\frac{\\partial f}{\\partial l}∂l∂f​ 有最大值，为梯度的模 ∣∇f(x,y)∣|\\nabla f(x,y)|∣∇f(x,y)∣ ，且此时 e⃗\\vec ee 的方向和梯度 ∇f(x,y)\\nabla f(x,y)∇f(x,y) 的方向保持一致。 梯度下降法 为求解函数的最小值以及最小值对应的坐标，数学上通常要么有直接的求解公式。要么就对函数进行求导，令导函数等于0进行求解。但面对机器学习这类场景，往往数据量大、维度高导致计算量过大，或者目标函数复杂本身无法获得解析解。机器学习场景下，往往利用梯度下降这类迭代优化算法快速逼近目标函数的最小值以获得最优化的参数值。 梯度下降法的作用不仅仅是求解函数最小值，在机器学习深度学习算法中，更重要的是获取函数最小值（尽可能小）时对应的坐标，即最优化的函数参数值。 梯度下降法的原理是：试图通过迭代的方式，每步都沿着函数数值下降最快的方向（也就是负梯度方向）走一小步。每走一步都重新确认一下负梯度方向，然后再沿着该方向下降，直到找到函数最小值以及对应的位置。 [!NOTE] 之所以每步都沿着负梯度方向走有两个原因： 负梯度方向一定是会让函数值变小（至少不变大）的方向。需要注意：走出了这一步不意味着函数值就一定会变小，也有可能步子迈大了，函数值反而有所变大。 负梯度是函数值下降最快的方向，便于更快速的找到最小值。其实，即使每步不是沿着负梯度方向，只要是沿着一个函数值变小的方向，最终也是能找到函数的最小值的。 代码实现梯度下降 仍然以二元函数 f(x,y)=x2+y2f(x,y)=x^2+y^2f(x,y)=x2+y2 为例，写代码实现梯度下降法求解函数最小值。代码其实很简单，关键需要自己手动把函数梯度先求出来。10次迭代之后可以看到已经很接近函数的最小值了。如果再多迭代几次就肯定能达到最小值。 # 原函数f(x,y) def f(x,y): return x ** 2 + y ** 2 # f(x,y)对x的偏导 def fx(x): return 2 * x # f(x,y)对y的偏导 def fy(y): return 2 * y # 设置梯度下降起始点 x=2; y=2 # 设置梯度下降步长 step = 0.1 # 循环迭代进行梯度下降 for i in range(10): # 设置梯度下降迭代次数为10 before = f(x, y) # 梯度下降开始前，起始坐标下的函数值 x = x - step * fx(x) # x轴方向进行梯度下降，获得新的新坐标 y = y - step * fy(y) # y轴方向进行梯度下降，获得新的y坐标 after = f(x, y) # 梯度下降完成，基于新坐标的函数值 theta = before - after # 完成一次梯度下降迭代，前后函数值的差 print(\"before:{:.2f}, after:{:.2f}, theta:{:.2f}, x:{:.2f}, y:{:.2f}\".format(before, after, theta, x, y)) # 输出结果并保留2为小数 before:8.00, after:5.12, theta:2.88, x:1.60, y:1.60 before:5.12, after:3.28, theta:1.84, x:1.28, y:1.28 before:3.28, after:2.10, theta:1.18, x:1.02, y:1.02 before:2.10, after:1.34, theta:0.75, x:0.82, y:0.82 before:1.34, after:0.86, theta:0.48, x:0.66, y:0.66 before:0.86, after:0.55, theta:0.31, x:0.52, y:0.52 before:0.55, after:0.35, theta:0.20, x:0.42, y:0.42 before:0.35, after:0.23, theta:0.13, x:0.34, y:0.34 before:0.23, after:0.14, theta:0.08, x:0.27, y:0.27 before:0.14, after:0.09, theta:0.05, x:0.21, y:0.21 实验：用方向导数替代梯度下降 假设用方向导数来替代梯度，可以看到其实也可以起到下降的效果，只是速度要慢一些，10次迭代后离最小值还有点距离。在真实的大数据应用场景中，速度是需要考虑的非常重要的点。梯度不仅下降快，求梯度也比求方向导数更加容易。 import math # 设置梯度下降起始点 x=2; y=2 # 设置梯度下降步长 step = 0.1 for i in range(10): # 设置迭代次数为10 before = f(x, y) # 开始前，起始坐标下的函数值 x = x - step * fx(x) * math.cos(math.pi/3) # 沿着切线投影与x成60度角的方向导数进行下降 y = y - step * fy(y) * math.sin(math.pi/3) # 沿着切线投影与x成60度角的方向导数进行下降 after = f(x, y) # 完成后，基于新坐标的函数值 theta = before - after # 完成一次迭代，前后函数值的差 print(\"before:{:.2f}, after:{:.2f}, theta:{:.2f}, x:{:.2f}, y:{:.2f}\".format(before, after, theta, x, y)) # 输出结果并保留2为小数 before:8.00, after:5.97, theta:2.03, x:1.80, y:1.65 before:5.97, after:4.49, theta:1.48, x:1.62, y:1.37 before:4.49, after:3.40, theta:1.09, x:1.46, y:1.13 before:3.40, after:2.60, theta:0.81, x:1.31, y:0.93 before:2.60, after:1.99, theta:0.60, x:1.18, y:0.77 before:1.99, after:1.54, theta:0.45, x:1.06, y:0.64 before:1.54, after:1.19, theta:0.34, x:0.96, y:0.53 before:1.19, after:0.93, theta:0.26, x:0.86, y:0.44 before:0.93, after:0.73, theta:0.20, x:0.77, y:0.36 before:0.73, after:0.58, theta:0.16, x:0.70, y:0.30 画图展示梯度下降 采用plotly进行作图。之所以不用matplot，是因为matplot不太好实现surface和scatter在同一张图里展示。如图所见，红色线表示梯度下降迭代过程，逐渐逼近函数曲面最小值的过程。而蓝色线才是真正每一步迭代的梯度。 import numpy as np # 设置梯度下降起始点 x=2; y=2 # 设置梯度下降步长 step = 0.1 pos_x = [] pos_y = [] pos_z = [] # 循环迭代进行梯度下降 for i in range(10): # 设置梯度下降迭代次数为10 before = f(x, y) # 梯度下降开始前，起始坐标下的函数值 pos_x.append(x) pos_y.append(y) pos_z.append(before) x = x - step * fx(x) # x轴方向进行梯度下降，获得新的新坐标 y = y - step * fy(y) # y轴方向进行梯度下降，获得新的y坐标 after = f(x, y) # 梯度下降完成，基于新坐标的函数值 pos_x.append(x) pos_y.append(y) pos_z.append(after) pos_z = np.array(pos_z) from plotly.offline import init_notebook_mode, iplot import plotly.graph_objects as go # 画二元函数曲面 xx = np.arange(-3,3,0.1) yy = np.arange(-3,3,0.1) X, Y = np.meshgrid(xx, yy) Z = X ** 2 +Y ** 2 trace_surface= go.Surface(x=X, y=Y, z=Z, colorscale='redor', showscale=False, opacity=0.7) # 画红色trace线 trace_scatter3d = go.Scatter3d(x=pos_x, y=pos_y, z=pos_z, mode='lines+markers', marker=dict(color='red', size=3), name = 'trace') # 画蓝色gradient线 trace_gradient = go.Scatter3d(x=pos_x, y=pos_y, z=np.zeros(len(pos_x)), mode='lines+markers', marker=dict(color='blue', size=3), name = 'gradient') data=[trace_surface, trace_scatter3d, trace_gradient] # 图片布局调整 layout = go.Layout(scene = dict(aspectratio = dict(x=1.5, y=1.5, z=1)), margin=dict(l=5, r=5, t=5, b=5), width=800) fig = dict(data = data, layout = layout) iplot(fig) "},"ABTest.html":{"url":"ABTest.html","title":"A/B实验","keywords":"","body":"A/B实验 "},"TimeSeries.html":{"url":"TimeSeries.html","title":"时间序列","keywords":"","body":"import pandas as pd df = pd.read_csv(\"AirPassengers.csv\", parse_dates=[\"Month\"]).rename(columns={\"Month\":\"ds\", \"#Passengers\":\"y\"}) import matplotlib.pyplot as plt plt.close(\"all\") X_train = df[df.ds=\"19580101\"] plt.plot(X_train['ds'], X_train['y']) plt.plot(X_test['ds'], X_test['y']) [] from prophet import Prophet pro = Prophet() pro.fit(X_train) 18:55:40 - cmdstanpy - INFO - Chain [1] start processing 18:55:40 - cmdstanpy - INFO - Chain [1] done processing pred = pro.predict(X_test) df.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ds y 139 1960-08-01 606 140 1960-09-01 508 141 1960-10-01 461 142 1960-11-01 390 143 1960-12-01 432 future = pro.make_future_dataframe(periods=10, freq = 'M') future.tail(20) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ds 98 1957-03-01 99 1957-04-01 100 1957-05-01 101 1957-06-01 102 1957-07-01 103 1957-08-01 104 1957-09-01 105 1957-10-01 106 1957-11-01 107 1957-12-01 108 1957-12-31 109 1958-01-31 110 1958-02-28 111 1958-03-31 112 1958-04-30 113 1958-05-31 114 1958-06-30 115 1958-07-31 116 1958-08-31 117 1958-09-30 df ds y 0 1949-01-01 112 1 1949-02-01 118 2 1949-03-01 132 3 1949-04-01 129 4 1949-05-01 121 ... ... ... 139 1960-08-01 606 140 1960-09-01 508 141 1960-10-01 461 142 1960-11-01 390 143 1960-12-01 432 144 rows × 2 columns "},"SQLKeypoints.html":{"url":"SQLKeypoints.html","title":"SQL要点","keywords":"","body":"只包含DML（Data Manipulation Language，数据操纵语言）的内容，不包含DDL（Data Definition Language，数据定义语言）和DCL（Data Control Language，数据控制语言） 基础语法 仅举例说明，不做过多解释 其中Student表为学生信息表，Score表为学生成绩表，建表语句为： CREATE TABLE Student ( s_id VARCHAR(20) COMMENT 'student ID', s_name VARCHAR(20) NOT NULL DEFAULT '' COMMENT 'student name', s_birth VARCHAR(20) NOT NULL DEFAULT '' COMMENT 'student birthday', s_sex VARCHAR(10) NOT NULL DEFAULT '' COMMENT 'student sex', PRIMARY KEY (s_id) ); CREATE TABLE Score ( s_id VARCHAR(20) COMMENT 'student ID', c_id VARCHAR(20) COMMENT 'course ID', s_score INT(3) COMMENT 'student score', PRIMARY KEY (s_id, c_id) ); 以下为包含基础语法的查询语句： select * from Student where s_name like \"李%\"； select s_id, s_score from Score where c_id in (\"0001\", \"0002\"); select count(distinct s_id) from Score where s_score >= 60; select c_id from Score where s_score60; select s_id, s_name, s_birth from Student where year(s_birth) = 2010 limit 10; select * from Student where month(s_birth) = month(now()); # 求年纪 select s_id, s_name, s_birth, floor(timestampdiff(month, s_birth, now())/12) from Student; 进阶语法 条件表达式 case表达式 /* 语法 CASE WHEN THEN WHEN THEN . ELSE END */ # 将s_sex字段的数值转成'male'、'female'和'null'的表示形式 select s_id, s_name, (case s_sex when 1 then \"male\" when 2 then \"female\" else \"uncertain\" end) as gender from Student; # 行转列 select sum(case s_sex when 1 then 1 else 0 end) as \"male\"， sum(case s_sex when 2 then 1 else 0 end) as \"female\" from Student; if表达式 /* 语法 IF(expr1, expr2, expr3) expr1的值为 TRUE，则返回值为expr2 expr1的值为FALSE，则返回值为expr3 */ select if(s_sex = 1, \"male\", \"female\") as gender from Student where s_sex != \"\"; /* 语法 IFNULL(expr1, expr2) 判断expr1是否为NULL： 如果expr1不为空，返回expr1； 如果expr1为空， 返回expr2 */ select ifnull(s_sex, \"uncertain\") as gender from Student; 子查询 子查询指一个查询语句嵌套在另一个查询语句内部的查询，子查询的结果可以作为一个表，也可以作为一个过滤条件 # 作为一个表 # 前10名从低到高排序 select * from (select * from Score where c_id = \"0001\" order by s_score desc limit 10) order by s_score; # 作为一个过滤条件 # 高于平均分 select * from Score where s_score >= (select avg(s_score) from Score); 表关联 UNION UNION 操作符用于合并两个或多个 SELECT 语句的结果集。 UNION 内部的每个 SELECT 语句必须拥有相同数量的列。列也必须拥有相似的数据类型。同时，每个 SELECT 语句中的列的顺序必须相同。 UNION 是去重的，UNION ALL 是不去重的，只能使用一条ORDER BY子句，它必须出现在最后一条SELECT语句之后。 # UNION去重 select u_id from Score where c_id = \"0001\" union select u_id from Score where c_id = \"0002\" order by s_score; # UNION ALL不去重 select u_id from Score where c_id = \"0001\" union all select u_id from Score where c_id = \"0002\" order by s_score; JOIN 常见的7种JOIN方式见下图： # 列出学生ID、学生姓名的同时也把对应学生的分数列出来 select st.s_id, st.s_name, sc.c_id, sc.s_score from Student as st left join Score as sc on st.s_id = sc.s_id; s_id s_name c_id s_score 01 李四 03 99 01 李四 02 90 01 李四 01 80 02 王二 03 80 02 王二 02 60 查询语句运行结果中需要注意，Student表中原本的一条数据被重复了N次 窗口函数 窗口函数的语法为： OVER ([PARTITION BY ] ORDER BY ) MySQL中只有8.0版本以上才支持窗口函数 # 按照c_id分组，按照s_score排序 select s_id, c_id, s_score, rank() over (partition by c_id order by s_score desc) as ranking from Score; s_id c_id s_score ranking 01 01 80 1 03 01 80 1 05 01 76 3 02 01 70 4 04 01 50 5 06 01 31 6 01 02 90 1 02 02 60 2 04 02 30 3 01 03 99 1 排序函数 三种不同排序函数：rank()，dense_rank()，row_number() # 按成绩排序，展示三种不同排序函数的差异 select s_id, c_id, s_score, rank() over (order by s_score desc) as ranking, dense_rank() over (order by s_score desc) as denserank, row_number() over (order by s_score desc) as rownumber from Score; s_id c_id s_score ranking denserank rownumber 01 03 99 1 1 1 07 03 98 2 2 2 01 02 90 3 3 3 01 01 80 4 4 4 02 03 80 4 4 5 03 01 80 4 4 6 05 01 76 7 5 7 02 01 70 8 6 8 rank()：有相同分数时排名相同，且会占用后一名的位置，一些名次不存在，存在名次跳跃 dense_rank()：有相同分数时排名相同，但不会占用后一名的位置，名次是连续的 row_number()：有相同分数时排名也会不相同，名次是连续的 聚合函数 在窗口函数中使用聚合函数（sum，avg，count，max，min）的含义为：根据order by字段的排序，求”到目前为止“的聚合值，比如sum就是累加的含义。语言描述起来有点困难，直接看查询语句的结果理解起来就不难了。 按照s_score字段降序排列。值得注意的是：相同分数count值相同。因为降序，最大分数值出现在第一位，所以每一行的max都等。 #只有order by，没有partition的查询 select s_id, c_id, s_score , sum(s_score) over (order by s_score desc) as current_sum, avg(s_score) over (order by s_score desc) as current_avg, count(s_score) over (order by s_score desc) as count_, max(s_score) over (order by s_score desc) as max_score, min(s_score) over (order by s_score desc) as min_score from Score s_id c_id s_score current_sum current_avg count_ max_score min_score 01 03 99 99 99.0000 1 99 99 07 03 98 197 98.5000 2 99 98 01 02 90 287 95.6667 3 99 90 01 01 80 527 87.8333 6 99 80 02 03 80 527 87.8333 6 99 80 03 01 80 527 87.8333 6 99 80 05 01 76 603 86.1429 7 99 76 02 01 70 673 84.1250 8 99 70 02 02 60 733 81.4444 9 99 60 04 01 50 783 78.3000 10 99 50 06 03 34 817 74.2727 11 99 34 06 01 31 848 70.6667 12 99 31 04 02 30 878 67.5385 13 99 30 04 03 20 898 64.1429 14 99 20 先按照c_id字段分组，分组之后按照s_score升序排列。注意所有的聚合操作都是在分组内进行，组与组之间没有联系。 #既有order by，又有partition的查询 select s_id, c_id, s_score , sum(s_score) over (partition by c_id order by s_score) as current_sum, avg(s_score) over (partition by c_id order by s_score) as current_avg, count(s_score) over (partition by c_id order by s_score) as count_, max(s_score) over (partition by c_id order by s_score) as max_score, min(s_score) over (partition by c_id order by s_score) as min_score from Score s_id c_id s_score current_sum current_avg count_ max_score min_score 04 01 50 81 40.5000 2 50 31 06 01 31 31 31.0000 1 31 31 02 01 70 151 50.3333 3 70 31 05 01 76 227 56.7500 4 76 31 01 01 80 387 64.5000 6 80 31 03 01 80 387 64.5000 6 80 31 04 02 30 30 30.0000 1 30 30 02 02 60 90 45.0000 2 60 30 01 02 90 180 60.0000 3 90 30 04 03 20 20 20.0000 1 20 20 06 03 34 54 27.0000 2 34 20 02 03 80 134 44.6667 3 80 20 07 03 98 232 58.0000 4 98 20 01 03 99 331 66.2000 5 99 20 滑动窗口 # 对前n行（包括当前行），求的值 OVER (ORDER BY ROWS n PRECEDING) # 对前n行到后n行，求的值 OVER (ORDER BY ROWS BETWEEN n PRECEDING AND n FOLLOWING) # UNBOUNDED PRECEDING表示分组内的第一行，UNBOUNDED FOLLOWING表示分组内的最后一行 OVER (PARTITIOIN BY ORDER BY ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) "},"SQLCases.html":{"url":"SQLCases.html","title":"SQL案例","keywords":"","body":"在互联网行业，会有一些常用的数据指标和常见的数据分析方法包括：事件分析，留存分析，漏斗分析，分布分析，路径分析，LTV分析等。在做数据分析，数据报表时，通常用SQL来计算这些数据指标或者实现这些数据分析方法。 下面模拟一个简单的用户行为事件表。基于这个事件表，用SQL来实现常见的数据指标和数据分析方法。 # 用户行为事件表 CREATE TABLE Events ( user_id VARCHAR(20) NOT NULL DEFAULT '', event_name VARCHAR(20) NOT NULL DEFAULT '', event_time VARCHAR(20) NOT NULL DEFAULT '' ); 事件量，用户量 一段时间内的事件量。在真实的大数据分析场景，因为数据量比较大，通常一定是需要限定时间范围的。而且时间字段通常用来作为分区表的分区字段，比如：按天分区。 一段时间内的用户量就是总事件量按用户做去重处理。 # 一段时间内的总事件量 SELECT count(event_name) FROM Events WHERE event_time >= '2022-01-01 00:00:00' AND event_time = '2022-01-01 00:00:00' AND event_time DAU，UV，PV 数据分析中经常需要按天展示DAU，在Web侧通常称为UV。或者按天展示PV，APP侧不太讲PV这个概念。 # 一段时间内特定事件的按天DAU SELECT date(event_time) AS day, COUNT(DISTINCT user_id) FROM Events WHERE event_name = \"play\" AND event_time >= \"2022-01-01 00:00:00\" AND event_time = \"2022-01-01 00:00:00\" AND event_time 以上SQL直接通过GROUP BY按天展示数据的一个问题是：如果某一天没有数据，那么在SQL查询的结果中就不会出现该天的数据。但因为最终的数据是要拿来做展示的，所以如果某天没有数据，也需要增加该日期的数据行，用0来补充。具体到项目中，如果用来展示数据的 BI 能够兼容缺失日期的数据当然最好。如果 BI 无法兼容，用于请求数据库或者数仓的后端服务也可以兼容该逻辑。如果后端服务也不愿意兼容，最后只能还是利用SQL实现，不过SQL实现这个需求并不简单且不直观。 SQL实现该需求需要分两步进行： 生成按行的连续日期。根据所用数据库或者数仓的不同，应该有不同的实现方式。 按行的连续日期和原数据做LEFT JOIN。利用LEFT JOIN时右表某行不存在则用NULL替代的特性，将NULL替换为0即可。 # 给定时间范围，生成按行的连续日期 SELECT @i := @i + 1 AS 'NO', DATE( DATE_ADD('2022-01-01',INTERVAL @i DAY)) as date # 开始时间 FROM Events, # 随便给表，需要保证表的行数比需要的天数多 (SELECT @i := - 1) t # 设置初始值为-1的变量i WHERE @i 以时间范围内的按行连续日期表为基础，结合之前查询DAU的数据，进行联表查询，获得每个日期的DAU数据。观察一下数据查询结果中，2022-01-08~2022-01-10三天的DAU数据均为0。 SELECT t1.day, ifnull(t2.DAU, 0) as DAU FROM (SELECT @i := @i + 1 AS NO, DATE( DATE_ADD( '2022-01-01', INTERVAL @i DAY )) AS day # 开始时间 FROM Events, ( SELECT @i := - 1 ) t WHERE @i = \"2022-01-01 00:00:00\" AND event_time day DAU 2022-01-01 3 2022-01-02 3 2022-01-03 3 2022-01-04 1 2022-01-05 2 2022-01-06 2 2022-01-07 2 2022-01-08 0 2022-01-09 0 2022-01-10 0 留存分析，留存率 A日期的活跃用户在B日期的留存率计算可以分为一下步骤： 计算A日期活跃的用户ID 计算A日期活跃的用户哪些在B日期也活跃过 将以上两步的计算结果做join，算出留存率 对于复杂的SQL，可读性通常很差。可以利用WITH AS来相对提升可读性。t1用来计算起始天活跃的用户 ，t2用来计算起始天用户在留存天留存的用户。 WITH t1 AS ( # 计算起始天的活跃用户 SELECT DISTINCT user_id FROM Events WHERE date(event_time) = '2022-01-01' ), t2 AS ( # 计算留存天的活跃用户 SELECT DISTINCT user_id FROM Events WHERE user_id IN (SELECT * FROM t1) AND date(event_time) = '2022-01-02' ) SELECT COUNT(t2.user_id) / COUNT(t1.user_id) AS retention # COUNT不会包含NULL FROM t1 LEFT JOIN t2 ON t1.user_id = t2.user_id; 以上SQL只是求得了某一日期的次日留存率。但在数据分析中，通常我们希望看到的是如下的留存曲线。留存曲线描绘了特定日期之后一段时间每天的留存数据。我们需要用SQL来实现绘制该留存曲线所需要的数据。 和求单日留存率最大的不同点在于需要同时对用户ID和日期做GROUP BY。LEFT JOIN的主要目的是为了求时间差。以下SQL展示了如何求得时间差diff这一中间结果。 WITH t1 AS ( SELECT user_id, date(event_time) AS day FROM Events WHERE date(event_time) = '2022-01-01' GROUP BY user_id, date(event_time) ), t2 AS ( SELECT user_id, date(event_time) AS day FROM Events WHERE event_time >= \"2022-01-01 00:00:00\" AND event_time user_id start_day retent_day diff 01 2022-01-01 2022-01-07 6 01 2022-01-01 2022-01-05 4 01 2022-01-01 2022-01-03 2 01 2022-01-01 2022-01-02 1 01 2022-01-01 2022-01-01 0 03 2022-01-01 2022-01-07 6 03 2022-01-01 2022-01-06 5 03 2022-01-01 2022-01-05 4 03 2022-01-01 2022-01-01 0 04 2022-01-01 2022-01-03 2 04 2022-01-01 2022-01-02 1 04 2022-01-01 2022-01-01 0 以上结果中retent_day表示该用户有留存的日期，diff就是留存日期到起始日期的差值，1就是次日留存，2就2日留存，3就是3日留存，以此类推。要求留人数存就只需要按照diff进行GROUP BY求COUNT。如果要求留存率就还要增加一步。而且这里同样存在某天如果是留存人数为0，则结果中不存在该天的问题，这里就不再赘述。 SELECT datediff(t2.day, t1.day) AS diff, COUNT(*) FROM t1 LEFT JOIN t2 ON t1.user_id = t2.user_id GROUP BY diff ORDER BY diff; diff count(*) 0 3 1 2 2 2 4 2 5 1 6 2 漏斗分析，转化率 漏斗分析中的转化率其实和留存率有相似之处。留存率可以看成是相对于日期的转化率，而漏斗分析中的转化率可以看成是相对于具体事件的转化率。但是通常在漏斗分析时，我们会对转化的时间做出限制，比如转化必须发生在1小时之内。 选取Events表中的login、play、like三个事件做转化分析。用Events表和自身进行JOIN，筛选其中间隔小于1小时的两个不相同的事件。每个转化，同一个用户只记录一次。 SELECT event1.event_name AS event1, event2.event_name AS event2, COUNT(DISTINCT event1.user_id) AS num_conversions FROM Events event1 INNER JOIN Events event2 ON event1.event_name <> event2.event_name AND event1.user_id = event2.user_id AND TIMESTAMPDIFF(SECOND, event1.event_time, event2.event_time) 0 WHERE event1.event_name IN ('login', 'play', 'like') AND event2.event_name IN ('play', 'like') GROUP BY event1.event_name, event2.event_name event1 event2 num_conversions login like 3 login play 4 play like 3 以上可以看出login到like发生了3人次的转化，login到play发生了4次转化，play到like发生了3次转化。如果最终要求出转化率，还要先把起始事件的人数求出。对以上SQL做一点改动。 SELECT event1.event_name AS event1, event2.event_name AS event2, COUNT(DISTINCT event1.user_id) AS num_conversions, CAST(COUNT(DISTINCT event1.user_id) AS FLOAT) / (SELECT COUNT(DISTINCT user_id) FROM Events AS e WHERE e.event_name = event1.event_name) AS conversion_rate FROM Events AS event1 INNER JOIN Events AS event2 ON event1.event_name <> event2.event_name AND event1.user_id = event2.user_id AND TIMESTAMPDIFF(SECOND, event1.event_time, event2.event_time) 0 where event1.event_name in ('login', 'play', 'like') and event2.event_name in ('play', 'like') GROUP BY event1.event_name, event2.event_name event1 event2 num_conversions conversion_rate login like 3 0.75 login play 4 1 play like 3 0.75 用户分群 在实际数据分析中，我们通常需要按照一定的用户属性或者行为特征对用户进行分群分析。一道经典的SQL面试题：求连续登录3天的用户ID，就是一个典型的用户分群案例。用户连续登录N天可以反应用户某种程度的活跃。用户连续N次做某件事情也会经常被应用于运营活动中。 以下就尝试用SQL求解连续登录3天的用户ID： 将事件时间event_time转换为事件天event_day。 利用窗口函数，按用户ID分组，按照事件天event_day排序。给每一天打上一个排序后的行号rn。 将事件天event_day减去对应的行号rn得到一个flag_day。如果是连续登录的，则flag_day应该是同一天。 按照用户ID和flag_day分组，如果同一天出现大于等3，则表示该用户从start_day开始连续登录了3天及以上。 WITH t1 AS ( # 将事件时间event_time转换为事件天event_day SELECT user_id, date(event_time) AS event_day FROM Events GROUP BY user_id, date(event_time) ), t2 AS ( # 利用窗口函数，按用户ID分组，按照事件天event_day排序。给每一天打上一个排序后的行号rn SELECT user_id, event_day, row_number() OVER (PARTITION BY user_id ORDER BY event_day) AS rn FROM t1 ), t3 AS ( # 将事件天event_day减去对应的行号rn得到一个flag_day。如果是连续登录的，则flag_day应该是同一天 SELECT *, date_sub(event_day, INTERVAL rn DAY) AS flag_day FROM t2 ) # 按照用户ID和flag_day分组，如果同一天出现大于等3，则表示该用户从start_day开始连续登录了3天及以上 SELECT date_add(flag_day, INTERVAL 1 DAY) AS start_day, user_id FROM t3 GROUP BY user_id, flag_day HAVING COUNT(user_id) >= 3; start_day user_id 2022-01-01 01 2022-01-02 02 2022-01-04 03 2022-01-01 04 附 以上内容所需的数据库表结构和数据库表数据如下： CREATE TABLE Events( user_id VARCHAR(20) NOT NULL DEFAULT '', event_name VARCHAR(20) NOT NULL DEFAULT '', event_time VARCHAR(20) NOT NULL DEFAULT '' ); insert into Events values('01', 'login', '2022-01-01 11:50:31'); insert into Events values('01', 'click', '2022-01-01 11:51:10'); insert into Events values('01', 'view', '2022-01-01 11:55:59'); insert into Events values('01', 'play', '2022-01-01 12:01:03'); insert into Events values('01', 'like', '2022-01-01 12:06:42'); insert into Events values('01', 'login', '2022-01-02 15:50:31'); insert into Events values('01', 'play', '2022-01-02 16:01:03'); insert into Events values('01', 'like', '2022-01-02 16:06:42'); insert into Events values('01', 'login', '2022-01-03 21:50:31'); insert into Events values('01', 'play', '2022-01-03 22:01:03'); insert into Events values('01', 'login', '2022-01-05 08:50:31'); insert into Events values('01', 'view', '2022-01-05 08:55:59'); insert into Events values('01', 'play', '2022-01-05 09:01:03'); insert into Events values('01', 'like', '2022-01-05 09:06:42'); insert into Events values('01', 'logout', '2022-01-05 09:51:10'); insert into Events values('01', 'login', '2022-01-07 22:50:31'); insert into Events values('02', 'login', '2022-01-02 09:10:31'); insert into Events values('02', 'play', '2022-01-02 09:15:03'); insert into Events values('02', 'like', '2022-01-02 09:20:55'); insert into Events values('02', 'login', '2022-01-03 13:15:31'); insert into Events values('02', 'play', '2022-01-03 13:20:03'); insert into Events values('02', 'logout', '2022-01-03 13:25:55'); insert into Events values('02', 'login', '2022-01-04 01:50:31'); insert into Events values('02', 'click', '2022-01-04 02:51:10'); insert into Events values('02', 'view', '2022-01-04 02:55:59'); insert into Events values('02', 'play', '2022-01-04 03:01:03'); insert into Events values('02', 'like', '2022-01-04 03:06:42'); insert into Events values('02', 'login', '2022-01-06 18:50:31'); insert into Events values('02', 'view', '2022-01-06 18:55:01'); insert into Events values('02', 'like', '2022-01-06 18:59:42'); insert into Events values('03', 'login', '2022-01-01 16:23:07'); insert into Events values('03', 'login', '2022-01-05 23:50:31'); insert into Events values('03', 'play', '2022-01-06 00:01:03'); insert into Events values('03', 'like', '2022-01-06 00:06:42'); insert into Events values('03', 'login', '2022-01-07 13:08:31'); insert into Events values('03', 'click', '2022-01-07 13:10:10'); insert into Events values('03', 'view', '2022-01-07 13:47:59'); insert into Events values('04', 'login', '2022-01-01 19:43:31'); insert into Events values('04', 'play', '2022-01-01 19:47:03'); insert into Events values('04', 'logout', '2022-01-01 19:53:55'); insert into Events values('04', 'login', '2022-01-02 18:43:31'); insert into Events values('04', 'play', '2022-01-02 18:47:03'); insert into Events values('04', 'logout', '2022-01-02 18:53:55'); insert into Events values('04', 'login', '2022-01-03 17:43:31'); insert into Events values('04', 'play', '2022-01-03 17:47:03'); insert into Events values('04', 'logout', '2022-01-03 17:53:55'); "},"ProbabilityAndStatistics.html":{"url":"ProbabilityAndStatistics.html","title":"概率论与数理统计","keywords":"","body":"基本概念定义 概率论基本概念 随机事件：随机实验中可能发生也可能不发生的事情，简称事件 必然事件：随机实验中必然发生写事件，用符号 Ω\\OmegaΩ 表示 不可能事件：随机试验中必然不发生的事件，用符号 ∅\\varnothing∅ 表示 随机实验 EEE 中必然发生一个且仅发生一个的最简单事件为实验 EEE 的基本事件，由若干基本事件组合而成的事件成为复合事件。一个事件是否为基本事件是相对于实验目的而言的。 我们用集合表示事件，对于随机实验 EEE 的每一个基本事件，用一个只包含一个元素 ω\\omegaω 的单元素 {ω}\\{\\omega\\}{ω} 表示；复合事件，则用对应的若干个元素所组成的集合表示； 由全体基本事件所对应的全部元素所组成的集合，称为随机实验 EEE 的样本空间，样本空间仍然用 Ω\\OmegaΩ 表示，和必然事件一样。样本空间的每一个元素 ω\\omegaω 为样本点。 随机变量：设 Ω\\OmegaΩ 是随机试验 EEE 的样本空间，若对于每一个样本点 ω∈Ω\\omega \\in \\Omegaω∈Ω ，都有唯一的实数 X(ω)X(\\omega)X(ω) 与之对应，且对于任意实数 xxx ，都有确定的概率 P{X(ω)≤x}P\\{X(\\omega) \\leq x\\}P{X(ω)≤x} 与之对应，则称 X(ω)X(\\omega)X(ω) 为随机变量，简记为 XXX 。随机变量是一个函数。 概率分布函数：设 Ω\\OmegaΩ 是随机试验 EEE 的样本空间，xxx 是任意实数，称函数 F(x)=P{X≤x}=P{ω:X(ω)≤x} F(x) = P\\{X \\leq x\\}= P\\{\\omega:X(\\omega) \\leq x\\} F(x)=P{X≤x}=P{ω:X(ω)≤x} 为随机变量 XXX 的分布函数 ，F(x)F(x)F(x) 也可以记作 Fx(x)F_x(x)Fx​(x) 。 概率密度：设 F(x)F(x)F(x) 是随机变量 XXX 的分布函数，若存在非负函数 f(x)f(x)f(x) ，对任意实数 xxx ，有 F(x)=∫−∞xf(u)du F(x) = \\int_{-\\infty}^x f(u)du F(x)=∫−∞x​f(u)du 则称 XXX 是连续型随机变量，称 f(x)f(x)f(x) 为 XXX 的概率密度。 数理统计基本概念 总体：研究对象的全体 个体：组成总体的每个基本元素 赋有一定概率分布的总体称为统计总体，其概率分布称为总体分布。当总体分布为正态分布时，称为正态分布总体或简称正态总体。 总体的概率分布是总体的核心。因此，进一步将总体看成具有相应的概率分布的随机变量，比如 XXX ，称作总体 XXX ，则随机变量 XXX 的概率分布就是总体分布。 样本是按一定的规定从总体中抽出的一部分个体。这里的”按一定的规定“，是指为保证总体中的每一个个体有同等的被抽出的机会而采取的一些措施。取得样本的过程，称为抽样。 样本是一组随机变量，记为 X1,X2,⋯ ,XnX_1,X_2,\\cdots,X_nX1​,X2​,⋯,Xn​ ，其中 nnn 称为样本容量或样本大小或样本量。实施抽样后得到的具体数据 x1,x2,⋯ ,xnx_1,x_2,\\cdots,x_nx1​,x2​,⋯,xn​ 称为样本观测值。 简单随机样本： 样本 X1,X2,⋯ ,XnX_1,X_2,\\cdots,X_nX1​,X2​,⋯,Xn​ 满足以下要求的称之为简单随机样本，如果没有特别说明，通常都是简单随机样本： 代表性。每个 XiX_iXi​ 应该与总体 XXX 有相同的分布； 独立性。 X1,X2,⋯ ,XnX_1,X_2,\\cdots,X_nX1​,X2​,⋯,Xn​ 应该是相互独立的随机变量。 统计量：设 X1,X2,⋯ ,XnX_1,X_2,\\cdots,X_nX1​,X2​,⋯,Xn​ 为来自总体 XXX 的一个样本，若样本函数 g(X1,X2,⋯ ,Xn)g(X_1,X_2,\\cdots,X_n)g(X1​,X2​,⋯,Xn​) 中不含任何未知参数，则称 g(X1,X2,⋯ ,Xn)g(X_1,X_2,\\cdots,X_n)g(X1​,X2​,⋯,Xn​) 为一个统计量。常用的统计量有：样本均值，样本方差，样本标准差。 将样本观测值 x1,x2,⋯ ,xnx_1,x_2,\\cdots,x_nx1​,x2​,⋯,xn​ 带入统计量公式中得到的值称之为统计值。 统计量也是随机变量，统计量的分布称为抽样分布，比如样本均值的抽样分布。 抽样分布定理 设 X1,X2,⋯ ,XnX_1,X_2,\\cdots,X_nX1​,X2​,⋯,Xn​ 是正态总体 N(μ,σ2)N(\\mu,\\sigma^2)N(μ,σ2) 的样本，Xˉ\\bar{X}Xˉ 、S2S^2S2 分别是样本均值和样本方差，则有： (1)Xˉ与S2相互独立(2)Xˉ∼N(μ,σ2n)(3)n−1σ2S2∼χ2(n−1)(4)Xˉ−μS/n∼t(n−1) \\begin{align*} &(1)\\quad \\bar{X} 与 S^2 相互独立\\\\ &(2)\\quad \\bar{X}\\sim N(\\mu,\\frac{\\sigma^2}{n}) \\\\ &(3)\\quad \\frac{n-1}{\\sigma^2}S^2\\sim \\chi^2(n-1) \\\\ &(4)\\quad \\frac{\\bar{X}-\\mu}{S/\\sqrt{n}}\\sim t(n-1)\\\\ \\end{align*} ​(1)Xˉ与S2相互独立(2)Xˉ∼N(μ,nσ2​)(3)σ2n−1​S2∼χ2(n−1)(4)S/n​Xˉ−μ​∼t(n−1)​ 参数估计 参数估计在A/B实验领域起着关键性的作用 参数的点估计 跟据矩估计法和极大似然估计法均可得出，若 X1,X2,⋯ ,XnX_1,X_2,\\cdots,X_nX1​,X2​,⋯,Xn​ 是正态总体 N(μ,σ2)N(\\mu,\\sigma^2)N(μ,σ2) 的样本，则均值 μ\\muμ 和方差 σ2\\sigma^2σ2 的估计为： μ^=Xˉ,σ^2=1n∑i=1n(Xi−Xˉ)2 \\hat{\\mu}=\\bar{X},\\quad \\hat{\\sigma}^2=\\frac{1}{n}\\sum_{i=1}^n(X_i-\\bar{X})^2 μ^​=Xˉ,σ^2=n1​i=1∑n​(Xi​−Xˉ)2 但基于参数估计无偏性的准则，由矩估计法和极大似然估计法求得的 σ^2\\hat{\\sigma}^2σ^2 并不是无偏的。需要将分母 nnn 修正为 n−1n-1n−1，也就是 S2=1n−1∑i=1n(Xi−Xˉ)2 S^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X})^2 S2=n−11​i=1∑n​(Xi​−Xˉ)2 为 σ2\\sigma^2σ2 的无偏估计。 区间估计 利用枢轴变量法构造置信区间 一个正态总体参数的置信区间 设 X1,X2,⋯ ,XnX_1,X_2,\\cdots,X_nX1​,X2​,⋯,Xn​ 是正态总体 N(μ,σ2)N(\\mu,\\sigma^2)N(μ,σ2) 的样本，求未知参数 μ\\muμ 的置信度为 1−α1-\\alpha1−α 的置信区间： σ2\\sigma^2σ2 已知 因为样本均值 Xˉ\\bar{X}Xˉ 是 μ\\muμ 的无偏估计，且根据抽样分布定理 Xˉ∼N(μ,σ2n)\\bar{X}\\sim N(\\mu,\\frac{\\sigma^2}{n})Xˉ∼N(μ,nσ2​) ，所以： U=Xˉ−μσ/n∼N(0,1) U=\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1) U=σ/n​Xˉ−μ​∼N(0,1) 于是由标准正态分布的上侧分位数的定义可知，对于给定的置信度 1−α1-\\alpha1−α ，有 P{∣U∣≤uα2}=1−αP\\{|U|\\leq u_\\frac{\\alpha}{2}\\}=1-\\alphaP{∣U∣≤u2α​​}=1−α，即： P{−uα2≤Xˉ−μσ/n≤uα2}=P{Xˉ−σnuα2≤μ≤Xˉ+σnuα2}=1−α \\begin{align*} & P\\{-u_\\frac{\\alpha}{2} \\leq \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\leq u_\\frac{\\alpha}{2}\\} \\\\ =& P\\{\\bar{X}-\\frac{\\sigma}{\\sqrt{n}}u_\\frac{\\alpha}{2} \\leq \\mu \\leq \\bar{X}+\\frac{\\sigma}{\\sqrt{n}}u_\\frac{\\alpha}{2}\\} \\\\ =& 1-\\alpha \\end{align*} ==​P{−u2α​​≤σ/n​Xˉ−μ​≤u2α​​}P{Xˉ−n​σ​u2α​​≤μ≤Xˉ+n​σ​u2α​​}1−α​ 从而得到 μ\\muμ 的置信度为 1−α1-\\alpha1−α 的置信区间为 [Xˉ−σnuα2,Xˉ+σnuα2][\\bar{X}-\\frac{\\sigma}{\\sqrt{n}}u_\\frac{\\alpha}{2},\\bar{X}+\\frac{\\sigma}{\\sqrt{n}}u_\\frac{\\alpha}{2}][Xˉ−n​σ​u2α​​,Xˉ+n​σ​u2α​​] σ2\\sigma^2σ2 未知 此时 U=Xˉ−μσ/nU=\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}U=σ/n​Xˉ−μ​ 不再构成枢轴变量，因为 σ2\\sigma^2σ2 未知，故用 S2S^2S2 代替 σ2\\sigma^2σ2 。根据抽样分布定理，枢轴变量 T=Xˉ−μS/n∼t(n−1)T=\\frac{\\bar{X}-\\mu}{S/\\sqrt{n}}\\sim t(n-1)T=S/n​Xˉ−μ​∼t(n−1) 。因为 ttt 分布也是关于 YYY 轴对称，于是有： P{−tα2(n−1)≤Xˉ−μS/n≤tα2(n−1)}=1−α P\\{-t_\\frac{\\alpha}{2}(n-1) \\leq \\frac{\\bar{X}-\\mu}{S/\\sqrt{n}} \\leq t_\\frac{\\alpha}{2}(n-1)\\}=1-\\alpha P{−t2α​​(n−1)≤S/n​Xˉ−μ​≤t2α​​(n−1)}=1−α 经过恒等变形，得到参数 μ\\muμ 的置信度为 1−α1-\\alpha1−α 的置信区间是 [Xˉ−Sntα2(n−1),Xˉ+Sntα2(n−1)][\\bar{X}-\\frac{S}{\\sqrt{n}}t_\\frac{\\alpha}{2}(n-1),\\bar{X}+\\frac{S}{\\sqrt{n}}t_\\frac{\\alpha}{2}(n-1)][Xˉ−n​S​t2α​​(n−1),Xˉ+n​S​t2α​​(n−1)] 两个正态总体的区间估计 大样本方法构造置信区间 单侧置信区间 "},"GitCheatSheet.html":{"url":"GitCheatSheet.html","title":"Git Cheat Sheet","keywords":"","body":" 添加仓库提交文件 创建一个版本库 $ git init Initialized empty Git repository in /Users/xxx/xxx/.git/ 把文件添加进暂存区 $ git add readme.txt $ git add file1.txt file2.txt # 添加当前目录下的所有文件进暂存区 $ git add . 把文件提交到本地仓库 $ git commit -m \"wrote a readme file\" 查看当前状态，也可以查看到位于哪个分支 $ git status # On branch master # Your branch is ahead of 'origin/master' by 202 commits. # # Untracked files: # (use \"git add ...\" to include in what will be committed) # # ../profile.svg # ../templates/customer_service.html nothing added to commit but untracked files present (use \"git add\" to track) 查看修改内容 $ git diff readme.txt diff --git a/readme.txt b/readme.txt index 46d49bf..9247db6 100644 --- a/readme.txt +++ b/readme.txt @@ -1,2 +1,2 @@ -Git is a version control system. +Git is a distributed version control system. Git is free software. 本地版本控制 版本查看 $ git log commit 1094adb7b9b3807259d8cb349e7df1d4d6477073 (HEAD -> master) Author: Michael Liao Date: Fri May 18 21:06:15 2018 +0800 append GPL commit e475afc93c209a690c39c13a46716e8fa000c366 Author: Michael Liao Date: Fri May 18 21:03:36 2018 +0800 add distributed 回退到上一个版本，回退到上上个版本 $ git reset --hard HEAD^ HEAD is now at e475afc add distributed $ git reset --hard HEAD^ 通过版本号更改到指定版本，可以是之后的版本，也可以是之前的版本 $ git reset --hard 1094a HEAD is now at 83b0afe append GPL 回退到之前的版本之后，无法通过git log查看到后面的版本，可以通过git relog来查看每一次输入的命令，查看到后面版本的版本号 $ git reflog e475afc HEAD@{1}: reset: moving to HEAD^ 1094adb (HEAD -> master) HEAD@{2}: commit: append GPL e475afc HEAD@{3}: commit: add distributed eaadf4e HEAD@{4}: commit (initial): wrote a readme file 远程仓库 关联远程仓库，并把远程仓库取名为origin $ git remote add origin git@github.com:xxx/learngit.git 第一次将本地仓库master分支推送到远程origin仓库。-u参数会将本地master分支和远程master分支做关联。 $ git push -u origin master Counting objects: 20, done. Delta compression using up to 4 threads. Compressing objects: 100% (15/15), done. Writing objects: 100% (20/20), 1.64 KiB | 560.00 KiB/s, done. Total 20 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), done. To github.com:michaelliao/learngit.git * [new branch] master -> master Branch 'master' set up to track remote branch 'master' from 'origin'. 后续将本地仓库master分支内容推送到远程仓库 $ git push origin master 查看远程仓库信息 $ git remote -v origin git@github.com:xxx/learn-git.git (fetch) origin git@github.com:xxx/learn-git.git (push) 解除和远程仓库origin的关联。如果要删除远程仓库，需要去到远程仓库所在地址进行删除。 $ git remote rm origin 从远程仓库克隆一个本地库 $ git clone git@github.com:xxx/gitskills.git Cloning into 'gitskills'... remote: Counting objects: 3, done. remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 3 Receiving objects: 100% (3/3), done. 从远程仓库拉取代码并合并本地版本 $ git pull $ git pull origin git pull其实就是 git fetch 和git merge FETCH_HEAD 的简写。格式如下： # git pull : # 拉取远程仓库origin的master分支，与本地的brantest分支合并 $ git pull origin master:brantest # 如果远程分支是与当前分支合并，则冒号后面的部分可以省略 $ git pull origin master 分支操作 创建dev分支，然后切换到dev分支 $ git checkout -b dev Switched to a new branch 'dev' git checkout命令加上-b参数表示创建并切换，相当于以下两条命令。其中git branch dev为创建本地分支，git checkout dev为切换分支 $ git branch dev $ git checkout dev Switched to branch 'dev' switch也可以用来切换分支，相比checkout更容易理解。因为撤销修改是git checkout -- ，同一个命令，有两种作用，确实有点令人迷惑。 创建并切换到新的dev分支 $ git switch -c dev 直接切换到已有的master分支 $ git switch master 用git branch命令查看当前分支 $ git branch * dev master 把dev分支的工作成果合并到当前所处的master分支。Fast-forward信息表示这次合并是“快进模式”，也就是直接把master指向dev的当前提交。也不是每次合并都能Fast-forward。 $ git merge dev Updating d46f35e..b17d20e Fast-forward readme.txt | 1 + 1 file changed, 1 insertion(+) 删除dev分支 $ git branch -d dev Deleted branch dev (was b17d20e). "}}